{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Teaching Your Monolith to Dance Hands on WebSphere Application Moderinization Journey from VMs to containers on OpenShift Last updated: July 2021 This material is designed as a Demo/Lab/Workshop asset that includes the following WebSphere Application Modernization use cases: Containerization Basics Operational Modernization Runtime Modernization Devops Modernization Application Management Modernization Application modernization is a journey of moving existing applications to a more modern cloud-native infrastructure. There are several approaches to application modernization and provided are key reference implementations to help you on your modernization journey. Operational Modernization Repackage the application to deploy within a container but maintaining a monolith application without changes to the application or runtime. This solution uses IBM Transforation Advisor and the Runtime Component Operator, supported by IBM WebSphere Hybrid Edition, to deploy and manage the containerized application on Red Hat OpenShift. Runtime Modernization Update the application runtime to Open Liberty , a suitable cloud-native framework. Modernize some aspects of the application by taking advantage of MicroProfile specifications, such as Health, Metrics, JWT, Config and OpenAPI. This solution uses Open Liberty Operator, also supported by IBM WebSphere Hybrid Edition, to deploy and manage the modernized application on Red Hat OpenShift. Application Management Monitor the applications easily using various dashboards such as Kibana and Grafana. Identify potential problems using logging dashboards. Get insights into the performance of your applications using metrics dashboards. Perform Day-2 operations, such as gathering traces and dumps, to debug potential issues easily using Open Liberty Operator. Devops Modernization Continuous Integration and Continuous Deployment (CI/CD) of applications is critical for repeatable and reliable deployments. Modern devops technologies such as Tekton and Argo CD are designed for Kubernetes (OSCP) and provide the CI/CD capabilities, and fully integrated and supported in OSCP via Kubernetes Operators. Hands-on Workshop Reserve lab environments Please follow the setup instructions to reserver a lab environemnt, or schedule a workshop. The lab environemtn is configured with the tools you need to be able to complete the labs. Containerization Labs Basic knowledge about containers, OpenShift, and BM Transfromation Advisor is recommended. Completing the following hands-on labs will help you to easily navigate this workshop: Lab 1: Introduction to Containerization Lab 2: Introduction to Container Orchestration using OpenShift Lab 3: Introduction to IBM Transformation Advisor Appplication Modernization Labs Lab 3: Operational Modernization Lab 4: Runtime Modernization Lab 5: Application Management DevOps Modernization Labs Lab 6: Openshift Pipelines Lab 7: GitOps with ArgoCD","title":"Home"},{"location":"#teaching-your-monolith-to-dance","text":"","title":"Teaching Your Monolith to Dance "},{"location":"HCTEST/","text":"Introduction to Containerization Table of Contents Introduction to Containerization Table of Contents Background Prerequisites What is a Container Login to the VM Check your environment Run a pre-built image Build and Run Your Own Image Managing Image Versions Extra Credit Next Background If you are expecting a lab about docker , you are at the right place. This lab will introduce you to the basic concepts of containerization, including: What are containers and container images How to start, stop, and remove containers. How to create container images How to version container images Prerequisites You have podman or docker installed. Only docker is installed for this lab. You have access to the internet. You have cloned this lab from github. If not, follows these steps: git clone https://github.com/IBM/openshift-workshop-was.git cd openshift-workshop-was/labs/Openshift/HelloContainer What is a Container Compared to virtual machines, containers supports virtualization at the process level. Think of them as virtual processes. The isolation abstraction provided by the operating system makes the process think that it's running in its own virtual machine. As processes, containers may be created, started, and stopped much more quickly than virtual machines. Everything you need to run your application, from the operating system and up, is stored in a special file called a container image. Container images are self contained and portable. You may run one or more instances anywhere. And you don't have to worry about missing prerequisites, because all prerequisites are stored in the image. Container images are created via tools such as docker or podman . Existing images are hosted in container registries. For example, docker hub, or registry.access.redhat.com, or your own internal registry. If you need more background on containers: https://www.docker.com/resources/what-container Login to the VM If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password: \\ Password: engageibm Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window. Check your environment Open a terminal window from within your VM. List version of docker: docker --version Example output: Docker version 20.10.7, build f0df350 For more background on docker command line: https://docs.docker.com/engine/reference/commandline/cli/ Run a pre-built image Container images must be available locally before they can be run. To list available local images: docker images KLPREPOSITORY TAG IMAGE ID CREATED SIZE Images are hosted in container registries. The default container registry for docker is docker hub, located at https://hub.docker.com. Let's pull a test image from docker hub: docker pull openshift/hello-openshift And the output: Using default tag: latest latest: Pulling from openshift/hello-openshift 4f4fb700ef54: Pull complete 8b32988996c5: Pull complete Digest: sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e Status: Downloaded newer image for openshift/hello-openshift:latest docker.io/openshift/hello-openshift:latest List available local images again: docker images REPOSITORY TAG IMAGE ID CREATED SIZE openshift/hello-openshift latest 7af3297a3fb4 2 years ago 6.09MB Inspect the image metadata: docker inspect openshift/hello-openshift Note that: It exposes two ports: 8080 and 8888 It runs as user 1001 The entry point executable is /hello-openshift [ { \"Id\": \"sha256:7af3297a3fb4487b740ed6798163f618e6eddea1ee5fa0ba340329fcae31c8f6\", \"RepoTags\": [ \"openshift/hello-openshift:latest\" ], \"RepoDigests\": [ \"openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e\" ], ... \"Config\": { \"User\": \"1001\", ... \"ExposedPorts\": { \"8080/tcp\": {}, \"8888/tcp\": {} }, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], ... \"Entrypoint\": [ \"/hello-openshift\" ] ... } ] Run the image in an container: docker run --name hello1 -d -p 8080:8080 -p 8888:8888 openshift/hello-openshift Note that: The --name option gives the container a name. The -d option runs the command in the background as a daemon The -p option maps the port on the host to the port in the container. Through virtual networking, the port within the container is always the same for all running instances. But to support multiple concurrent running instances, the actual port on the host must be different for each instance. When you start the container, you may assign a new port on the host dynamically. The output of the command is the container ID for the running container. If the container starts successfully, the executable specified by the Entrypoint in the metadata is run. For our sample, it is /hello-openshift . Access the application in the container. Open the Firefox Web Browser from inside of the VM. Go to the URL http://localhost:8080 Also try port 8888 Run another instance of the same image. Note that this new instance is assigned new port numbers 8081 and 8889 on the host. This is so that they don't conflict with the ports 8080 and 8888 already allocated to the first instance. docker run --name hello2 -d -p 8081:8080 -p 8889:8888 openshift/hello-openshift Question: how does this compare to the time it takes to start a new virtual machine? Access the application in the new container the same way. Return to the Firefox Web Browser but instead go to the URL http://localhost:8081 Also try port 8889 Verify there are two containers running in the same host: docker ps : CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5a62f8527b44 openshift/hello-openshift \"/hello-openshift\" About a minute ago Up About a minute 0.0.0.0:8081->8080/tcp, 0.0.0.0:8889->8888/tcp hello2 c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 4 minutes ago Up 4 minutes 0.0.0.0:8080->8080/tcp, 0.0.0.0:8888->8888/tcp hello1 View the logs: docker logs hello1 And the output: ``` serving on 8888 serving on 8080 ``` View the logs on the second container: docker logs hello2 And the output: serving on 8888 serving on 8080 Note: within the container, each instance behaves as if it's running in its own virtual environment, and has opened the same ports. Outside of the container, different ports are opened. To export the file system of a running container: docker export hello1 > hello1.tar List the files on the file system: tar -tvf hello1.tar Note that this is a very small image. -rwxr-xr-x 0/0 0 2020-04-29 16:48 .dockerenv drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/ -rwxr-xr-x 0/0 0 2020-04-29 16:48 dev/console drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/pts/ drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/shm/ drwxr-xr-x 0/0 0 2020-04-29 16:48 etc/ -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/hostname -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/hosts lrwxrwxrwx 0/0 0 2020-04-29 16:48 etc/mtab -> /proc/mounts -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/resolv.conf -rwxr-xr-x 0/0 6089990 2018-04-18 10:22 hello-openshift drwxr-xr-x 0/0 0 2020-04-29 16:48 proc/ drwxr-xr-x 0/0 0 2020-04-29 16:48 sys/ Run another command in the running container. You can reach into the running container to run another command. The typical use case is to run a shell command, so you can use the shell to navigate within the container and run other commands. However, our image is tiny, and there is no built-in shell. For the purpose of this lab, we'll execute the same command again: docker exec -ti hello1 /hello-openshift . Running this command again in the same container results in an error, because there is already another copy running in the background that is bound to the ports 8080 and 8888: serving on 8888 serving on 8080 panic: ListenAndServe: listen tcp :8888: bind: address already in use ... Stop the containers: docker stop hello1 docker stop hello2 List running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES List all containers, including stopped ones: docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5a62f8527b44 openshift/hello-openshift \"/hello-openshift\" 28 minutes ago Exited (2) 28 seconds ago hello2 c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 31 minutes ago Exited (2) 32 seconds ago hello1 Restart a stopped container: docker restart hello1 List running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 33 minutes ago Up 8 seconds 0.0.0.0:8080->8080/tcp, 0.0.0.0:8888->8888/tcp hello1 Stop the container: docker stop hello1 Remove stopped containers, and note that there are no more containers: docker rm hello1 docker rm hello2 docker ps -a Remove the image from local cache: View current images: docker images Example output: REPOSITORY TAG IMAGE ID CREATED SIZE openshift/hello-openshift latest 7af3297a3fb4 2 years ago 6.09MB Remove the image: docker rmi openshift/hello-openshift Example output: Untagged: openshift/hello-openshift:latest Untagged: openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e Deleted: sha256:7af3297a3fb4487b740ed6798163f618e6eddea1ee5fa0ba340329fcae31c8f6 Deleted: sha256:8fd6a1ece3ceceae6d714004614bae5b581c83ab962d838ef88ce760583dcb80 Deleted: sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef Check that the image has been removed: docker images Example output: REPOSITORY TAG IMAGE ID CREATED SIZE Build and Run Your Own Image We use a Containerfile , which contains the instructions to create the new layers of your image. For those familiar with docker, the Containerfile is equivalent to Dockerfile . Recall that an image contains the entire file system that you want to use to run your virtual process in a container. For this sample, we are building a new image for a Java EE web application ServletApp.war. It is configured to run on the WebSphere Liberty Runtime. The configuration file for the server is in the server.xml. Change directory to openshift-workshop-was/labs/Openshift/HelloContainer cd openshift-workshop-was/labs/Openshift/HelloContainer Review the provided Containerfile from the directory: cat Containerfile Content of Containerfile : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi COPY server.xml /config COPY ServletApp.war /config/dropins/app.war RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml To create a new image, you start with a pre-existing image. The first line FROM specifies the existing image to be used as the base. If this is not in the local registry, it will be pulled from a remote registry such as docker hub. The base image we are using, ibmcom/websphere-liberty , is already prepackaged for us and made available on docker hub. The second line COPY is a straight copy of the file server.xml from the local directory to /config/server.xml in the image. This adds a new layer to the image with the actual server configuration to be used. The third line, another COPY , copies ServletApp.war from the current directory into a new layer in the image you are creating, at the location /config/dropins/app.war . The last line RUN runs the installUtility command within the image to install additional features required to run the server as specified in server.xml . You can use the RUN command to run any command that is available within the image to customize the image itself. Run the build. Ensure you include . at the end of the command (the dot indicates using the file from the current directory): docker build -t app -f Containerfile . The -t option tags the name of the image as app . The -f option specifies the name of the Containerfile . The build command runs the commands in Containerfile to build a new image called app . Example output: Sending build context to Docker daemon 25.6kB Step 1/4 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi kernel-java8-ibmjava-ubi: Pulling from ibmcom/websphere-liberty ee2244abc66f: Pull complete befb03b11956: Pull complete 137dc88f6a93: Pull complete 5bdd69a33184: Pull complete d4e2554981d7: Pull complete 32c91bc0f2e1: Pull complete db7e931336a9: Pull complete 3b32f9956ae2: Pull complete 304584ffa0a2: Pull complete 9f6da4c82b7e: Pull complete b6fa5b2e2325: Pull complete Digest: sha256:d76f79695afe2f653fc7b272f9a629105446e6b78ff0d733d494c93ff05728e7 Status: Downloaded newer image for ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> 4d9265befb26 Step 2/4 : COPY server.xml /config ---> 4a02d03d3725 Step 3/4 : COPY ServletApp.war /config/dropins/app.war ---> b2def2a0feac Step 4/4 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Running in 5f5b05aec1ae Checking for missing features required by the server ... The server requires the following additional features: appsecurity-2.0 servlet-3.0. Installing features from the repository ... Establishing a connection to the configured repositories ... This process might take several minutes to complete. Successfully connected to all configured repositories. Preparing assets for installation. This process might take several minutes to complete. Additional Liberty features must be installed for this server. To install the additional features, review and accept the feature license agreement: The --acceptLicense argument was found. This indicates that you have accepted the terms of the license agreement. Step 1 of 12: Downloading ssl-1.0 ... Step 2 of 12: Installing ssl-1.0 ... Step 3 of 12: Downloading appSecurity-2.0 ... Step 4 of 12: Installing appSecurity-2.0 ... Step 5 of 12: Downloading servlet-3.0 ... Step 6 of 12: Installing servlet-3.0 ... Step 7 of 12: Downloading jndi-1.0 ... Step 8 of 12: Installing jndi-1.0 ... Step 9 of 12: Downloading distributedMap-1.0 ... Step 10 of 12: Installing distributedMap-1.0 ... Step 11 of 12: Validating installed fixes ... Step 12 of 12: Cleaning up temporary files ... All assets were successfully installed. Start product validation... Product validation completed successfully. Removing intermediate container 5f5b05aec1ae ---> e1c6bfabda76 Successfully built e1c6bfabda76 Successfully tagged app:latest List the images to see that the new image app is built: docker images Note that the base image, ibmcom/websphere-liberty has also been pulled into the local registry. REPOSITORY TAG IMAGE ID CREATED SIZE app latest baa6bb9ad29d 2 minutes ago 544 MB ibmcom/websphere-liberty kernel-java8-ibmjava-ubi 7ea3d0a2b3fe 4 hours ago 544 MB Start the container. Note that you are running with both http and https ports: docker run -d -p 9080:9080 -p 9443:9443 --name=app-instance app Access the application running in the container: Open the Firefox Web Browswer and go to URL http://localhost:9080/app Check that it renders a page showing Simple Servlet ran successfully . Also point your browser to 9443: https://localhost:9443/app List the running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 595cdc49c710 app \"/opt/ibm/helpers/ru\u2026\" 8 minutes ago Up 8 minutes 0.0.0.0:9080->9080/tcp, 0.0.0.0:9443->9443/tcp app-instance Access the logs to your container: docker logs -f app-instance Use Ctrl-C To exit. Remote shell into your running container to poke around: docker exec -it app-instance /bin/sh In the shell session, run whoami and then run id , note you're not running as root. Note that this is a stripped down environment where many commands are not available. For example, try which ps . cd /logs to find the log files cd /liberty/wlp to find the location of the liberty install cd /liberty/wlp/usr/servers/defaultServer to find the server configuration. cd /opt/ibm/wlp/output/defaultServer to find the workarea files required by the server runtime. Exit from the container: exit Cleanup: docker stop app-instance docker rm app-instance Managing Image Versions There is no built-in versioning for container images. However, you may use a tagging convention to version your images. The convention is to use major.minor.patch , such as 1.3.5 . The default tag if you don't specify one is latest . Let's assume that the first version we will build for our environment is 1.3.5. (The earlier versions are built in a different environment.) Run the commands to tag the latest app image for our first version: docker tag app app:1 docker tag app app:1.3 docker tag app app:1.3.5 List the images again: docker images And the output: REPOSITORY TAG IMAGE ID CREATED SIZE app 1 d98cbdf82a0d 21 hours ago 542MB app 1.3 d98cbdf82a0d 21 hours ago 542MB app 1.3.5 d98cbdf82a0d 21 hours ago 542MB app latest d98cbdf82a0d 21 hours ago 542MB Note that all the different tags are currently associated with the same image, as they have the same image ID. After tagging, the command docker run app:<version> ... or docker pull app:<version> ... resolves the available versions as follows: app:1 resolves to the latest 1.x.x version, which in this case is 1.3.5 . app:1.3 resolves to the latest 1.3.x version, which in this case is the 1.3.5 app:1.3.5 resolves to the exact version 1.3.5 . After you build a new patch image containing defect fixes, you want to manage the tags for the new image so that a new docker run app:<version> ... or docker pull app:<version> ... command resolves the images as follows: app:1.3.5 : resolves to the existing 1.3.5 image. app:1.3.6 : resolves to the new image app:1.3 : resolves to the new image. app:1 : resolves to the new image Let's simulate a defect fix by building a new image using Containerfile1 instead of Containerfile : docker build -t app -f Containerfile1 . Example output: Sending build context to Docker daemon 32.26kB Step 1/5 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> bb79b9e26fd3 Step 2/5 : COPY server.xml /config ---> Using cache ---> f10659bc62b2 Step 3/5 : COPY ServletApp.war /config/dropins/app.war ---> Using cache ---> 24d85579e404 Step 4/5 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Using cache ---> 5e924d776a9c Step 5/5 : RUN echo test1 > /config/test1 ---> Running in 08e6135b00f5 Removing intermediate container 08e6135b00f5 ---> 69d20332a5e0 Successfully built 69d20332a5e0 Successfully tagged app:latest Tag it as follows : docker tag app app:1 docker tag app app:1.3 docker tag app app:1.3.6 Verify that these are the same images: app:1 , app:1.3 , app:1.3.6 . A new minor version involves compatible changes beyond just bug fixes. After you build a new minor version image, you want to manage the tags such that: app:1.3.5 : resolves to the existing 1.3.5 image. app:1.3.6 : resolves to the existing 1.3.6 image app:1.4.0 : resolves to the new image app:1.3 : resolves to the existing 1.3.6 image. app:1.4 : resolves to the new image. app:1 : resolves to the new image Build a new image using Containerfile2 : docker build -t app -f Containerfile2 . Sending build context to Docker daemon 32.26kB Step 1/6 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> bb79b9e26fd3 Step 2/6 : COPY server.xml /config ---> Using cache ---> f10659bc62b2 Step 3/6 : COPY ServletApp.war /config/dropins/app.war ---> Using cache ---> 24d85579e404 Step 4/6 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Using cache ---> 5e924d776a9c Step 5/6 : RUN echo test1 > /config/test1 ---> Using cache ---> 69d20332a5e0 Step 6/6 : RUN echo test2 > /config/test2 ---> Running in 96ea24b9ba66 Removing intermediate container 96ea24b9ba66 ---> 31b27169b3bc Successfully built 31b27169b3bc Tag it as follows: docker tag app app:1 docker tag app app:1.4 docker tag app app:1.4.0 Verify that 1 , 1.4 , and 1.4.0 are the same image 1.3 and 1.3.6 are the same image Extra Credit Search the internet for information about multi-stage build. In a single stage build, the final image contains both build and runtime artifacts. A multi-stage build allows you to build with one base image, and copy the result of the build to another base image. This gives you even more control over the output of the build, and the size of the final image. Start another instances of the app image for vertical scaling, but with different port numbers on the host. Point your browser to hub.docker.com , click \"Explore\" and explore the millions of available images. Think about how you would tag a new image at a major version, 2.0.0 . Think about what would be required to manage containers across multiple machines to support horizontal scaling. Congratulations! You have completed the Introduction to Containerization lab. Next Please follow the link to do the next lab Introduction to Container Orchestration using Openshift : - Introduction to Container Orchestration using Openshift","title":"Introduction to Containerization"},{"location":"HCTEST/#introduction-to-containerization","text":"","title":"Introduction to Containerization"},{"location":"HCTEST/#table-of-contents","text":"Introduction to Containerization Table of Contents Background Prerequisites What is a Container Login to the VM Check your environment Run a pre-built image Build and Run Your Own Image Managing Image Versions Extra Credit Next","title":"Table of Contents"},{"location":"HCTEST/#background","text":"If you are expecting a lab about docker , you are at the right place. This lab will introduce you to the basic concepts of containerization, including: What are containers and container images How to start, stop, and remove containers. How to create container images How to version container images","title":"Background"},{"location":"HCTEST/#prerequisites","text":"You have podman or docker installed. Only docker is installed for this lab. You have access to the internet. You have cloned this lab from github. If not, follows these steps: git clone https://github.com/IBM/openshift-workshop-was.git cd openshift-workshop-was/labs/Openshift/HelloContainer","title":"Prerequisites"},{"location":"HCTEST/#what-is-a-container","text":"Compared to virtual machines, containers supports virtualization at the process level. Think of them as virtual processes. The isolation abstraction provided by the operating system makes the process think that it's running in its own virtual machine. As processes, containers may be created, started, and stopped much more quickly than virtual machines. Everything you need to run your application, from the operating system and up, is stored in a special file called a container image. Container images are self contained and portable. You may run one or more instances anywhere. And you don't have to worry about missing prerequisites, because all prerequisites are stored in the image. Container images are created via tools such as docker or podman . Existing images are hosted in container registries. For example, docker hub, or registry.access.redhat.com, or your own internal registry. If you need more background on containers: https://www.docker.com/resources/what-container","title":"What is a Container"},{"location":"HCTEST/#login-to-the-vm","text":"If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password: \\ Password: engageibm Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window.","title":"Login to the VM"},{"location":"HCTEST/#check-your-environment","text":"Open a terminal window from within your VM. List version of docker: docker --version Example output: Docker version 20.10.7, build f0df350 For more background on docker command line: https://docs.docker.com/engine/reference/commandline/cli/","title":"Check your environment"},{"location":"HCTEST/#run-a-pre-built-image","text":"Container images must be available locally before they can be run. To list available local images: docker images KLPREPOSITORY TAG IMAGE ID CREATED SIZE Images are hosted in container registries. The default container registry for docker is docker hub, located at https://hub.docker.com. Let's pull a test image from docker hub: docker pull openshift/hello-openshift And the output: Using default tag: latest latest: Pulling from openshift/hello-openshift 4f4fb700ef54: Pull complete 8b32988996c5: Pull complete Digest: sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e Status: Downloaded newer image for openshift/hello-openshift:latest docker.io/openshift/hello-openshift:latest List available local images again: docker images REPOSITORY TAG IMAGE ID CREATED SIZE openshift/hello-openshift latest 7af3297a3fb4 2 years ago 6.09MB Inspect the image metadata: docker inspect openshift/hello-openshift Note that: It exposes two ports: 8080 and 8888 It runs as user 1001 The entry point executable is /hello-openshift [ { \"Id\": \"sha256:7af3297a3fb4487b740ed6798163f618e6eddea1ee5fa0ba340329fcae31c8f6\", \"RepoTags\": [ \"openshift/hello-openshift:latest\" ], \"RepoDigests\": [ \"openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e\" ], ... \"Config\": { \"User\": \"1001\", ... \"ExposedPorts\": { \"8080/tcp\": {}, \"8888/tcp\": {} }, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], ... \"Entrypoint\": [ \"/hello-openshift\" ] ... } ] Run the image in an container: docker run --name hello1 -d -p 8080:8080 -p 8888:8888 openshift/hello-openshift Note that: The --name option gives the container a name. The -d option runs the command in the background as a daemon The -p option maps the port on the host to the port in the container. Through virtual networking, the port within the container is always the same for all running instances. But to support multiple concurrent running instances, the actual port on the host must be different for each instance. When you start the container, you may assign a new port on the host dynamically. The output of the command is the container ID for the running container. If the container starts successfully, the executable specified by the Entrypoint in the metadata is run. For our sample, it is /hello-openshift . Access the application in the container. Open the Firefox Web Browser from inside of the VM. Go to the URL http://localhost:8080 Also try port 8888 Run another instance of the same image. Note that this new instance is assigned new port numbers 8081 and 8889 on the host. This is so that they don't conflict with the ports 8080 and 8888 already allocated to the first instance. docker run --name hello2 -d -p 8081:8080 -p 8889:8888 openshift/hello-openshift Question: how does this compare to the time it takes to start a new virtual machine? Access the application in the new container the same way. Return to the Firefox Web Browser but instead go to the URL http://localhost:8081 Also try port 8889 Verify there are two containers running in the same host: docker ps : CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5a62f8527b44 openshift/hello-openshift \"/hello-openshift\" About a minute ago Up About a minute 0.0.0.0:8081->8080/tcp, 0.0.0.0:8889->8888/tcp hello2 c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 4 minutes ago Up 4 minutes 0.0.0.0:8080->8080/tcp, 0.0.0.0:8888->8888/tcp hello1 View the logs: docker logs hello1 And the output: ``` serving on 8888 serving on 8080 ``` View the logs on the second container: docker logs hello2 And the output: serving on 8888 serving on 8080 Note: within the container, each instance behaves as if it's running in its own virtual environment, and has opened the same ports. Outside of the container, different ports are opened. To export the file system of a running container: docker export hello1 > hello1.tar List the files on the file system: tar -tvf hello1.tar Note that this is a very small image. -rwxr-xr-x 0/0 0 2020-04-29 16:48 .dockerenv drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/ -rwxr-xr-x 0/0 0 2020-04-29 16:48 dev/console drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/pts/ drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/shm/ drwxr-xr-x 0/0 0 2020-04-29 16:48 etc/ -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/hostname -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/hosts lrwxrwxrwx 0/0 0 2020-04-29 16:48 etc/mtab -> /proc/mounts -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/resolv.conf -rwxr-xr-x 0/0 6089990 2018-04-18 10:22 hello-openshift drwxr-xr-x 0/0 0 2020-04-29 16:48 proc/ drwxr-xr-x 0/0 0 2020-04-29 16:48 sys/ Run another command in the running container. You can reach into the running container to run another command. The typical use case is to run a shell command, so you can use the shell to navigate within the container and run other commands. However, our image is tiny, and there is no built-in shell. For the purpose of this lab, we'll execute the same command again: docker exec -ti hello1 /hello-openshift . Running this command again in the same container results in an error, because there is already another copy running in the background that is bound to the ports 8080 and 8888: serving on 8888 serving on 8080 panic: ListenAndServe: listen tcp :8888: bind: address already in use ... Stop the containers: docker stop hello1 docker stop hello2 List running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES List all containers, including stopped ones: docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5a62f8527b44 openshift/hello-openshift \"/hello-openshift\" 28 minutes ago Exited (2) 28 seconds ago hello2 c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 31 minutes ago Exited (2) 32 seconds ago hello1 Restart a stopped container: docker restart hello1 List running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 33 minutes ago Up 8 seconds 0.0.0.0:8080->8080/tcp, 0.0.0.0:8888->8888/tcp hello1 Stop the container: docker stop hello1 Remove stopped containers, and note that there are no more containers: docker rm hello1 docker rm hello2 docker ps -a Remove the image from local cache: View current images: docker images Example output: REPOSITORY TAG IMAGE ID CREATED SIZE openshift/hello-openshift latest 7af3297a3fb4 2 years ago 6.09MB Remove the image: docker rmi openshift/hello-openshift Example output: Untagged: openshift/hello-openshift:latest Untagged: openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e Deleted: sha256:7af3297a3fb4487b740ed6798163f618e6eddea1ee5fa0ba340329fcae31c8f6 Deleted: sha256:8fd6a1ece3ceceae6d714004614bae5b581c83ab962d838ef88ce760583dcb80 Deleted: sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef Check that the image has been removed: docker images Example output: REPOSITORY TAG IMAGE ID CREATED SIZE","title":"Run a pre-built image"},{"location":"HCTEST/#build-and-run-your-own-image","text":"We use a Containerfile , which contains the instructions to create the new layers of your image. For those familiar with docker, the Containerfile is equivalent to Dockerfile . Recall that an image contains the entire file system that you want to use to run your virtual process in a container. For this sample, we are building a new image for a Java EE web application ServletApp.war. It is configured to run on the WebSphere Liberty Runtime. The configuration file for the server is in the server.xml. Change directory to openshift-workshop-was/labs/Openshift/HelloContainer cd openshift-workshop-was/labs/Openshift/HelloContainer Review the provided Containerfile from the directory: cat Containerfile Content of Containerfile : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi COPY server.xml /config COPY ServletApp.war /config/dropins/app.war RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml To create a new image, you start with a pre-existing image. The first line FROM specifies the existing image to be used as the base. If this is not in the local registry, it will be pulled from a remote registry such as docker hub. The base image we are using, ibmcom/websphere-liberty , is already prepackaged for us and made available on docker hub. The second line COPY is a straight copy of the file server.xml from the local directory to /config/server.xml in the image. This adds a new layer to the image with the actual server configuration to be used. The third line, another COPY , copies ServletApp.war from the current directory into a new layer in the image you are creating, at the location /config/dropins/app.war . The last line RUN runs the installUtility command within the image to install additional features required to run the server as specified in server.xml . You can use the RUN command to run any command that is available within the image to customize the image itself. Run the build. Ensure you include . at the end of the command (the dot indicates using the file from the current directory): docker build -t app -f Containerfile . The -t option tags the name of the image as app . The -f option specifies the name of the Containerfile . The build command runs the commands in Containerfile to build a new image called app . Example output: Sending build context to Docker daemon 25.6kB Step 1/4 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi kernel-java8-ibmjava-ubi: Pulling from ibmcom/websphere-liberty ee2244abc66f: Pull complete befb03b11956: Pull complete 137dc88f6a93: Pull complete 5bdd69a33184: Pull complete d4e2554981d7: Pull complete 32c91bc0f2e1: Pull complete db7e931336a9: Pull complete 3b32f9956ae2: Pull complete 304584ffa0a2: Pull complete 9f6da4c82b7e: Pull complete b6fa5b2e2325: Pull complete Digest: sha256:d76f79695afe2f653fc7b272f9a629105446e6b78ff0d733d494c93ff05728e7 Status: Downloaded newer image for ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> 4d9265befb26 Step 2/4 : COPY server.xml /config ---> 4a02d03d3725 Step 3/4 : COPY ServletApp.war /config/dropins/app.war ---> b2def2a0feac Step 4/4 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Running in 5f5b05aec1ae Checking for missing features required by the server ... The server requires the following additional features: appsecurity-2.0 servlet-3.0. Installing features from the repository ... Establishing a connection to the configured repositories ... This process might take several minutes to complete. Successfully connected to all configured repositories. Preparing assets for installation. This process might take several minutes to complete. Additional Liberty features must be installed for this server. To install the additional features, review and accept the feature license agreement: The --acceptLicense argument was found. This indicates that you have accepted the terms of the license agreement. Step 1 of 12: Downloading ssl-1.0 ... Step 2 of 12: Installing ssl-1.0 ... Step 3 of 12: Downloading appSecurity-2.0 ... Step 4 of 12: Installing appSecurity-2.0 ... Step 5 of 12: Downloading servlet-3.0 ... Step 6 of 12: Installing servlet-3.0 ... Step 7 of 12: Downloading jndi-1.0 ... Step 8 of 12: Installing jndi-1.0 ... Step 9 of 12: Downloading distributedMap-1.0 ... Step 10 of 12: Installing distributedMap-1.0 ... Step 11 of 12: Validating installed fixes ... Step 12 of 12: Cleaning up temporary files ... All assets were successfully installed. Start product validation... Product validation completed successfully. Removing intermediate container 5f5b05aec1ae ---> e1c6bfabda76 Successfully built e1c6bfabda76 Successfully tagged app:latest List the images to see that the new image app is built: docker images Note that the base image, ibmcom/websphere-liberty has also been pulled into the local registry. REPOSITORY TAG IMAGE ID CREATED SIZE app latest baa6bb9ad29d 2 minutes ago 544 MB ibmcom/websphere-liberty kernel-java8-ibmjava-ubi 7ea3d0a2b3fe 4 hours ago 544 MB Start the container. Note that you are running with both http and https ports: docker run -d -p 9080:9080 -p 9443:9443 --name=app-instance app Access the application running in the container: Open the Firefox Web Browswer and go to URL http://localhost:9080/app Check that it renders a page showing Simple Servlet ran successfully . Also point your browser to 9443: https://localhost:9443/app List the running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 595cdc49c710 app \"/opt/ibm/helpers/ru\u2026\" 8 minutes ago Up 8 minutes 0.0.0.0:9080->9080/tcp, 0.0.0.0:9443->9443/tcp app-instance Access the logs to your container: docker logs -f app-instance Use Ctrl-C To exit. Remote shell into your running container to poke around: docker exec -it app-instance /bin/sh In the shell session, run whoami and then run id , note you're not running as root. Note that this is a stripped down environment where many commands are not available. For example, try which ps . cd /logs to find the log files cd /liberty/wlp to find the location of the liberty install cd /liberty/wlp/usr/servers/defaultServer to find the server configuration. cd /opt/ibm/wlp/output/defaultServer to find the workarea files required by the server runtime. Exit from the container: exit Cleanup: docker stop app-instance docker rm app-instance","title":"Build and Run Your Own Image"},{"location":"HCTEST/#managing-image-versions","text":"There is no built-in versioning for container images. However, you may use a tagging convention to version your images. The convention is to use major.minor.patch , such as 1.3.5 . The default tag if you don't specify one is latest . Let's assume that the first version we will build for our environment is 1.3.5. (The earlier versions are built in a different environment.) Run the commands to tag the latest app image for our first version: docker tag app app:1 docker tag app app:1.3 docker tag app app:1.3.5 List the images again: docker images And the output: REPOSITORY TAG IMAGE ID CREATED SIZE app 1 d98cbdf82a0d 21 hours ago 542MB app 1.3 d98cbdf82a0d 21 hours ago 542MB app 1.3.5 d98cbdf82a0d 21 hours ago 542MB app latest d98cbdf82a0d 21 hours ago 542MB Note that all the different tags are currently associated with the same image, as they have the same image ID. After tagging, the command docker run app:<version> ... or docker pull app:<version> ... resolves the available versions as follows: app:1 resolves to the latest 1.x.x version, which in this case is 1.3.5 . app:1.3 resolves to the latest 1.3.x version, which in this case is the 1.3.5 app:1.3.5 resolves to the exact version 1.3.5 . After you build a new patch image containing defect fixes, you want to manage the tags for the new image so that a new docker run app:<version> ... or docker pull app:<version> ... command resolves the images as follows: app:1.3.5 : resolves to the existing 1.3.5 image. app:1.3.6 : resolves to the new image app:1.3 : resolves to the new image. app:1 : resolves to the new image Let's simulate a defect fix by building a new image using Containerfile1 instead of Containerfile : docker build -t app -f Containerfile1 . Example output: Sending build context to Docker daemon 32.26kB Step 1/5 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> bb79b9e26fd3 Step 2/5 : COPY server.xml /config ---> Using cache ---> f10659bc62b2 Step 3/5 : COPY ServletApp.war /config/dropins/app.war ---> Using cache ---> 24d85579e404 Step 4/5 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Using cache ---> 5e924d776a9c Step 5/5 : RUN echo test1 > /config/test1 ---> Running in 08e6135b00f5 Removing intermediate container 08e6135b00f5 ---> 69d20332a5e0 Successfully built 69d20332a5e0 Successfully tagged app:latest Tag it as follows : docker tag app app:1 docker tag app app:1.3 docker tag app app:1.3.6 Verify that these are the same images: app:1 , app:1.3 , app:1.3.6 . A new minor version involves compatible changes beyond just bug fixes. After you build a new minor version image, you want to manage the tags such that: app:1.3.5 : resolves to the existing 1.3.5 image. app:1.3.6 : resolves to the existing 1.3.6 image app:1.4.0 : resolves to the new image app:1.3 : resolves to the existing 1.3.6 image. app:1.4 : resolves to the new image. app:1 : resolves to the new image Build a new image using Containerfile2 : docker build -t app -f Containerfile2 . Sending build context to Docker daemon 32.26kB Step 1/6 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> bb79b9e26fd3 Step 2/6 : COPY server.xml /config ---> Using cache ---> f10659bc62b2 Step 3/6 : COPY ServletApp.war /config/dropins/app.war ---> Using cache ---> 24d85579e404 Step 4/6 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Using cache ---> 5e924d776a9c Step 5/6 : RUN echo test1 > /config/test1 ---> Using cache ---> 69d20332a5e0 Step 6/6 : RUN echo test2 > /config/test2 ---> Running in 96ea24b9ba66 Removing intermediate container 96ea24b9ba66 ---> 31b27169b3bc Successfully built 31b27169b3bc Tag it as follows: docker tag app app:1 docker tag app app:1.4 docker tag app app:1.4.0 Verify that 1 , 1.4 , and 1.4.0 are the same image 1.3 and 1.3.6 are the same image","title":"Managing Image Versions"},{"location":"HCTEST/#extra-credit","text":"Search the internet for information about multi-stage build. In a single stage build, the final image contains both build and runtime artifacts. A multi-stage build allows you to build with one base image, and copy the result of the build to another base image. This gives you even more control over the output of the build, and the size of the final image. Start another instances of the app image for vertical scaling, but with different port numbers on the host. Point your browser to hub.docker.com , click \"Explore\" and explore the millions of available images. Think about how you would tag a new image at a major version, 2.0.0 . Think about what would be required to manage containers across multiple machines to support horizontal scaling. Congratulations! You have completed the Introduction to Containerization lab.","title":"Extra Credit"},{"location":"HCTEST/#next","text":"Please follow the link to do the next lab Introduction to Container Orchestration using Openshift : - Introduction to Container Orchestration using Openshift","title":"Next"},{"location":"Intro/","text":"Why Operations Modernization Table of Contents Introduction WebSphere Constraints Requirements Driving Operations Modernization Openshift for Operations Modernization Introduction WebSphere Application Server is one of the most widely adopted application servers in the industry. Initially released in 1998, its current lineage may be traced back to V5.0, released in 2002. V5.0 was a major re-write that enabled it to continue to innovate and remain relevant to its users. Almost two decades later, as new requirements and new technologies emerged, WebSphere and its users must once make adjustments for operations modernization, to adopt cloud based technologies to meet emerging business requirements. Our focus will be on IBM Cloudpak for Applications and Openshift. IBM Cloudpak for Applications is an offering that includes both WebSphere Application Server and Openshift, enabling you deploy WebSphere Application Server both in your existing environment and to the Openshift. Openshift is a container based cloud derived from Kubernetes open source, the most popular contrainer orchestrator. Openshift adds certification, additional security, additional graphical user interface, and qualities of service on top of open source Kubernetes into an enterprise offering. In this chapter we will: - Review WebSphere infrastructure and its constraints. - List requirements driving changes in IT infrastructure towards operations modernization. - Explain why Cloudpak for Applications and Openshift are the technolog iesto adopt for operations modernization. WebSphere Constraints In this section, we review the constraints of the WebSphere Application Server and WebSphere Liberty environments. Traditional WebSphere Application Server A WebSphere cell is managed through a Deployment Manager, which is in control of one or more WebSphere nodes. After installation, the administrator uses either an administrative console or scripting for application or configuration management. The changes are stored on the deployment manager repository, then synchronized to the nodes. This design works well for traditional monolithic applications when the size of the environment is smaller, or when there are in-frequent changes to the environment. It does not work as well as the size of the environment increases, or as the frequency of administrative operations increases. Consider: item WAs Cell constraint size of cell 300 - 1000 JVMs size of installation 2 Gb deployment unit application create a new WAS cell > 30 minutes add a WAS node > 30 minutes create cluster seconds to 5 minutes add cluster member seconds to 5 minutes deploy application seconds to 1 hour start application > 30 seconds programming models Java/Jakarta EE, Spring configuration migration months or more application migration months or more Note that the time it takes to perform configuration operations gets progressive longer as the number of JVMs increases in the environment. It may take seconds to add a cluster member when there are only a few JVMs, but may take up to 5 minutes when there are 1000. In addition, the time required for application management also depends on the size of the application. There are multiple copies made when deploying the application to the deployment manager repository, or synchronizing the changes to the nodes. WebSphere Liberty WebSphere Liberty and Open Liberty uses much of the same runtime as traditional WebSphere. However, instead of storing configuration in a central repository, configurations are stored with individual Liberty Servers. One way to manage WebSphere Liberty is through the Collective Controller. It serves a similar purpose to the deployment manager. It can scale much higher due to the decentralized design, but its GUI or scripting interface is not as full function as that offered through the deployment manager. item Liberty Collective constraint size of collective 10000 JVMs size of install > 30 Mb deployment unit zip file create a new WAS collective seconds to minutes add a new node seconds create cluster seconds add cluster member seconds deploy application seconds start application > 3-5 seconds programming models Java/Jakarta EE, Spring configuration migration no migration appliation migration no migration WebSphere Liberty overcame many of the size and speed limitations of traditional WebSphere. However, as we will see in later sections, it still falls behind in terms of the scalability and administration features offered by Openshift. Requirements Driving Operations Modernization The following requirements are the factors driving infrastructure innovation and operations modernization: - Speed and agility of delivery - Scale of Infrastructure - Polyglot programming languages With respect to speed and agility of delivery: the faster you deliver new features, and the faster you adjust to changing requirements, the better an edge you have over your competitors. Instead of a waterfall development process where applications are updated with frequency measured in months or even years, some organizations can now deliver multiple times a day. To increase delivery frequency, organizations are adopting: - agile development process that reacts quickly to changing requirements, even on a daily basis. - Cloud based IT infrastructure that uses automation to enables extremely fast turn-around when delivering new features. With respect to scale of of infrastructure: it has grown enormously with increased usage via mobile applications, and increased number of users as rate of technology adoption increases. Some organizations are supporting hundreds of millions of users. Not only does the underlying infrastructure needs to support a large number of users, it must also support changing usage patterns. For example, paydays and holidays may place much higher demands on the infrastructure. The switch from large monolithic applications to smaller microservices also places higher demand on infrastructure, as each service is independently developed and tested. With respect to polyglot: application programmers now have multiple options, and increasingly they will choose the option that best fits their tasks. Examples of languages and runtimes include Java/Jakart EE, Spring, reactive, node, Ruby, Swift, and Python. An infrastructure capable of managing a polyglot environment provides a consistent management experience, ease of use, and cost savings compared to having to administer multiple environments. Cloudpak for Applications and Openshift for Operations Modernization IBM Cloudpak for applications and Openshift supports a much more scalable environment compared to both traditional WebSphere Application Server and WebSphere Liberty. The time required to perform operations and the scale of the environment is 10 to 100 times better compared to WebSphere. And it is inherently polyglot: item Openshift Contraints size of Openshift environment 100,000 pods deployment unit image create cluster seconds add cluster member seconds deploy image seconds start application same as underlying runtime programming models polyglot configuration migration minimal (for alpha or beta configuration) application migration runtime dependent Cloudpak for Applications and Openshift draws from the ongoing improvements made by the Kubernetes open source community, a community that spans than the resources of multiple organizations. On top of open source, it provides value adds, such as - certification of the environment - security - improved graphical user interface It also matches the qualities of service similar to those offered by WebSphere, such as: - auto-scaling - health management - application updates","title":"Why Operations Modernization"},{"location":"Intro/#why-operations-modernization","text":"","title":"Why Operations Modernization"},{"location":"Intro/#table-of-contents","text":"Introduction WebSphere Constraints Requirements Driving Operations Modernization Openshift for Operations Modernization","title":"Table of Contents"},{"location":"Intro/#introduction","text":"WebSphere Application Server is one of the most widely adopted application servers in the industry. Initially released in 1998, its current lineage may be traced back to V5.0, released in 2002. V5.0 was a major re-write that enabled it to continue to innovate and remain relevant to its users. Almost two decades later, as new requirements and new technologies emerged, WebSphere and its users must once make adjustments for operations modernization, to adopt cloud based technologies to meet emerging business requirements. Our focus will be on IBM Cloudpak for Applications and Openshift. IBM Cloudpak for Applications is an offering that includes both WebSphere Application Server and Openshift, enabling you deploy WebSphere Application Server both in your existing environment and to the Openshift. Openshift is a container based cloud derived from Kubernetes open source, the most popular contrainer orchestrator. Openshift adds certification, additional security, additional graphical user interface, and qualities of service on top of open source Kubernetes into an enterprise offering. In this chapter we will: - Review WebSphere infrastructure and its constraints. - List requirements driving changes in IT infrastructure towards operations modernization. - Explain why Cloudpak for Applications and Openshift are the technolog iesto adopt for operations modernization.","title":"Introduction"},{"location":"Intro/#websphere-constraints","text":"In this section, we review the constraints of the WebSphere Application Server and WebSphere Liberty environments.","title":"WebSphere Constraints"},{"location":"Intro/#traditional-websphere-application-server","text":"A WebSphere cell is managed through a Deployment Manager, which is in control of one or more WebSphere nodes. After installation, the administrator uses either an administrative console or scripting for application or configuration management. The changes are stored on the deployment manager repository, then synchronized to the nodes. This design works well for traditional monolithic applications when the size of the environment is smaller, or when there are in-frequent changes to the environment. It does not work as well as the size of the environment increases, or as the frequency of administrative operations increases. Consider: item WAs Cell constraint size of cell 300 - 1000 JVMs size of installation 2 Gb deployment unit application create a new WAS cell > 30 minutes add a WAS node > 30 minutes create cluster seconds to 5 minutes add cluster member seconds to 5 minutes deploy application seconds to 1 hour start application > 30 seconds programming models Java/Jakarta EE, Spring configuration migration months or more application migration months or more Note that the time it takes to perform configuration operations gets progressive longer as the number of JVMs increases in the environment. It may take seconds to add a cluster member when there are only a few JVMs, but may take up to 5 minutes when there are 1000. In addition, the time required for application management also depends on the size of the application. There are multiple copies made when deploying the application to the deployment manager repository, or synchronizing the changes to the nodes.","title":"Traditional WebSphere Application Server"},{"location":"Intro/#websphere-liberty","text":"WebSphere Liberty and Open Liberty uses much of the same runtime as traditional WebSphere. However, instead of storing configuration in a central repository, configurations are stored with individual Liberty Servers. One way to manage WebSphere Liberty is through the Collective Controller. It serves a similar purpose to the deployment manager. It can scale much higher due to the decentralized design, but its GUI or scripting interface is not as full function as that offered through the deployment manager. item Liberty Collective constraint size of collective 10000 JVMs size of install > 30 Mb deployment unit zip file create a new WAS collective seconds to minutes add a new node seconds create cluster seconds add cluster member seconds deploy application seconds start application > 3-5 seconds programming models Java/Jakarta EE, Spring configuration migration no migration appliation migration no migration WebSphere Liberty overcame many of the size and speed limitations of traditional WebSphere. However, as we will see in later sections, it still falls behind in terms of the scalability and administration features offered by Openshift.","title":"WebSphere Liberty"},{"location":"Intro/#requirements-driving-operations-modernization","text":"The following requirements are the factors driving infrastructure innovation and operations modernization: - Speed and agility of delivery - Scale of Infrastructure - Polyglot programming languages With respect to speed and agility of delivery: the faster you deliver new features, and the faster you adjust to changing requirements, the better an edge you have over your competitors. Instead of a waterfall development process where applications are updated with frequency measured in months or even years, some organizations can now deliver multiple times a day. To increase delivery frequency, organizations are adopting: - agile development process that reacts quickly to changing requirements, even on a daily basis. - Cloud based IT infrastructure that uses automation to enables extremely fast turn-around when delivering new features. With respect to scale of of infrastructure: it has grown enormously with increased usage via mobile applications, and increased number of users as rate of technology adoption increases. Some organizations are supporting hundreds of millions of users. Not only does the underlying infrastructure needs to support a large number of users, it must also support changing usage patterns. For example, paydays and holidays may place much higher demands on the infrastructure. The switch from large monolithic applications to smaller microservices also places higher demand on infrastructure, as each service is independently developed and tested. With respect to polyglot: application programmers now have multiple options, and increasingly they will choose the option that best fits their tasks. Examples of languages and runtimes include Java/Jakart EE, Spring, reactive, node, Ruby, Swift, and Python. An infrastructure capable of managing a polyglot environment provides a consistent management experience, ease of use, and cost savings compared to having to administer multiple environments.","title":"Requirements Driving Operations Modernization"},{"location":"Intro/#cloudpak-for-applications-and-openshift-for-operations-modernization","text":"IBM Cloudpak for applications and Openshift supports a much more scalable environment compared to both traditional WebSphere Application Server and WebSphere Liberty. The time required to perform operations and the scale of the environment is 10 to 100 times better compared to WebSphere. And it is inherently polyglot: item Openshift Contraints size of Openshift environment 100,000 pods deployment unit image create cluster seconds add cluster member seconds deploy image seconds start application same as underlying runtime programming models polyglot configuration migration minimal (for alpha or beta configuration) application migration runtime dependent Cloudpak for Applications and Openshift draws from the ongoing improvements made by the Kubernetes open source community, a community that spans than the resources of multiple organizations. On top of open source, it provides value adds, such as - certification of the environment - security - improved graphical user interface It also matches the qualities of service similar to those offered by WebSphere, such as: - auto-scaling - health management - application updates","title":"Cloudpak for Applications and Openshift for Operations Modernization"},{"location":"OpenshiftConcepts/","text":"Concepts for WebSphere Administrators Introduction High Level Architecture Configuration model Controller model and declarative specification Custom Resources and Operators Internal Networking Storage Immutable images Automation Comparision With Kubernetes Cloud Pak For Applications Introduction In this chapter, we introduce the most important concepts and processes in Openshift. The prerequisite is that you already familiar with the following concepts: image . container. container orchestration. If you are not, please refer to the previous chapter. High Level Architecture At a high level, there is quite a bit of similarity between the organization of WebSphere and Openshift: component openshift WebSphere ND WebSphere Liberty comment domain of management cluster cell collective management node master deployment manager collective controller Multiple instance HA on Openshift and Collective management end point API server dmgr collective controller management interface REST JMX REST+ JMX management model controller command command admin enhancement custom resource JMX JMX, REST configuration repository etcd deployment manager repository Liberty server.xml Configuration Model REST WebSphere Common Configuration Model (WCCM) XML where to run applications worker node node node worker agent kubelet node agent N/A unit of deployment container image application packaged server unit of runtime Pod JVM server JVM Server External Load Balancing Route or Ingress http server http server Internal load balancing Service N/A N/A firewall built-in N/A N/A Beyond the high level, there significant differences that enables Openshift to be more flexible, scalable, and highly available. The main differences for day to day administration are: REST based configuration model Controller based operations model Image based deployment Built-in firewall and networking Configuration Model Resources in Openshift are stored in REST format. Each resource type is identified by a group, version, and kind. Each resource instance for a resource type also has a unique name. A group that has no name (empty string) is built-in to Openshift. Here are examples of some resource types: group version kind description v1 Namespace for administration isolation v1 Pod for running containers v1 Deployment to deploy an image v1 ConfigMap configuration customization v1 Secret stores secrets for re-use later v1 Service logical endpoint for routing in the cluster v1 Route exposes endpoint access from outside of built-in firewall, Openshift specific v1 Ingress exposes endpoint access from outside of built-in firewall openliberty.io v1beta1 OpenLibertyApplication Deploy an Liberty image Openshift may be used to manage a very large environment. Namespaces are used to allow you to isolate disparate resources within the environment, e.g. between different teams, or applications. Resource instances may be scoped to a namespace, or be cluster wide if they are applicable to the entire cluster. In openshift, a pod is is the most basic unit of runtime to be managed. It is used to run one or more related containers. Normally you do not run define a pod . Instead, you define a deployment that controls the number of instances in pod . Here is a sample redacted configuration for a deployment for 2 pods: apiVersion: extensions/v1beta1 kind: Deployment metadata: name: example namespace: default spec: replicas: 2 selector: matchLabels: app: hello-openshift template: metadata: labels: app: hello-openshift spec: containers: - image: openshift/hello-openshift imagePullPolicy: Always name: hello-openshift ports: - containerPort: 8080 protocol: TCP status: availableReplicas: 2 conditions: - lastTransitionTime: \"2020-04-13T11:50:39Z\" lastUpdateTime: \"2020-04-13T11:50:39Z\" message: Deployment has minimum availability. reason: MinimumReplicasAvailable status: \"True\" type: Available - lastTransitionTime: \"2020-04-13T11:50:22Z\" lastUpdateTime: \"2020-04-13T11:50:39Z\" message: ReplicaSet \"example-75778c488\" has successfully progressed. reason: NewReplicaSetAvailable status: \"True\" type: Progressing observedGeneration: 1 readyReplicas: 2 replicas: 2 updatedReplicas: 2 Note the following: replicas: 2 : the specification is for 2 instances image: openshift/hello-openshift : specifies the image to run containerPort: 8080 : specifies the port it opens. readyReplicas: 2 : from the status field, shows there are 2 replicas ready to run. The controller for the above deployment follows the above specification, and creates two pods . Here is a sample redacted configuration for one of the pods : piVersion: v1 kind: Pod metadata: labels: app: hello-openshift pod-template-hash: 75778c488 name: example-75778c488-8wgsw namespace: default spec: containers: - image: openshift/hello-openshift imagePullPolicy: Always name: hello-openshift ports: - containerPort: 8080 protocol: TCP status: conditions: - lastProbeTime: null lastTransitionTime: \"2020-04-13T11:50:23Z\" status: \"True\" type: Initialized - lastProbeTime: null lastTransitionTime: \"2020-04-13T11:50:38Z\" status: \"True\" type: Ready - lastProbeTime: null lastTransitionTime: \"2020-04-13T11:50:38Z\" status: \"True\" type: ContainersReady - lastProbeTime: null lastTransitionTime: \"2020-04-13T11:50:23Z\" status: \"True\" type: PodScheduled containerStatuses: - containerID: cri-o://fd909c5a3f6cbf8cd9b430b39b3173c830c668daff739f8b425d9803755f11b3 image: docker.io/openshift/hello-openshift:latest imageID: docker.io/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e lastState: {} name: hello-openshift ready: true restartCount: 0 started: true state: running: startedAt: \"2020-04-13T11:50:37Z\" hostIP: 10.16.20.129 phase: Running podIP: 10.254.21.218 podIPs: - ip: 10.254.21.218 qosClass: BestEffort startTime: \"2020-04-13T11:50:23Z\" Note that: name: example-75778c488-8wgsw is generated by the controller for the deployment. Each pod it generates has a unique suffix. The name of the image and the port mirrors what you defined in the deployment . The controller for the pod is responsible for taking the specification to create a running pod within the Openshift cluster. The status sub-resource shows the current status of the pod . Controller Model and Declarative Specification The administration model for WebSphere is command based. You provide the step by step instructions either via a script or through interaction with the graphical console. On the other hand, almost all commands in Openshift is declarative. You provide the specification of what you want in REST, stored in either JSON or YAML, and Openshift will do its best to honor that specification. A good analogy to explain the controller model is a thermostat, which is an agent to control temperature. The controller compares the current room temperature with the desired temperature. If the temperatures don't agree, it initiates heating or cooling to bring the temperatures into agreement. Note that: The desired temperature and the room temperature may be constantly changing, and the thermostat will react to the changes. The thermostat does this on a best effort basis. There is no guarantee for how frequent or how long the room temperature matches the desired temperature. Similarly there is a controller for every resource type within Openshift. It monitors for changes as follows: resource added: performs the operations required to add the resource. For example, in the case of a pod, starts container(s) by finding where a new pod may be created, and fetching image(s) to run new containers within the pod. resource modified: Determines the differences between the current state of the resource, and the new desired state, and performs actions needed to bring the resource to the new desired state. For example, if any part of the pod specification is changed, such as image location, or environment variables, the controller determines what changes are required, and carries them out. resource deleted: Deletes the underlying resource. In the case of a pod, the hardware resources required to run the container(s) are release. Custom Resources and Operators For WebSphere administrators, custom resources are similar to custom JMX MBeans or custom services. They are separately installed to add to the capabilities of the default management infrastructure. The configuration and management model for custom resources are the same as existing resources. Custom resources have been used to add capabilities to install and manage: Databases Messaging infrastructure Logging and monitoring infrastructure Security services and much more In order to enhance Openshift with new custom resources, the definitions of custom resources and the controller code that handles the custom resources must first be installed. The controllers reside in container images, and must be configured to run on Openshift. The operator framework for Openshift is designed to manage the life cycle of a custom resource. An administrator may: use the Openshift console to find both certified and community (non-certified ) operators. Install the operator. This effectively installed a new product, such as a new database. Create new custom resources that are then managed by the controller shipped with the operator. For example, create new database instance, or perform database backup. Internal Networking Openshift comes installed with a firewall and internal software defined network. Applications are not visible outside of an Openshift cluster unless explicitly configured. You may define a service to load balance internal applications. A service leverages the internal kube proxy to determine how to route from the client to the server. Associated with each service is a DNS name. This means you may look up a service by its logical name and depend on the kube proxy to load balance between the running members. To expose an application outside of the Kubernetes cluster,use a route or ingress resource. The route resource is only available in Openshift. It was created before the Kubernetes ingress resource became available. The ingress resource is part of Kubernetes. The underlying technology used to implement a route or ingress is environment dependent. For on-premise cloud, your administrator may configure one or more load balancers that bridges the internal and external network. Public clouds will have their own networking infrastructure. Storage Conceptually a pod is indistinguishable from any other pod, and can be quickly created or destroyed at will. When a pod is destroyed, you lose all storage associated with that pod. However, there are cases where you may want to assign permanent storage to the pod. For stateful applications such as databases that require local storage. For debugging: you may want to take heap or thread dumps for a malfunctioning pod before it is killed Your Openshift administrator defines the availability of persistent storage through one or more Persistent Volume resources. These declare the amount of available storage. The underlying technology varies depending on what is available. For example, your administrator for on-premise cloud may be using NFS as the backing storage for persistent volumes , while public clouds will have their own storage services. When deploying an application, you define its persistent storage requirements via a persistent volume claim . The persistent volume claim requests a certain amount of storage with certain permissions. If the request can be met, the persistent volume claim is bound, and a file system path in the pod is created that maps to persistent storage. Immutable Images Use immutable images to ensure what your run in production is what you have tested, except for environment specific customizations. The above diagram shows a sample flow propagating an application through various stages: The developer develops and unit tests the application in the development environment. The developer environment is not shown in the diagram. After the code is committed to source repository, build automation initiates a build which Fetches source code from the source repository Pushes the application image to the image registry. The next stage in the flow is to deploy the application in the test stage. If the test succeeds, the application is propagated to the next stage, and so on. Note that in this scenario, namespaces are used to segregate the different stages. Usually there are multiple applications in the same Openshift cluster, in which case you may want to include the name of the application in the namespace, such as app1-build , app1-test , and app1-qa . Use configmaps or secrets to customize your image for the different stages. For our example, we are using a configmap to store the location of the database for the different environments. Use secrets if you need to store sensitive data, such as passwords and keys. The contents of configmaps and secrets may be mapped automatically to environment variables or local files for use by the application. Automation As a WebSphere administrator, you likely have been using wsadmin scripts in conjunction with other tools to automate your environment. With Openshift, there is no single tool that everyone uses. Therefore, in this section, we will only present central automation concepts, and list example tools. Infrastructure as Code With Openshift and declarative specification, you no longer need to write a script. You use .yaml or .json files. Like application code, these configuration files may be checked into source repository and versioned. In effect, the source repository is the source of truth of what you should be running. Furthermore, you may use additional tools to: automatically deploy new images into an environment Automatically re-deploy the configuration when a new version is available. Ensure that the configuration in environment reflects the configuration in the source repository. Any manual changes in the environment are automatically rolled back. Tools that support the above scenario are named gitops tools, because they connect git the source repository to ops , for operations. Example of tools for gitops include: Weaveworks Argo Razee Continuous Integration and Continuous Delivery Continuous integration means that your code is (almost) always ready to be committed into the the source repository. Continuous delivery means that your code is (almost) always ready to be rolled into production. You must have sufficient automation in place to make continuous integration and continuous delivery possible. Examples of tools that may be used to support continuous integration and continuous delivery include: Argo Spinnaker Jenkins Jenkin-X Tekton or Red Hat Pipelines Developer Environments Developer environment may be installed locally, such as via Eclipse or VSCode. Or they may be provisioned remotely, such as via codewind open source, or Red Hat Codeready workspaces. Providing IDE environments remotely has several advantages: No need to manage the developer desktop environment. The remote IDE is accessible through a browser. The same Openshift interface is used to manage the developer environment. Certified developer images may be developed once, after which it is very easy to create new environments for developers. Comparision_Kubernetes Kubernetes Openshift basics framework product installer support upstream OKD open source without support Usability basic dashboard Full Fledged UI kubectl oc, kubectl security configure your own Secure out of box Pods may not run as root by default built-in security to infrastructure, such Prometheus, Tekton, Jenkins, etc Additional features project: namespace + default security policy + quotas built-in image registry Prometheus, ELK Openshift Pipelines Codeready Workspaces WebSphere Hybrid Edition For those currently using WebSphere, the natural transition is from WebSphere Application Server to WebSphere Hybrid Edition. It enables you to run both traditional WebSphere workload and new containerized workloads on top of Opneshift. Both WebSphere Liberty and WebSphere Traditional Base container images are available.","title":"Openshift Concepts"},{"location":"OpenshiftConcepts/#concepts-for-websphere-administrators","text":"","title":"Concepts for WebSphere Administrators"},{"location":"OpenshiftConcepts/#introduction","text":"","title":"Introduction"},{"location":"OpenshiftConcepts/#high-level-architecture","text":"","title":"High Level Architecture"},{"location":"OpenshiftConcepts/#configuration-model","text":"","title":"Configuration Model"},{"location":"OpenshiftConcepts/#controller-model-and-declarative-specification","text":"","title":"Controller Model and Declarative Specification"},{"location":"OpenshiftConcepts/#custom-resources-and-operators","text":"","title":"Custom Resources and Operators"},{"location":"OpenshiftConcepts/#internal-networking","text":"","title":"Internal Networking"},{"location":"OpenshiftConcepts/#storage","text":"","title":"Storage"},{"location":"OpenshiftConcepts/#immutable-images","text":"","title":"Immutable Images"},{"location":"OpenshiftConcepts/#automation","text":"","title":"Automation"},{"location":"OpenshiftConcepts/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"OpenshiftConcepts/#continuous-integration-and-continuous-delivery","text":"","title":"Continuous Integration and Continuous Delivery"},{"location":"OpenshiftConcepts/#developer-environments","text":"","title":"Developer Environments"},{"location":"OpenshiftConcepts/#comparision_kubernetes","text":"","title":"Comparision_Kubernetes"},{"location":"OpenshiftConcepts/#websphere-hybrid-edition","text":"","title":"WebSphere Hybrid Edition"},{"location":"WebSphereCloud/","text":"Moving WebSphere Workload to Cloud Table of Contents Introduction Introduction to Container Based Cloud WebSphere and Openshift Comparison Options For moving WebSphere Workload Introduction In this chapter we will: - Introduce the reader to the basic concepts of container based cloud. - Compare the differences between WebSphere environment and Openshift Environment - List options to move WebSphere workload to cloud Introduction to Container Based Cloud As a container based cloud, the unit of deployment is an image, not an application. An image contains the complete file system of a machine, starting from the operating system. However, unlike a traditional virtual machine image, it can be started as a process on top of an existing operating system. An image built for WebSphere Application Server looks like: Image are named and layered. You may start from an existing layer, add one or more layers to build a new layer. Instead of installing WebSphere Application server multiple times, once for each node, you start with a pre-built WebSphere Application Server image shipped with the product, then add your own customizations. Or alternatively, you may start with an operating system image and install WebSphere Application Server to create a new image. This new image then serves as the basis from which to create additional images containing additional prerequisites, shared libraries, applications, and configurations. A container is used to run an image. Containers provide isolated environments at the process level. Within this isolated environment, the amount of resources, and visibility to the resources, such as cpu, memory, and disk can all be strictly controlled. In addition, due to the the process level isolation, containers can be created and destroyed much more quickly compared to virtual machines, as quickly as starting and stopping processes. To run just one or a few containers, a container runtime such as docker or podman may be used. However, to manage many containers across different nodes, you need a container orchestrator such as Openshift. Deployment through an container orchestrator looks looks like: A container orchestrator serves many of the same functions as the WebSphere deployment manager, except that the deployment unit is an image. It: - Creates running containers from images. - Allocates the containers across multiple physical nodes. - Monitors the health of the containers - Adjusts number of running containers based on load WebSphere Openshift Comparison In this section, we compare in detail WebSphere Application server cell, Liberty Collective, and IBM Cloudpak for Applications. WAS Cell Liberty Collective ICPA + Openshift topology max env 300-1000 JVMs 10,000 JVMs 100,000 pods disk 2GB per node up to 200 MB depends on underlying runtime firewall N N Y load balancer Y Y Y Performance initial heap > 100 MB > 30 MB depends on underlying runtime server start > 30s 3-5 sec depends on underlying runtime migration configuration migration months to years no migration minimal migration (of alpha or beta configuration) application migration months to hears no migration depends on underlying runtime high availability auto-scaling yes yes yes routing policy yes yes yes health management yes yes yes disaster recovery roll you own roll you rown roll your own administration admin model scripting + GUI scripting + GUI declarative deployment unit app zip file image app config separate in zip file in image infra config in infra in zip file source control + infra libraries separate in zip file in image app edition yes no yes (a/b) infra upgrade app + infra migration no migration depends on underlying runtime security admin roles 6+ 1-2 namespace + user defined admin audit yes yes yes (?) app security domain yes no (seperated by JVM) depends on underlying runtime certificate management some no yes? secret management password hash/encryption password hash/encryption underlying runtime + config map + external value security scans no no yes? problem determination health management yes yes yes centralized logging no no yes centralized metrics no no yes devops roll you own roll your own jenkins, Kabanero qualities of service in-memory session cache DRS, WXS WSX Redhat data grid built-in messaging SIBus yes (not HA) Redhat AMQ 2-phase transactions yes yes (not HA) yes (not HA) EJB clustering yes no(use local EJB) no( use local EJB) stateful session failover yes no no EJB timer failover yes no no EJB transaction propagation yes no depends on underlying runtime dynamic caching yes subset depends on underlying runtime programming models Java/Jakarta EE Java/Jakarta EE polyglot Spring Spring restrictions no JAX-RPC restrictions depends on underlying runtime Note that that some qualities of service are currently not available in Openshift, and have no suitable substitutes. You may need to re-architect your application. These include: - EJB timer failover - Stateful session bean failover - 2-phase transaction failover Options for Moving WebSphere Workload In this section, we will evaluate several options for operations modernization. The options are: - WebSphere Cell in VM - WebSphere Liberty in VM - WebSphere in Openshift WebSphere Cell in VM You may pre-install WebSphere onto a VM, then clone the VM to quickly create WAS cells. You may do this on your own, use VM Quickstarter, or use the WebSphere as a Service in IBM public cloud. Ultimately, this approach only enables you to quickly create new cells, which is useful if you need to frequently create new environments for development or testing, but it does not address the other performance or scalability issues with the cell: item WAs Cell constraint size of cell 300 - 1000 JVMs size of installation 2 Gb deployment unit application create a new WAS cell minutes add a WAS node minutes create cluster seconds to 5 minutes add cluster member seconds to 5 minutes deploy application seconds to hour start application > 30 seconds programming models Java/Jakarta EE, Spring configuration migration months or more application migration months or more WebSphere Liberty in VM You may also pre-install WebSphere Liberty onto a VM, then clone the VM to quickly create Liberty collectives. However, since Liberty is already designed for quick installation and fast startup, there are no significant savings using this approach. In addition, the scalability and administration issues still remain. WebSphere Cell in VM There are two recommended methods to run applications currently running in WebSphere in Openshift cloud: - using WebSphere Liberty image: recommended if the application already runs or can be port to Liberty. - using Websphere Base image: recommended if it is too difficult to port the application to Liberty. Note that it is also possible and supported to run applications using Tomcat or JBOSS images. However, running on either Liberty or Base gives you the most runtime compatibility for an application currently running on WebSphere. With Liberty or WebSphere Base, The scalability of the environment is that of the Kubernetes environment, not of the original cell or collective environment. The JVMs are stand-alone, not part of a collective or a cell. This also means that much of the qualities of service, such as clustering or failover, is provided by Openshift rather than WebSphere. It should be clear that Liberty has many advantages over WebSphere Base when running in Openshift cloud. However, there are also some programming model and qualities of service limitations that makes some applications difficult to port to Openshift or Liberty without code change, as listed in the last section.","title":"Moving WebSphere applications to Cloud"},{"location":"WebSphereCloud/#moving-websphere-workload-to-cloud","text":"","title":" Moving WebSphere Workload to Cloud"},{"location":"WebSphereCloud/#table-of-contents","text":"","title":"Table of Contents"},{"location":"WebSphereCloud/#introduction","text":"","title":"Introduction"},{"location":"WebSphereCloud/#introduction-to-container-based-cloud","text":"","title":"Introduction to Container Based Cloud"},{"location":"WebSphereCloud/#websphere-openshift-comparison","text":"","title":"WebSphere Openshift Comparison"},{"location":"WebSphereCloud/#options-for-moving-websphere-workload","text":"","title":"Options for Moving WebSphere Workload"},{"location":"WebSphereCloud/#websphere-cell-in-vm","text":"","title":"WebSphere Cell in VM"},{"location":"WebSphereCloud/#websphere-liberty-in-vm","text":"","title":"WebSphere Liberty in VM"},{"location":"WebSphereCloud/#websphere-cell-in-vm_1","text":"","title":"WebSphere Cell in VM"},{"location":"index - Copy (2)/","text":"WebSphere Application Modernization Workshop This material is designed to be a single point of access for the Demo/Lab assets for WebSphere Application Modernization. Last updated: July 2021 Application modernization is a journey of moving existing applications to a more modern cloud-native infrastructure. There are several approaches to application modernization and provided are key reference implementations to help you on your modernization journey. Operational Modernization Repackage the application to deploy within a container but maintaining a monolith application without changes to the application or runtime. This solution uses Appsody Operator, supported by IBM Cloud Pak for Applications, to deploy and manage the containerized application on Red Hat OpenShift. Runtime Modernization Update the application runtime to Open Liberty , a suitable cloud-native framework. Modernize some aspects of the application by taking advantage of MicroProfile specifications, such as Health, Metrics, JWT, Config and OpenAPI. This solution uses Open Liberty Operator, also supported by IBM Cloud Pak for Applications, to deploy and manage the modernized application on Red Hat OpenShift. Application Management Monitor the applications easily using various dashboards available as part of IBM Cloud Pak for Applications. Identify potential problems using logging dashboards. Get insights into the performance of your applications using metrics dashboards. Perform Day-2 operations, such as gathering traces and dumps, to debug potential issues easily using Open Liberty Operator. Devops Modernization Continuous Integration and Countinuous Deployment (CI/CD) of applications is critical for repeatable and reliable deployments. Modern devops technologies such as Tekton and Argo CD are designed for Kubernetes (OSCP) and provide the CI/CD capabilites, and fully integrated and supported in OSCP vis Kuberntes Operators. Hands-on Workshop Reserve lab environments Please follow the setup instructions to reserver a lab environemnt, or schedule a workshop. The lab environemtn is configured with the tools you need to be able to complete the labs. Containerization Labs Basic knowledge about containers, OpenShift, and BM Transfromation Advisor is recommended. Completing the following hands-on labs will help you to easily navigate this workshop: Lab 1: Introduction to Containerization Lab 2: Introduction to Container Orchestration using OpenShift Lab 3: Introduction to IBM Transformation Advisor Appplication Modernization Labs Lab 3: Operational Modernization Lab 4: Runtime Modernization Lab 5: Application Management DevOps Modernization Labs Lab 6: Openshift Pipelines Lab 7: GitOps with ArgoCD","title":"<h1 style=\"color:green\">WebSphere Application Modernization Workshop </h1>"},{"location":"index - Copy (2)/#websphere-application-modernization-workshop","text":"","title":"WebSphere Application Modernization Workshop "},{"location":"index - Copy/","text":"Application Modernization & Openshift Workshop for WebSphere Users This cookbook is designed to be a single point of access for the Demo/Lab assets for WebSphere Application Modernization. Last updated: July 2021 This information is designed to help with WebSphere user's journey in application and operations modernization, to transition to containers, Liberty, and Openshift. For administrators, the goals are: Present value of operations modernization from a WebSphere administrator's point of view. Explain new concepts and processes in Openshift Provide scenario-based guides that map scenarios in WebSphere to similar scenarios in Openshift. Provide corresponding scenario based hands-on labs to help WebSphere administrator become proficient in Openshift. For application developers, the goals are: Present the value of application modernization. Introduce the user to microservices and new programming models such microprofile. Introduce the user to the Liberty and Open Liberty as the runtime for Java microservices. Provide scenario based guides on development and containerization with Liberty. When available, you may use the material here for different venues: 1 - 2 hours presentation + short lab Half day workshop Full day workshop Potentially 2-day workshop The administrator sections are: Why Operations Modernization Options to Move WebSphere Workload to Cloud Openshift Concepts for WebSphere Administrators Migrating Applications Application and Configuration Management Logging, Monitoring, and Metrics High Availability and Scalability Devops and CI/CD Labs Containzerization Labs Introduction to Containerization . App Modernization Labs Introduction to Container Orchestration using Openshift Operational Modernization using traditional WebSphere images Runtime Modernization using Liberty images Tools to Manage Your Environment Liberty Labs These labs enable you to get started on Liberty quickly. First, start with these two labs to familiarize yourself with installing and navigating Liberty: Setup Liberty Discover Liberty Next, pick any lab for further exploration: KLP. UPDATE LIST to the 4 labs we created for the Linerty Bootcamp. Need to be converted to MarkDown. Simple Development Lab with JSP and JEE Security Migration Toolkit JDBC JMS Web Cache Web Socket JAXWS Java Batch Open ID Connect and JWT Monitoring and Debugging : With Jconsole and other tools. Packaging and Customizing Liberty : Options to package and run Liberty as a zip file, as a runnable JAR, or customize for containers. Finally, explore these additional labs: Micro Profile Open Liberty Guides","title":"Application Modernization & Openshift Workshop for WebSphere Users"},{"location":"index - Copy/#application-modernization-openshift-workshop-for-websphere-users","text":"This cookbook is designed to be a single point of access for the Demo/Lab assets for WebSphere Application Modernization. Last updated: July 2021 This information is designed to help with WebSphere user's journey in application and operations modernization, to transition to containers, Liberty, and Openshift. For administrators, the goals are: Present value of operations modernization from a WebSphere administrator's point of view. Explain new concepts and processes in Openshift Provide scenario-based guides that map scenarios in WebSphere to similar scenarios in Openshift. Provide corresponding scenario based hands-on labs to help WebSphere administrator become proficient in Openshift. For application developers, the goals are: Present the value of application modernization. Introduce the user to microservices and new programming models such microprofile. Introduce the user to the Liberty and Open Liberty as the runtime for Java microservices. Provide scenario based guides on development and containerization with Liberty. When available, you may use the material here for different venues: 1 - 2 hours presentation + short lab Half day workshop Full day workshop Potentially 2-day workshop The administrator sections are: Why Operations Modernization Options to Move WebSphere Workload to Cloud Openshift Concepts for WebSphere Administrators Migrating Applications Application and Configuration Management Logging, Monitoring, and Metrics High Availability and Scalability Devops and CI/CD","title":"Application Modernization &amp; Openshift Workshop for WebSphere Users"},{"location":"index - Copy/#labs","text":"","title":"Labs"},{"location":"index - Copy/#containzerization-labs","text":"Introduction to Containerization .","title":"Containzerization Labs"},{"location":"index - Copy/#app-modernization-labs","text":"Introduction to Container Orchestration using Openshift Operational Modernization using traditional WebSphere images Runtime Modernization using Liberty images Tools to Manage Your Environment","title":"App Modernization Labs"},{"location":"index - Copy/#liberty-labs","text":"These labs enable you to get started on Liberty quickly. First, start with these two labs to familiarize yourself with installing and navigating Liberty: Setup Liberty Discover Liberty Next, pick any lab for further exploration: KLP. UPDATE LIST to the 4 labs we created for the Linerty Bootcamp. Need to be converted to MarkDown. Simple Development Lab with JSP and JEE Security Migration Toolkit JDBC JMS Web Cache Web Socket JAXWS Java Batch Open ID Connect and JWT Monitoring and Debugging : With Jconsole and other tools. Packaging and Customizing Liberty : Options to package and run Liberty as a zip file, as a runnable JAR, or customize for containers. Finally, explore these additional labs: Micro Profile Open Liberty Guides","title":"Liberty Labs"},{"location":"resources/","text":"Resources and Documentation WebSphere Hybrid Edition Overview Documentation Installing components of WebShere Hybrid Edition Transformation Advisor Documentation Transformation Advisor Web Page Check out the Open Liberty Guides on developing microservice with ease using MicroProfile. Each guide takes 15-30 minutes and at the end of it, you would\u2019ve learned about a particular concept or technology! Check out the official Docker images for Open Liberty and WebSphere Traditional Base Learn more about Open Liberty Operator . Try other functionalities provided by the operator. Review the high level overview of key application modernization concepts available in the Application Modernization Field Guide and the IBM Architecture Center Application Modernization reference architecture . Other Application Modernization assets","title":"Additional Resources"},{"location":"resources/#resources-and-documentation","text":"","title":"Resources and Documentation"},{"location":"ApplicationManagement/","text":"Application Management Table of Contents Introduction Prerequisites Application Logging (Hands-on) Application Monitoring (Hands-on) Day-2 Operations (Hands-on) Summary Introduction In this lab, you'll learn about managing your running applications efficiently using various tools available to you as part of OpenShift and OperatorHub, including the Open Liberty Operator. Login to the VM If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password: \\ Password: engageibm Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window. Prerequisites Open a terminal window from the VM desktop. Login to OpenShift CLI with the oc login command from the web terminal. When prompted for the username and password, enter the following login credentials: Username: ibmadmin Password: engageibm If you have not yet cloned the GitHub repo with the lab artifacts, then run the following command on your terminal: git clone https://github.com/IBM/openshift-workshop-was.git Build and deploy the traditional WebSphere application (Hands-on) Change to the lab's directory: cd openshift-workshop-was/labs/Openshift/OperationalModernization Create and switch over to the project apps-was . > Note: The first step oc new-project may fail if the project already exists. If so, proceed to the next command. oc new-project apps-was oc project apps-was Build and deploy the application by running the commands in the following sequence. Reminder: the . at the end of the first command. docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was . docker login -u openshift -p $(oc whoami -t) default-route-openshift-image-registry.apps.demo.ibmdte.net docker push default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was oc apply -f deploy Example output: deployment.apps/cos-was created route.route.openshift.io/cos-was created secret/authdata created service/cos-was created Wait for the pod to be available, check status via oc get pod The output should be: NAME READY STATUS RESTARTS AGE cos-was-7d5ff6945-4hjzr 1/1 Running 0 88s Get the URL to the application: echo http://$(oc get route cos-was --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-was-apps-was.apps.demo.ibmdte.net/CustomerOrderServicesWeb Return to your Firefox browser window, and go to the URL outputted by the command run in the previous step. You will be prompted to login in order to access the application. Enter the following credentials: Username: skywalker Password: force After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total . Close the browser. Build and deploy the Liberty application (Skip this step if you just finished the previous lab Runtime Modernization and did not clean up the deployment) Note: The following steps are to help you re-deploy the Liberty application if the deployment has been deleted from the previous lab Runtime Modernization. The deployment is required to generate data for the follow-on steps in monitoring. From the top level directory, change to the lab's directory RuntimeModernization folder: cd openshift-workshop-was/labs/Openshift/RuntimeModernization Create and switch over to the project apps . Also, enable monitoring for the project. > Note: The first step oc new-project may fail if the project already exists. If so, proceed to the next command. oc new-project apps oc project apps oc label namespace apps app-monitoring=true Build and deploy the application by running the commands in the following sequence. Reminder: the . at the end of the first command: docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos . docker login -u openshift -p $(oc whoami -t) default-route-openshift-image-registry.apps.demo.ibmdte.net docker push default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos oc apply -k deploy/overlay-apps Example output: configmap/cos-config created secret/db-creds created secret/liberty-creds created openlibertyapplication.openliberty.io/cos created Verify the route for the application is created: oc get route cos Example output: NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD cos cos-apps.apps.demo.ibmdte.net cos 9443-tcp reencrypt/Redirect None Verify your pod is ready: oc get pod Example output: NAME READY STATUS RESTARTS AGE cos-54975b94c6-rh6kt 1/1 Running 0 3m11s Get the application URL: echo http://$(oc get route cos --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-apps.apps.demo.ibmdte.net/CustomerOrderServicesWeb Return to your Firefox browser window and go to the URL outputted by the command run in the previous step. You will be prompted to login in order to access the application. Enter the following credentials: Username: skywalker Password: force After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add multiple items to the shopping cart to trigger more logging. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total . Application Logging (Hands-on) Pod processes running in OpenShift frequently produce logs. To effectively manage this log data and ensure no loss of log data occurs when a pod terminates, a log aggregation tool should be deployed on the cluster. Log aggregation tools help users persist, search, and visualize the log data that is gathered from the pods across the cluster. Let's look at application logging with log aggregation using EFK (Elasticsearch, Fluentd, and Kibana). Elasticsearch is a search and analytics engine. Fluentd receives, cleans and parses the log data. Kibana lets users visualize data stored in Elasticsearch with charts and graphs. If it has been a long time (more than 15 minutes) since the Liberty or WebSphere pods last started, you may want to delete each pod and let a new one start, to ensure that Liberty and WebSphere create some recent logs for Kibana to find. Launch Kibana (Hands-on) In OpenShift console, from the left-panel, select Networking > Routes . From the Project drop-down list, select openshift-logging . Click on the route URL (listed under the Location column). Click on Log in with OpenShift . Click on Allow selected permissions . In Kibana console, you'll be prompted to create an index pattern. An index pattern tells Kibana what indices to look for in Elasticsearch. Type app so that the index pattern looks like this screenshot: You should see that your pattern matches at least one index. Then click Next step . Click the drop-down for Time Filter field name and choose @timestamp . Then click Create index pattern . You should see a number of fields populated, around 260. To check that the correct fields have been detected, type ibm in the Filter text box. You should see many fields beginning with the text ibm . If not, try clicking the refresh button (arrows in a circle) in the top right of the page. Import dashboards (Hands-on) Download this zip file containing dashboards to your computer and unzip to a local directory. (Look for the download button on the page.) Let's import dashboards for Liberty and WAS. From the left-panel, click on Management . Click on Saved Objects tab and then click on Import , then Import at the top of the panel that appears. Navigate to the kibana sub-directory and select ibm-open-liberty-kibana5-problems-dashboard.json file. Then click the Import button at the bottom of the panel. When prompted to resolve pattern conflicts, click select app* as the new index and click Confirm all changes . It'll take few seconds for the dashboard to import. Click Done when it finishes. Repeat the steps to import ibm-open-liberty-kibana5-traffic-dashboard.json and ibm-websphere-traditional-kibana5-dashboard.json . Explore dashboards (Hands-on) In Kibana console, from the left-panel, click on Dashboard . You'll see 3 dashboards on the list. The first 2 are for Liberty. The last one is for WAS traditional. Read the description next to each dashboard. Liberty applications (Hands-on) Click on Liberty-Problems-K5-20191122 . This dashboard visualizes message, trace and FFDC information from Liberty applications. By default, data from the last 15 minutes are rendered. Adjust the time-range (from the top-right corner), so that it includes data from when you tried the Open Liberty application. Once the data is rendered, you'll see some information about the namespace, pod, containers where events/problems occurred along with a count for each. Scroll down to Liberty Potential Problem Count section which lists the number of ERROR, FATAL, SystemErr and WARNING events. You'll likely see some WARNING events. Below that you'll see Liberty Top Message IDs . This helps to quickly identify most occurring events and their timeline. Scroll-up and click on the number below WARNING. Dashboard will change other panels to show just the events for warnings. Using this, you can determine: whether the failures occurred on one particular pod/server or in multiple instances, whether they occurred around the same or different time. Scroll-down to the actual warning messages. In this case some files from dojo were not found. Even though they are warnings, it'll be good to fix them by updating the application (we won't do that as part of this workshop). Go back to the list of dashboards and click on Liberty-Traffic-K5-20191122 . This dashboard helps to identify failing or slow HTTP requests on Liberty applications. As before, adjust the time-range as necessary if no data is rendered. You'll see some information about the namespace, pod, containers for the traffic along with a count for each. Scroll-down to Liberty Error Response Code Count section which lists the number of requests failed with HTTP response codes in 400s and 500s ranges. Scroll-down to Liberty Top URLs which lists the most frequently accessed URLs The /health and /metrics endpoints are running on the same server and are queried frequently for readiness/liveness probes and for scraping metrics information. It's possible to add a filter to include/exclude certain applications. On the right-hand side, you'll see list of endpoints that had the slowest response times. Scroll-up and click on the number listed below 400s. Dashboard will change other panels to show just the traffic with response code in 400s. You can see the timeline and the actual messages below. These are related to warnings from last dashboard about dojo files not being found (response code 404). Traditional WebSphere applications (Hands-on) Go back to the list of dashboards and click on WAS-traditional-Problems-K5-20190609 . Similar to the first dashboard for Liberty, this dashboard visualizes message and trace information for WebSphere Application Server traditional. As before, adjust the time-range as necessary if no data is rendered. Explore the panels and filter through the events to see messages corresponding to just those events. Application Monitoring (Hands-on) Building observability into applications externalizes the internal status of a system to enable operations teams to monitor systems more effectively. It is important that applications are written to produce metrics. When the Customer Order Services application was modernized, we used MicroProfile Metrics and it provides a /metrics endpoint from where you can access all metrics emitted by the JVM, Open Liberty server and deployed applications. Operations teams can gather the metrics and store them in a database by using tools like Prometheus. The metrics data can then be visualized and analyzed in dashboards, such as Grafana. Grafana dashboard (Hands-on) Custom resource GrafanaDashboard defines a set of dashboards for monitoring Customer Order Services application and Open Liberty. In your terminal, run the following command to create the dashboard resource: Before running the command, change directory to /openshift-workshop-was/labs/Openshift/ApplicationManagement if it's not already done. oc apply -f dashboards/grafana/grafana-dashboard-cos.yaml The following steps to access the created dashboard are illustrated in the screen recording at the end of this section: In OpenShift console, from the left-panel, select Networking > Routes . From the Project drop-down list, select app-monitoring . Click on the route URL (listed under the Location column). Click on Log in with OpenShift . Click on Allow selected permissions . In Grafana, from the left-panel, hover over the dashboard icon and click on Manage . You should see Liberty-Metrics-Dashboard on the list. Click on it. Explore the dashboards. The first 2 are for Customer Order Services application. The rest are for Liberty. Click on Customer Order Services - Shopping Cart . By default, it'll show the data for the last 15 minutes. Adjust the time-range from the top-right as necessary. You should see the frequency of requests, number of requests, pod information, min/max request times. Scroll-down to expand the CPU section. You'll see information about process CPU time, CPU system load for pods. Scroll-down to expand the Servlets section. You'll see request count and response times for application servlet as well as health and metrics endpoints. Explore the other sections. Day-2 Operations (Hands-on) You may need to gather server traces and/or dumps for analyzing some problems. Open Liberty Operator makes it easy to gather these on a server running inside a container. A storage must be configured so the generated artifacts can persist, even after the Pod is deleted. This storage can be shared by all instances of the Open Liberty applications. RedHat OpenShift on IBM Cloud utilizes the storage capabilities provided by IBM Cloud. Let's create a request for storage. Request storage (Hands-on) In OpenShift console, from the left-panel, select Storage > Persistent Volume Claims . From the Project drop-down list, select apps . Click on Create Persistent Volume Claim button. Ensure that Storage Class is managed-nfs . If not, make the selection from the list. Enter liberty for Persistent Volume Claim Name field. Request 1 GiB by entering 1 in the text box for Size . Click on Create . Created Persistent Volume Claim will be displayed. The Status field would display Pending . Wait for it to change to Bound . It may take 1-2 minutes. Once bound, you should see the volume displayed under Persistent Volume field. Enable serviceability (Hands-on) Enable serviceability option for the Customer Order Services application. In productions systems, it's recommended that you do this step with the initial deployment of the application - not when you encounter an issue and need to gather server traces or dumps. OpenShift cannot attach volumes to running Pods so it'll have to create a new Pod, attach the volume and then take down the old Pod. If the problem is intermittent or hard to reproduce, you may not be able to reproduce it on the new instance of server running in the new Pod. The volume can be shared by all Liberty applications that are in the same namespace and the volumes wouldn't be used unless you perform day-2 operation on a particular application - so that should make it easy to enable serviceability with initial deployment. Specify the name of the storage request (Persistent Volume Claim) you made earlier to spec.serviceability.volumeClaimName parameter provided by OpenLibertyApplication custom resource. Open Liberty Operator will attach the volume bound to the claim to each instance of the server. In your terminal, run the following command: oc patch olapp cos -n apps --patch '{\"spec\":{\"serviceability\":{\"volumeClaimName\":\"liberty\"}}}' --type=merge Above command patches the definition of olapp (shortname for OpenLibertyApplication ) instance cos in namespace apps (indicated by -n option). The --patch option specifies the content to patch with. In this case, we set the value of spec.serviceability.volumeClaimName field to liberty , which is the name of the Persistent Volume Claim you created earlier. The --type=merge option specifies to merge the previous content with newly specified field and its value. Run the following command to get the status of cos application, to verify that the changes were reconciled and there is no error: oc get olapp cos -n apps -o wide The value under RECONCILED column should be True . Note: If it's False then an error occurred. The REASON and MESSAGE columns will display the cause of the failure. A common mistake is creating the Persistent Volume Claim in another namespace. Ensure that it is created in the apps namespace. In OpenShift console, from the left-panel, click on Workloads > Pods . Wait until there is only 1 pod on the list and its Readiness column changed to Ready . Pod's name is needed for requesting server dump and trace in the next sections. Click on the pod and copy the value under Name field. Request server dump (Hands-on) You can request a snapshot of the server status including different types of server dumps, from an instance of Open Liberty server running inside a Pod, using Open Liberty Operator and OpenLibertyDump custom resource (CR). The following steps to request a server dump are illustrated in the screen recording below: From the left-panel, click on Operators > Installed Operators . From the Open Liberty Operator row, click on Open Liberty Dump (displayed under Provided APIs column). Click on Create OpenLibertyDump button. Replace Specify_Pod_Name_Here with the pod name you copied earlier. The include field specifies the type of server dumps to request. Heap and thread dumps are specified by default. Let's use the default values. Click on Create . Click on example-dump from the list. Scroll-down to the Conditions section and you should see Started status with True value. Wait for the operator to complete the dump operation. You should see status Completed with True value. Request server traces (Hands-on) You can also request server traces, from an instance of Open Liberty server running inside a Pod, using OpenLibertyTrace custom resource (CR). The following steps to request a server trace are illustrated in the screen recording below: From the left-panel, click on Operators > Installed Operators . From the Open Liberty Operator row, click on Open Liberty Trace . Click on Create OpenLibertyTrace button. Replace Specify_Pod_Name_Here with the pod name you copied earlier. The traceSpecification field specifies the trace string to be used to selectively enable trace on Liberty server. Let's use the default value. Click on Create . Click on example-trace from the list. Scroll-down to the Conditions section and you should see Enabled status with True value. Note: Once the trace has started, it can be stopped by setting the disable parameter to true. Deleting the CR will also stop the tracing. Changing the podName will first stop the tracing on the old Pod before enabling traces on the new Pod. Maximum trace file size (in MB) and the maximum number of files before rolling over can be specified using maxFileSize and maxFiles parameters. Accessing the generated files (Hands-on) The generated trace and dump files should now be in the persistent volume. You used storage from IBM Cloud and we have to go through a number of steps using a different tool to access those files. Since the volume is attached to the Pod, we can instead use Pod's terminal to easily verify that trace and dump files are present. The following steps to access the files are illustrated in the screen recording below: Remote shell to your pod via one of two ways: From your terminal: oc rsh <pod-name> From console: click on Workloads > Pods . Click on the pod and then click on Terminal tab. Enter ls -R serviceability/apps to list the files. The shared volume is mounted at serviceability folder. The sub-folder apps is the namespace of the Pod. You should see a zip file for dumps and trace log files. These are produced by the day-2 operations you performed. Summary Congratulations! You've completed Application Management lab!","title":"Application Management"},{"location":"ApplicationManagement/#application-management","text":"","title":"Application Management"},{"location":"ApplicationManagement/#table-of-contents","text":"Introduction Prerequisites Application Logging (Hands-on) Application Monitoring (Hands-on) Day-2 Operations (Hands-on) Summary","title":"Table of Contents"},{"location":"ApplicationManagement/#introduction","text":"In this lab, you'll learn about managing your running applications efficiently using various tools available to you as part of OpenShift and OperatorHub, including the Open Liberty Operator.","title":"Introduction"},{"location":"ApplicationManagement/#login-to-the-vm","text":"If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password: \\ Password: engageibm Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window.","title":"Login to the VM"},{"location":"ApplicationManagement/#prerequisites","text":"Open a terminal window from the VM desktop. Login to OpenShift CLI with the oc login command from the web terminal. When prompted for the username and password, enter the following login credentials: Username: ibmadmin Password: engageibm If you have not yet cloned the GitHub repo with the lab artifacts, then run the following command on your terminal: git clone https://github.com/IBM/openshift-workshop-was.git","title":"Prerequisites"},{"location":"ApplicationManagement/#build-and-deploy-the-traditional-websphere-application-hands-on","text":"Change to the lab's directory: cd openshift-workshop-was/labs/Openshift/OperationalModernization Create and switch over to the project apps-was . > Note: The first step oc new-project may fail if the project already exists. If so, proceed to the next command. oc new-project apps-was oc project apps-was Build and deploy the application by running the commands in the following sequence. Reminder: the . at the end of the first command. docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was . docker login -u openshift -p $(oc whoami -t) default-route-openshift-image-registry.apps.demo.ibmdte.net docker push default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was oc apply -f deploy Example output: deployment.apps/cos-was created route.route.openshift.io/cos-was created secret/authdata created service/cos-was created Wait for the pod to be available, check status via oc get pod The output should be: NAME READY STATUS RESTARTS AGE cos-was-7d5ff6945-4hjzr 1/1 Running 0 88s Get the URL to the application: echo http://$(oc get route cos-was --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-was-apps-was.apps.demo.ibmdte.net/CustomerOrderServicesWeb Return to your Firefox browser window, and go to the URL outputted by the command run in the previous step. You will be prompted to login in order to access the application. Enter the following credentials: Username: skywalker Password: force After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total . Close the browser.","title":"Build and deploy the traditional WebSphere application (Hands-on)"},{"location":"ApplicationManagement/#build-and-deploy-the-liberty-application","text":"(Skip this step if you just finished the previous lab Runtime Modernization and did not clean up the deployment) Note: The following steps are to help you re-deploy the Liberty application if the deployment has been deleted from the previous lab Runtime Modernization. The deployment is required to generate data for the follow-on steps in monitoring. From the top level directory, change to the lab's directory RuntimeModernization folder: cd openshift-workshop-was/labs/Openshift/RuntimeModernization Create and switch over to the project apps . Also, enable monitoring for the project. > Note: The first step oc new-project may fail if the project already exists. If so, proceed to the next command. oc new-project apps oc project apps oc label namespace apps app-monitoring=true Build and deploy the application by running the commands in the following sequence. Reminder: the . at the end of the first command: docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos . docker login -u openshift -p $(oc whoami -t) default-route-openshift-image-registry.apps.demo.ibmdte.net docker push default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos oc apply -k deploy/overlay-apps Example output: configmap/cos-config created secret/db-creds created secret/liberty-creds created openlibertyapplication.openliberty.io/cos created Verify the route for the application is created: oc get route cos Example output: NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD cos cos-apps.apps.demo.ibmdte.net cos 9443-tcp reencrypt/Redirect None Verify your pod is ready: oc get pod Example output: NAME READY STATUS RESTARTS AGE cos-54975b94c6-rh6kt 1/1 Running 0 3m11s Get the application URL: echo http://$(oc get route cos --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-apps.apps.demo.ibmdte.net/CustomerOrderServicesWeb Return to your Firefox browser window and go to the URL outputted by the command run in the previous step. You will be prompted to login in order to access the application. Enter the following credentials: Username: skywalker Password: force After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add multiple items to the shopping cart to trigger more logging. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total .","title":"Build and deploy the Liberty application"},{"location":"ApplicationManagement/#application-logging-hands-on","text":"Pod processes running in OpenShift frequently produce logs. To effectively manage this log data and ensure no loss of log data occurs when a pod terminates, a log aggregation tool should be deployed on the cluster. Log aggregation tools help users persist, search, and visualize the log data that is gathered from the pods across the cluster. Let's look at application logging with log aggregation using EFK (Elasticsearch, Fluentd, and Kibana). Elasticsearch is a search and analytics engine. Fluentd receives, cleans and parses the log data. Kibana lets users visualize data stored in Elasticsearch with charts and graphs. If it has been a long time (more than 15 minutes) since the Liberty or WebSphere pods last started, you may want to delete each pod and let a new one start, to ensure that Liberty and WebSphere create some recent logs for Kibana to find.","title":"Application Logging (Hands-on)"},{"location":"ApplicationManagement/#launch-kibana-hands-on","text":"In OpenShift console, from the left-panel, select Networking > Routes . From the Project drop-down list, select openshift-logging . Click on the route URL (listed under the Location column). Click on Log in with OpenShift . Click on Allow selected permissions . In Kibana console, you'll be prompted to create an index pattern. An index pattern tells Kibana what indices to look for in Elasticsearch. Type app so that the index pattern looks like this screenshot: You should see that your pattern matches at least one index. Then click Next step . Click the drop-down for Time Filter field name and choose @timestamp . Then click Create index pattern . You should see a number of fields populated, around 260. To check that the correct fields have been detected, type ibm in the Filter text box. You should see many fields beginning with the text ibm . If not, try clicking the refresh button (arrows in a circle) in the top right of the page.","title":"Launch Kibana (Hands-on)"},{"location":"ApplicationManagement/#import-dashboards-hands-on","text":"Download this zip file containing dashboards to your computer and unzip to a local directory. (Look for the download button on the page.) Let's import dashboards for Liberty and WAS. From the left-panel, click on Management . Click on Saved Objects tab and then click on Import , then Import at the top of the panel that appears. Navigate to the kibana sub-directory and select ibm-open-liberty-kibana5-problems-dashboard.json file. Then click the Import button at the bottom of the panel. When prompted to resolve pattern conflicts, click select app* as the new index and click Confirm all changes . It'll take few seconds for the dashboard to import. Click Done when it finishes. Repeat the steps to import ibm-open-liberty-kibana5-traffic-dashboard.json and ibm-websphere-traditional-kibana5-dashboard.json .","title":"Import dashboards (Hands-on)"},{"location":"ApplicationManagement/#explore-dashboards-hands-on","text":"In Kibana console, from the left-panel, click on Dashboard . You'll see 3 dashboards on the list. The first 2 are for Liberty. The last one is for WAS traditional. Read the description next to each dashboard.","title":"Explore dashboards (Hands-on)"},{"location":"ApplicationManagement/#liberty-applications-hands-on","text":"Click on Liberty-Problems-K5-20191122 . This dashboard visualizes message, trace and FFDC information from Liberty applications. By default, data from the last 15 minutes are rendered. Adjust the time-range (from the top-right corner), so that it includes data from when you tried the Open Liberty application. Once the data is rendered, you'll see some information about the namespace, pod, containers where events/problems occurred along with a count for each. Scroll down to Liberty Potential Problem Count section which lists the number of ERROR, FATAL, SystemErr and WARNING events. You'll likely see some WARNING events. Below that you'll see Liberty Top Message IDs . This helps to quickly identify most occurring events and their timeline. Scroll-up and click on the number below WARNING. Dashboard will change other panels to show just the events for warnings. Using this, you can determine: whether the failures occurred on one particular pod/server or in multiple instances, whether they occurred around the same or different time. Scroll-down to the actual warning messages. In this case some files from dojo were not found. Even though they are warnings, it'll be good to fix them by updating the application (we won't do that as part of this workshop). Go back to the list of dashboards and click on Liberty-Traffic-K5-20191122 . This dashboard helps to identify failing or slow HTTP requests on Liberty applications. As before, adjust the time-range as necessary if no data is rendered. You'll see some information about the namespace, pod, containers for the traffic along with a count for each. Scroll-down to Liberty Error Response Code Count section which lists the number of requests failed with HTTP response codes in 400s and 500s ranges. Scroll-down to Liberty Top URLs which lists the most frequently accessed URLs The /health and /metrics endpoints are running on the same server and are queried frequently for readiness/liveness probes and for scraping metrics information. It's possible to add a filter to include/exclude certain applications. On the right-hand side, you'll see list of endpoints that had the slowest response times. Scroll-up and click on the number listed below 400s. Dashboard will change other panels to show just the traffic with response code in 400s. You can see the timeline and the actual messages below. These are related to warnings from last dashboard about dojo files not being found (response code 404).","title":"Liberty applications (Hands-on)"},{"location":"ApplicationManagement/#traditional-websphere-applications-hands-on","text":"Go back to the list of dashboards and click on WAS-traditional-Problems-K5-20190609 . Similar to the first dashboard for Liberty, this dashboard visualizes message and trace information for WebSphere Application Server traditional. As before, adjust the time-range as necessary if no data is rendered. Explore the panels and filter through the events to see messages corresponding to just those events.","title":"Traditional WebSphere applications (Hands-on)"},{"location":"ApplicationManagement/#application-monitoring-hands-on","text":"Building observability into applications externalizes the internal status of a system to enable operations teams to monitor systems more effectively. It is important that applications are written to produce metrics. When the Customer Order Services application was modernized, we used MicroProfile Metrics and it provides a /metrics endpoint from where you can access all metrics emitted by the JVM, Open Liberty server and deployed applications. Operations teams can gather the metrics and store them in a database by using tools like Prometheus. The metrics data can then be visualized and analyzed in dashboards, such as Grafana.","title":"Application Monitoring (Hands-on)"},{"location":"ApplicationManagement/#grafana-dashboard-hands-on","text":"Custom resource GrafanaDashboard defines a set of dashboards for monitoring Customer Order Services application and Open Liberty. In your terminal, run the following command to create the dashboard resource: Before running the command, change directory to /openshift-workshop-was/labs/Openshift/ApplicationManagement if it's not already done. oc apply -f dashboards/grafana/grafana-dashboard-cos.yaml The following steps to access the created dashboard are illustrated in the screen recording at the end of this section: In OpenShift console, from the left-panel, select Networking > Routes . From the Project drop-down list, select app-monitoring . Click on the route URL (listed under the Location column). Click on Log in with OpenShift . Click on Allow selected permissions . In Grafana, from the left-panel, hover over the dashboard icon and click on Manage . You should see Liberty-Metrics-Dashboard on the list. Click on it. Explore the dashboards. The first 2 are for Customer Order Services application. The rest are for Liberty. Click on Customer Order Services - Shopping Cart . By default, it'll show the data for the last 15 minutes. Adjust the time-range from the top-right as necessary. You should see the frequency of requests, number of requests, pod information, min/max request times. Scroll-down to expand the CPU section. You'll see information about process CPU time, CPU system load for pods. Scroll-down to expand the Servlets section. You'll see request count and response times for application servlet as well as health and metrics endpoints. Explore the other sections.","title":"Grafana dashboard (Hands-on)"},{"location":"ApplicationManagement/#day-2-operations-hands-on","text":"You may need to gather server traces and/or dumps for analyzing some problems. Open Liberty Operator makes it easy to gather these on a server running inside a container. A storage must be configured so the generated artifacts can persist, even after the Pod is deleted. This storage can be shared by all instances of the Open Liberty applications. RedHat OpenShift on IBM Cloud utilizes the storage capabilities provided by IBM Cloud. Let's create a request for storage.","title":"Day-2 Operations (Hands-on)"},{"location":"ApplicationManagement/#request-storage-hands-on","text":"In OpenShift console, from the left-panel, select Storage > Persistent Volume Claims . From the Project drop-down list, select apps . Click on Create Persistent Volume Claim button. Ensure that Storage Class is managed-nfs . If not, make the selection from the list. Enter liberty for Persistent Volume Claim Name field. Request 1 GiB by entering 1 in the text box for Size . Click on Create . Created Persistent Volume Claim will be displayed. The Status field would display Pending . Wait for it to change to Bound . It may take 1-2 minutes. Once bound, you should see the volume displayed under Persistent Volume field.","title":"Request storage (Hands-on)"},{"location":"ApplicationManagement/#enable-serviceability-hands-on","text":"Enable serviceability option for the Customer Order Services application. In productions systems, it's recommended that you do this step with the initial deployment of the application - not when you encounter an issue and need to gather server traces or dumps. OpenShift cannot attach volumes to running Pods so it'll have to create a new Pod, attach the volume and then take down the old Pod. If the problem is intermittent or hard to reproduce, you may not be able to reproduce it on the new instance of server running in the new Pod. The volume can be shared by all Liberty applications that are in the same namespace and the volumes wouldn't be used unless you perform day-2 operation on a particular application - so that should make it easy to enable serviceability with initial deployment. Specify the name of the storage request (Persistent Volume Claim) you made earlier to spec.serviceability.volumeClaimName parameter provided by OpenLibertyApplication custom resource. Open Liberty Operator will attach the volume bound to the claim to each instance of the server. In your terminal, run the following command: oc patch olapp cos -n apps --patch '{\"spec\":{\"serviceability\":{\"volumeClaimName\":\"liberty\"}}}' --type=merge Above command patches the definition of olapp (shortname for OpenLibertyApplication ) instance cos in namespace apps (indicated by -n option). The --patch option specifies the content to patch with. In this case, we set the value of spec.serviceability.volumeClaimName field to liberty , which is the name of the Persistent Volume Claim you created earlier. The --type=merge option specifies to merge the previous content with newly specified field and its value. Run the following command to get the status of cos application, to verify that the changes were reconciled and there is no error: oc get olapp cos -n apps -o wide The value under RECONCILED column should be True . Note: If it's False then an error occurred. The REASON and MESSAGE columns will display the cause of the failure. A common mistake is creating the Persistent Volume Claim in another namespace. Ensure that it is created in the apps namespace. In OpenShift console, from the left-panel, click on Workloads > Pods . Wait until there is only 1 pod on the list and its Readiness column changed to Ready . Pod's name is needed for requesting server dump and trace in the next sections. Click on the pod and copy the value under Name field.","title":"Enable serviceability (Hands-on)"},{"location":"ApplicationManagement/#request-server-dump-hands-on","text":"You can request a snapshot of the server status including different types of server dumps, from an instance of Open Liberty server running inside a Pod, using Open Liberty Operator and OpenLibertyDump custom resource (CR). The following steps to request a server dump are illustrated in the screen recording below: From the left-panel, click on Operators > Installed Operators . From the Open Liberty Operator row, click on Open Liberty Dump (displayed under Provided APIs column). Click on Create OpenLibertyDump button. Replace Specify_Pod_Name_Here with the pod name you copied earlier. The include field specifies the type of server dumps to request. Heap and thread dumps are specified by default. Let's use the default values. Click on Create . Click on example-dump from the list. Scroll-down to the Conditions section and you should see Started status with True value. Wait for the operator to complete the dump operation. You should see status Completed with True value.","title":"Request server dump (Hands-on)"},{"location":"ApplicationManagement/#request-server-traces-hands-on","text":"You can also request server traces, from an instance of Open Liberty server running inside a Pod, using OpenLibertyTrace custom resource (CR). The following steps to request a server trace are illustrated in the screen recording below: From the left-panel, click on Operators > Installed Operators . From the Open Liberty Operator row, click on Open Liberty Trace . Click on Create OpenLibertyTrace button. Replace Specify_Pod_Name_Here with the pod name you copied earlier. The traceSpecification field specifies the trace string to be used to selectively enable trace on Liberty server. Let's use the default value. Click on Create . Click on example-trace from the list. Scroll-down to the Conditions section and you should see Enabled status with True value. Note: Once the trace has started, it can be stopped by setting the disable parameter to true. Deleting the CR will also stop the tracing. Changing the podName will first stop the tracing on the old Pod before enabling traces on the new Pod. Maximum trace file size (in MB) and the maximum number of files before rolling over can be specified using maxFileSize and maxFiles parameters.","title":"Request server traces (Hands-on)"},{"location":"ApplicationManagement/#accessing-the-generated-files-hands-on","text":"The generated trace and dump files should now be in the persistent volume. You used storage from IBM Cloud and we have to go through a number of steps using a different tool to access those files. Since the volume is attached to the Pod, we can instead use Pod's terminal to easily verify that trace and dump files are present. The following steps to access the files are illustrated in the screen recording below: Remote shell to your pod via one of two ways: From your terminal: oc rsh <pod-name> From console: click on Workloads > Pods . Click on the pod and then click on Terminal tab. Enter ls -R serviceability/apps to list the files. The shared volume is mounted at serviceability folder. The sub-folder apps is the namespace of the Pod. You should see a zip file for dumps and trace log files. These are produced by the day-2 operations you performed.","title":"Accessing the generated files (Hands-on)"},{"location":"ApplicationManagement/#summary","text":"Congratulations! You've completed Application Management lab!","title":"Summary"},{"location":"OperationalModernization/","text":"Operational Modernization Table of Contents Introduction Analysis (Hands-on) Build (Hands-on) Deploy without operator (Hands-on) Access the Application without operator (Hands-on) Alternate Deployment via Runtime Component Operator (Hands-on) Summary Next Introduction Operational modernization gives an operations team the opportunity to embrace modern operations best practices without putting change requirements on the development team. Modernizing from WebSphere Network Deployment (ND) to the traditional WebSphere Application Server Base V9 runtime in a container allows the application to be moved to the cloud without code changes. This type of modernization shouldn't require any code changes and can be driven by the operations team. This path gets the application in to a container with the least amount of effort but doesn't modernize the application or the runtime. In this lab, we'll use Customer Order Services application as an example. In order to modernize, the application will go through analysis , build and deploy phases. Click here and get to know the application, its architecture and components. Login to the VM If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password: \\ Password: engageibm Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window. Analysis (Hands-on) IBM Cloud Transformation Advisor can be used to analyze the Customer Order Service Application running in the WebSphere ND environment. The Transformation Advisor helps you to analyze your on-premises workloads for modernization. It determines the complexity of your applications, estimates a development cost to perform the move to the cloud, and recommends the best target environment. The steps needed to analyze the existing Customer Order Services application are: 1. Open a Firefox browser window from within the VM. Click on the openshift console bookmark in the top left and log in with the htpasswd option. Log in to the OpenShift account using the following credentials: Username: ibmadmin Password: engageibm From the Red Hat OpenShift Container Platform console, go to the Networking tab and click on Routes . Ensure that you are in the ta project by using the project drop down and click on the Location URL next to ta-ui-route . This will open the Transformation Advisor user interface. Click Create new under Workspaces to create a new workspace. Name it OperationalModernization and click Next . You'll be asked to create a new collection to store the data collected from the Customer Order Services application. Name it CustomerOrderServices . Click Create . To provide data and receive recommendations, you can either download and execute the Data Collector against an existing WebSphere environment or upload an existing data collection archive. The archive has already been created for you and the resulting data is stored here . Upload the results of the data collection (the datacollector.zip file) to IBM Cloud Transformation Advisor. When the upload is complete, you will see a list of applications analyzed from the source environment. At the top of the page, you can see the source environment and the target environment settings. Under the Migration target field, click the down arrow and select Compatible runtimes . This will show you an entry for each application for each compatible destination runtime you can migrate it to. Click the CustomerOrderServicesApp.ear application with the WebSphere traditional migration target to open the Application details page . Look over the migration analysis. You can view a summary of the complexity of migrating this application to this target, see detailed information about issues, and view additional reports about the application. In summary, no code changes are required to move this application to the traditional WebSphere Base v9 runtime, so it is a good candidate to proceed with the operational modernization. Click on View migration plan in the top right corner of the page. This page will help you assemble an archive containing: - your application's source or binary files (you upload these here or specify Maven coordinates to download them) - any required drivers or libraries (you upload these here or specify Maven coordinates to download them) - the wsadmin scripts needed to configure your application and its resources (generated by Transformation Advisor and automatically included) - the deployment artifacts needed to create the container image and deploy the application to OCP (generated by Transformation Advisor and automatically included) NOTE: These artifacts have already been provided for you as part of the lab files, so you don't need to download the migration plan. However, you can do so if you wish to look around at the files. These files can also be sent to a Git repository by Transformation Advisor. For a more detailed walkthrough of the Transformation Advisor process, see this document . Build (Hands-on) In this section, you'll learn how to build a Docker image for Customer Order Services application running on traditional WebSphere Base v9. Building this image could take around ~8 minutes. So, let's kick that process off before explaining what you did. The image should be built by the time you complete this section. Open a new terminal window from the VM desktop. Login to OpenShift CLI with the oc login command from the web terminal. When prompted for the username and password, enter the following login credentials: Username: ibmadmin Password: engageibm If you have not yet cloned the GitHub repo with the lab artifacts, run the following command on your terminal: git clone https://github.com/IBM/openshift-workshop-was.git Change directory to where this lab is located: cd openshift-workshop-was/labs/Openshift/OperationalModernization ls Run the following command to create a new project named apps-was in OpenShift. oc new-project apps-was Example output: Now using project \"apps-was\" on server \"https://c115-e.us-south.containers.cloud.ibm.com:32661\". . . . Run the following command to start building the image. Make sure to copy the entire command, including the \".\" at the end (which indicates current directory). docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was . Build image (Hands-on) Review the command you ran earlier: docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was . It instructs docker to build the image following the instructions in the Dockerfile in current directory (indicated by the \".\" at the end). A specific name to tag the built image is also specified after --tag . The value default-route-openshift-image-registry.apps.demo.ibmdte.net in the tag is the default address of the internal image registry provided by OpenShift. Image registry is a content server that can store and serve container images. The registry is accessible within the cluster via its exposed Service . The format of a Service address: name . namespace . svc . In this case, the image registry is named image-registry and it's in namespace openshift-image-registry . Later when we push the image to OpenShift's internal image registry, we'll refer to the image by the same values. You should see the following message if the image was successfully built. Please wait if it's still building. Successfully built aa6babbb5ce9 Successfully tagged default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was:latest Validate that image is in the repository by running the command: docker images Notice that the base image, websphere-traditional, is also listed. It was pulled as the first step of building application image. Example output: REPOSITORY TAG IMAGE ID CREATED SIZE default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was latest 9394150a5a15 10 minutes ago 2.05GB ibmcom/websphere-traditional latest 898f9fd79b36 12 minutes ago 1.86GB Note that docker images only lists those images that are cached locally. The name of the image also contains the host name where the image is hosted. If there is no host name, the image is hosted on docker hub. For example, the image ibmcom/websphere-traditional has no host name. It is hosted on docker hub. The image we just built, default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was , has host name default-route-openshift-image-registry.apps.demo.ibmdte.net . It is to be hosted in the Openshift image registry for your lab cluster. If you change an image, or build a new image, the changes are only available locally. You must push the image to propagate the changes to the remote registry. Let's push the image you just built to your OpenShift cluster's built-in image registry. First, login to the image registry by running the following command in the terminal. Note: A session token is obtained from the value of another command oc whoami -t and used as the password to login. docker login -u $(oc whoami) -p $(oc whoami -t) default-route-openshift-image-registry.apps.demo.ibmdte.net Example output: WARNING! Using --password via the CLI is insecure. Use --password-stdin. WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded Now, push the image into OpenShift cluster's internal image registry, which will take 1-2 minutes: docker push default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was Example output: Using default tag: latest The push refers to repository [default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was] 470e7d3b0bec: Pushed c38e61da8211: Pushed 2a72e88fe5eb: Pushed 334c79ff1b2e: Pushed ccf8ea26529f: Pushed af0f17433f77: Pushed 4254aef2aa12: Pushed 855301ffdcce: Pushed ea252e2474a5: Pushed 68a4c9686496: Pushed 87ecb86bc8e5: Pushed 066b59214d49: Pushed 211f972e9c63: Pushed b93eee2b1ddb: Pushed a6ab5ae423d9: Pushed 3f785cf0a0ae: Pushed latest: digest: sha256:4f4e8ae82fa22c83febc4f884b5026d01815fc704df6196431db8ed7a7def6a0 size: 3672 Verify that the image is in the image registry. The following command will get the images in the registry. Filter through the results to get only the image you pushed. Run the following command: oc get images | grep apps-was/cos-was The application image you just pushed should be listed. The hash of the image is stored alongside (indicated by the SHA-256 value). Example output: image-registry.openshift-image-registry.svc:5000/apps-was/cos-was@sha256:bc072d3b78ae6adcd843af75552965e5ed863bcce4fc3f1bc5d194570bc16953 OpenShift uses ImageStream to provide an abstraction for referencing container images from within the cluster. When an image is pushed to registry, an ImageStream is created automatically, if one doesn't already exist. Run the following command to see the ImageStream that's created: oc get imagestreams -n apps-was Example output: ``` NAME IMAGE REPOSITORY TAGS UPDATED cos-was default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was latest 2 minutes ago ``` You can also use the OpenShift console (UI) to see the ImageStream : From the panel on left-side, click on Builds > Image Streams . Then select apps-was from the Project drop-down menu. Click on cos-was from the list. Scroll down to the bottom to see the image that you pushed. Deploy without operator The following steps will deploy the modernized Customer Order Services application in a traditional WebSphere Base container to a RedHat OpenShift cluster. Customer Order Services application uses DB2 as its database. You can connect to an on-prem database that already exists or migrate the database to cloud. Since migrating the database is not the focus of this particular workshop and to save time, the database needed by the application is already configured in the OpenShift cluster you are using. Deploy application without operator (Hands-on) Run the following command to deploy the resources (*.yaml files) in the deploy directory: oc apply -f deploy Output: deployment.apps/cos-was created route.route.openshift.io/cos-was created secret/authdata created service/cos-was created Let's review what we just did. The directory deploy contains the following yaml files: Deployment.yaml : the specification for creating a Kubernetes deployment Service.yaml : the specification to expose the deployment as a cluster-wide Kubernetes service. Route.yaml : the specification to expose the service as a route visible outside of the cluster. Secret.yaml : the specification that the properties based configuration properties file used to configure database user/password when the container starts. The file Deployment.yaml looks like: apiVersion: apps/v1 kind: Deployment metadata: name: cos-was namespace: apps-was spec: selector: matchLabels: app: cos-was replicas: 1 template: metadata: labels: app: cos-was spec: containers: - name: cos-was image: image-registry.openshift-image-registry.svc:5000/apps-was/cos-was ports: - containerPort: 9080 livenessProbe: httpGet: path: /CustomerOrderServicesWeb/index.html port: 9080 periodSeconds: 30 failureThreshold: 6 initialDelaySeconds: 90 readinessProbe: httpGet: path: /CustomerOrderServicesWeb/index.html port: 9080 periodSeconds: 10 failureThreshold: 3 volumeMounts: - mountPath: /etc/websphere name: authdata readOnly: true volumes: - name: authdata secret: secretName: authdata Note: The liveness probe is used to tell Kubernetes when the application is live. Due to the size of the traditional WAS image, the initialDelaySeconds attribute has been set to 90 seconds to give the container time to start. The readiness probe is used to tell Kubernetes whether the application is ready to serve requests. You may store property file based configuration files such as configmaps and secrets, and bind their contents into the /etc/websphere directory. When the container starts, the server startup script will apply all the property files found in the /etc/websphere directory to reconfigure the server. For our example, the volumeMounts and volumes are used to bind the contents of the secret authdata into the directory /etc/websphere during container startup. After it is bound, it will appear as the file /etc/websphere/authdata.properties . For volumeMounts: The mountPath, /etc/websphere , specifies the directory where the files are bound. the name, authdata , specifies the name of the volume For volumes: the secretName specifies the name of the secret whose contents are to be bound. The file Secret.yaml looks like: apiVersion: v1 kind: Secret metadata: name: authdata namespace: apps-was type: Opaque stringData: authdata.props: |- # # Configuration properties file for cells/DefaultCell01|security.xml#JAASAuthData_1597094577206# # Extracted on Tue Aug 11 15:30:36 UTC 2020 # # # Section 1.0 ## Cell=!{cellName}:Security=:JAASAuthData=alias#DBUser # # # SubSection 1.0.0 # JAASAuthData Section # ResourceType=JAASAuthData ImplementingResourceType=GenericType ResourceId=Cell=!{cellName}:Security=:JAASAuthData=alias#DBUser AttributeInfo=authDataEntries # # #Properties # password=\"{xor}Oz1tNjEsK24=\" #required alias=DBUser #required userId=db2inst1 #required description= # # End of Section 1.0# Cell=!{cellName}:Security=:JAASAuthData=alias#DBUser # # # EnvironmentVariablesSection # # #Environment Variables cellName=DefaultCell01 The attribute authdata.properties contains the properties file based configure used to update the database userId and password for the JAASAuthData whose alias is DBUser. The configuration in Deployment.yaml maps it as the file /etc/websphere/authdata.properties during container startup so that the application server startup script can automatically configure the server with these entries. Access the application without operator (Hands-on) Confirm you're at the current project apps-was : oc project Example output: Using project \"apps-was\" on server \"https://c114-e.us-south.containers.cloud.ibm.com:30016\". - If it's not at the project apps-was , then switch: oc project apps-was 1. Run the following command to verify the pod is running: oc get pod If the status does not show 1/1 READY, wait a while, checking status periodically: NAME READY STATUS RESTARTS AGE cos-was-6bd4767bf6-xhr92 1/1 Running 0 120m Run the following command to get the URL of your application (the route URL plus the application contextroot): echo http://$(oc get route cos-was --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-was-apps-was.apps.demo.ibmdte.net/CustomerOrderServicesWeb Return to your Firefox browser window and go to the URL outputted by the command run in the previous step. You will be prompted to login in order to access the application. Enter the following credentials: Username: skywalker Password: force After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total . Close the browser. Review the application workload flow without operator (Hands-on) Below is an overview diagram on the deployment you've completed from the above steps: Note: DB2 in the middle of the diagram is pre-installed through a different project db and has been up and running before your hands-on. Also it will not be impacted when you're removing the deployment in next step. Return to the Firefox browser and open the OpenShift Console to view the resources on the deployment. View the resources in the project apps-was : Select the apps-was project from the project drop down menu. View deployment details: Click on the Deployments tab under Workloads from the left menu and select cos-was Navigate to the YAML tab to view the content of yaml View pod details: Click on the Pods tab under Workloads from the left menu and select the pod with name starting with cos-was Navigate to the Logs tab to view the WebSphere Application Server log Navigate to the Terminal tab to view the files inside the container View secret details: Click on the Secrets tab under Workloads from the left menu and select the authdata secret. Scroll down to the Data section and click on the copy icon to view the content. View service details: Click on the Services tab under Networking from the left menu and select the cos-was service. Review service information including address and port mapping details. View route details: Click on the Routes tab under Networking from the left menu and select the cos-was route. 1. View the resources in the project db : - Select the db project from the project drop down menu. View deployment details: Click on the Deployments tab under Workloads from the left menu and select cos-db-was Navigate to the YAML tab to view the content of yaml View pod details: Click on the Pods tab under Workloads from the left menu and select the pod with name starting with cos-db-was Navigate to the Logs tab to view the database logs Navigate to the Terminal tab to view the files in the database container View service details: Click on the Services tab under Networking from the left menu and select the cos-db-was service. Remove your deployment (standard deployment without operator) (Hands-on) To remove the deploment from the above scenario without the operator, run the command: Note: The pre-installed resources such as DB2, are not removed. oc delete -f deploy Output: deployment.apps \"cos-was\" deleted route.route.openshift.io \"cos-was\" deleted secret \"authdata\" deleted service \"cos-was\" deleted Alternate Deployment Via Runtime Component Operator Another way to deploy the application is via the Runtime Component Operator. It is a generic operator used to deploy different types of application images. The operator has already been installed into your environment. For more information, see: https://github.com/application-stacks/runtime-component-operator Deploy application (via Runtime Component Operator) (Hands-on) Run the following command which uses the Runtime Component Operator to deploy the same Customer Order Service application image: oc apply -f deploy-rco Output: runtimecomponent.app.stacks/cos-was-rco created secret/authdata-rco created Let's review what we just did. First, list the contents of the deploy-rco directory: ls deploy-rco The output shows there are only two yaml files: RuntimeComponent.yaml Secret.yaml Review Secret.yaml: cat deploy-rco/Secret.yaml Note that it is the same as the Secret.yaml in the deploy directory, except the name has been changed to authdata-rco . It serves the same purpose for this new deployment - to override the database user/password. Review RuntimeComponent.yaml: cat deploy-rco/RuntimeComponent.yaml And the output: apiVersion: app.stacks/v1beta1 kind: RuntimeComponent metadata: name: cos-was-rco namespace: apps-was spec: applicationImage: image-registry.openshift-image-registry.svc:5000/apps-was/cos-was service: port: 9080 readinessProbe: httpGet: path: /CustomerOrderServicesWeb/index.html port: 9080 periodSeconds: 10 failureThreshold: 3 livenessProbe: httpGet: path: /CustomerOrderServicesWeb/index.html port: 9080 periodSeconds: 30 failureThreshold: 6 initialDelaySeconds: 90 expose: true route: termination: edge insecureEdgeTerminationPolicy: Redirect volumeMounts: - mountPath: /etc/websphere name: authdata-rco readOnly: true volumes: - name: authdata-rco secret: secretName: authdata-rco Note that: - The kind is RuntimeComponent - The expose attribute is set to true to expose a route - The attributes within the yaml file are essentially the same information that you provided for the Service , Route , and Deployment resources in the deploy directory. - The controller for the RuntimeComponent custom resource reacts to changes in the above specification, and creates the corresponding Service , Route , and Deployment objects. Issue the following commands to view what the controller has created: ``` oc get Deployment cos-was-rco -o yaml oc get Service cos-was-rco -o yaml oc get Route cos-was-rco -o yaml ``` Access the application (via Runtime Component Operator) (Hands-on) Confirm you're at the current project apps-was : oc project Example output: Using project \"apps-was\" on server \"https://c114-e.us-south.containers.cloud.ibm.com:30016\". - If it's not at the project apps-was , then switch: oc project apps-was 1. Run the following command to verify the pod is running: oc get pod If the status does not show 1/1 READY, wait a while, checking status periodically. Note the prefix name for the pod is cos-was-rco . NAME READY STATUS RESTARTS AGE cos-was-rco-6779784fc8-pz92m 1/1 Running 0 2m59s Run the following command to get the URL of your application (the route URL plus the application contextroot): echo http://$(oc get route cos-was-rco --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-was-rco-apps-was.apps.demo.ibmdte.net/CustomerOrderServicesWeb Return to your Firefox browser window and go to the URL outputted by the command run in the previous step. The steps to access the application are the same as those used when deploying without the operator. You will be prompted to login in order to access the application. Enter the following credentials: Username: skywalker Password: force After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total . Close the browser. Review the application workload flow with Runtime Component Operator (Hands-on) Below is an overview diagram on the deployment you've completed from the above steps using Runtime Component Operator: Note: DB2 in the middle of the diagram is pre-installed through a different project db and has been up and running before your hands-on. Return to the Firefox browser and open the OpenShift Console to view the resources on the deployment. View the resources in the project openshift-operators : Select the openshift-operators project from the project drop down menu. View operator deployment details: Click on the Deployments tab under Workloads from the left menu and select runtime-component-operator Navigate to the YAML tab to view the content of yaml View operator pod details: Click on the Pods tab under Workloads from the left menu and select the pod with name starting with runtime-component-operator Navigate to Logs to view the runtime-component-operator container log Navigate to Terminal to view the files in the container View the resources in the project apps-was : Select the apps-was project from the project drop down menu. View Runtime Component instance details: Click on the Installed Operators tab under Operators from the left menu and select Runtime Component Operator . > Note: The operator is installed at cluster level and is visible to all existing projects, but Runtime Component instance is created under the project apps-was . Navigate to the YAML tab to view the content of yaml Navigate to the Runtime Component tab and select cos-was-rco to view the deails of Runtime Component instance View deployment details: Click on the Deployments tab under Workloads from the left menu and select cos-was-rco . Navigate to the YAML tab to view the content of yaml. > Note the deployment is created through the controller of RuntimeComponent custom resource. View pod details: Click on the Pods tab under Workloads from the left menu and select the pod starting with cos-was-rco Navigate to the Logs tab to view the WebSphere Application Server log View service details: Click on the Services tab under Networking from the left menu and select cos-was-rco Navigate to the YAML tab to view the content of yaml. > Note the service is created through the controller of RuntimeComponent custom resource. View route details: Click on the Routes tab under Networking from the left menu and select cos-was-rco Navigate to the YAML tab to view the content of yaml. > Note the route is created through the controller of RuntimeComponent custom resource. View secret details: Click on the Secrets tab under Workloads from the left menu and select authdata-rco Resources in the project db : Same information as listed in the section above in Review the application workload flow without operator . Cleanup (the deployment with Runtime Component Operator) (Hands-on) Run the following command in a terminal window to remove the deployment from the above secenario with Runtime Component instance: Note: The pre-installed resources such as Runtime Component Operator, DB2, are not removed. oc delete -f deploy-rco Output: runtimecomponent.app.stacks \"cos-was-rco\" deleted secret \"authdata-rco\" deleted Verify that the corresponding application Service , Route , and Deployment have also been deleted: oc get Deployment oc get Service oc get Route Output from each get command above should be: No resources found in apps-was namespace. Summary Congratulations! You've completed the Operational Modernization lab. You containerized and deployed a monolith application to cloud! Next Please follow the link to the next lab Runtime Modernization : - Runtime Modernization","title":"Operational Modernization"},{"location":"OperationalModernization/#operational-modernization","text":"","title":"Operational Modernization"},{"location":"OperationalModernization/#table-of-contents","text":"Introduction Analysis (Hands-on) Build (Hands-on) Deploy without operator (Hands-on) Access the Application without operator (Hands-on) Alternate Deployment via Runtime Component Operator (Hands-on) Summary Next","title":"Table of Contents"},{"location":"OperationalModernization/#introduction","text":"Operational modernization gives an operations team the opportunity to embrace modern operations best practices without putting change requirements on the development team. Modernizing from WebSphere Network Deployment (ND) to the traditional WebSphere Application Server Base V9 runtime in a container allows the application to be moved to the cloud without code changes. This type of modernization shouldn't require any code changes and can be driven by the operations team. This path gets the application in to a container with the least amount of effort but doesn't modernize the application or the runtime. In this lab, we'll use Customer Order Services application as an example. In order to modernize, the application will go through analysis , build and deploy phases. Click here and get to know the application, its architecture and components.","title":"Introduction"},{"location":"OperationalModernization/#login-to-the-vm","text":"If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password: \\ Password: engageibm Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window.","title":"Login to the VM"},{"location":"OperationalModernization/#analysis-hands-on","text":"IBM Cloud Transformation Advisor can be used to analyze the Customer Order Service Application running in the WebSphere ND environment. The Transformation Advisor helps you to analyze your on-premises workloads for modernization. It determines the complexity of your applications, estimates a development cost to perform the move to the cloud, and recommends the best target environment. The steps needed to analyze the existing Customer Order Services application are: 1. Open a Firefox browser window from within the VM. Click on the openshift console bookmark in the top left and log in with the htpasswd option. Log in to the OpenShift account using the following credentials: Username: ibmadmin Password: engageibm From the Red Hat OpenShift Container Platform console, go to the Networking tab and click on Routes . Ensure that you are in the ta project by using the project drop down and click on the Location URL next to ta-ui-route . This will open the Transformation Advisor user interface. Click Create new under Workspaces to create a new workspace. Name it OperationalModernization and click Next . You'll be asked to create a new collection to store the data collected from the Customer Order Services application. Name it CustomerOrderServices . Click Create . To provide data and receive recommendations, you can either download and execute the Data Collector against an existing WebSphere environment or upload an existing data collection archive. The archive has already been created for you and the resulting data is stored here . Upload the results of the data collection (the datacollector.zip file) to IBM Cloud Transformation Advisor. When the upload is complete, you will see a list of applications analyzed from the source environment. At the top of the page, you can see the source environment and the target environment settings. Under the Migration target field, click the down arrow and select Compatible runtimes . This will show you an entry for each application for each compatible destination runtime you can migrate it to. Click the CustomerOrderServicesApp.ear application with the WebSphere traditional migration target to open the Application details page . Look over the migration analysis. You can view a summary of the complexity of migrating this application to this target, see detailed information about issues, and view additional reports about the application. In summary, no code changes are required to move this application to the traditional WebSphere Base v9 runtime, so it is a good candidate to proceed with the operational modernization. Click on View migration plan in the top right corner of the page. This page will help you assemble an archive containing: - your application's source or binary files (you upload these here or specify Maven coordinates to download them) - any required drivers or libraries (you upload these here or specify Maven coordinates to download them) - the wsadmin scripts needed to configure your application and its resources (generated by Transformation Advisor and automatically included) - the deployment artifacts needed to create the container image and deploy the application to OCP (generated by Transformation Advisor and automatically included) NOTE: These artifacts have already been provided for you as part of the lab files, so you don't need to download the migration plan. However, you can do so if you wish to look around at the files. These files can also be sent to a Git repository by Transformation Advisor. For a more detailed walkthrough of the Transformation Advisor process, see this document .","title":"Analysis (Hands-on)"},{"location":"OperationalModernization/#build-hands-on","text":"In this section, you'll learn how to build a Docker image for Customer Order Services application running on traditional WebSphere Base v9. Building this image could take around ~8 minutes. So, let's kick that process off before explaining what you did. The image should be built by the time you complete this section. Open a new terminal window from the VM desktop. Login to OpenShift CLI with the oc login command from the web terminal. When prompted for the username and password, enter the following login credentials: Username: ibmadmin Password: engageibm If you have not yet cloned the GitHub repo with the lab artifacts, run the following command on your terminal: git clone https://github.com/IBM/openshift-workshop-was.git Change directory to where this lab is located: cd openshift-workshop-was/labs/Openshift/OperationalModernization ls Run the following command to create a new project named apps-was in OpenShift. oc new-project apps-was Example output: Now using project \"apps-was\" on server \"https://c115-e.us-south.containers.cloud.ibm.com:32661\". . . . Run the following command to start building the image. Make sure to copy the entire command, including the \".\" at the end (which indicates current directory). docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was .","title":"Build (Hands-on)"},{"location":"OperationalModernization/#build-image-hands-on","text":"Review the command you ran earlier: docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was . It instructs docker to build the image following the instructions in the Dockerfile in current directory (indicated by the \".\" at the end). A specific name to tag the built image is also specified after --tag . The value default-route-openshift-image-registry.apps.demo.ibmdte.net in the tag is the default address of the internal image registry provided by OpenShift. Image registry is a content server that can store and serve container images. The registry is accessible within the cluster via its exposed Service . The format of a Service address: name . namespace . svc . In this case, the image registry is named image-registry and it's in namespace openshift-image-registry . Later when we push the image to OpenShift's internal image registry, we'll refer to the image by the same values. You should see the following message if the image was successfully built. Please wait if it's still building. Successfully built aa6babbb5ce9 Successfully tagged default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was:latest Validate that image is in the repository by running the command: docker images Notice that the base image, websphere-traditional, is also listed. It was pulled as the first step of building application image. Example output: REPOSITORY TAG IMAGE ID CREATED SIZE default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was latest 9394150a5a15 10 minutes ago 2.05GB ibmcom/websphere-traditional latest 898f9fd79b36 12 minutes ago 1.86GB Note that docker images only lists those images that are cached locally. The name of the image also contains the host name where the image is hosted. If there is no host name, the image is hosted on docker hub. For example, the image ibmcom/websphere-traditional has no host name. It is hosted on docker hub. The image we just built, default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was , has host name default-route-openshift-image-registry.apps.demo.ibmdte.net . It is to be hosted in the Openshift image registry for your lab cluster. If you change an image, or build a new image, the changes are only available locally. You must push the image to propagate the changes to the remote registry. Let's push the image you just built to your OpenShift cluster's built-in image registry. First, login to the image registry by running the following command in the terminal. Note: A session token is obtained from the value of another command oc whoami -t and used as the password to login. docker login -u $(oc whoami) -p $(oc whoami -t) default-route-openshift-image-registry.apps.demo.ibmdte.net Example output: WARNING! Using --password via the CLI is insecure. Use --password-stdin. WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded Now, push the image into OpenShift cluster's internal image registry, which will take 1-2 minutes: docker push default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was Example output: Using default tag: latest The push refers to repository [default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was] 470e7d3b0bec: Pushed c38e61da8211: Pushed 2a72e88fe5eb: Pushed 334c79ff1b2e: Pushed ccf8ea26529f: Pushed af0f17433f77: Pushed 4254aef2aa12: Pushed 855301ffdcce: Pushed ea252e2474a5: Pushed 68a4c9686496: Pushed 87ecb86bc8e5: Pushed 066b59214d49: Pushed 211f972e9c63: Pushed b93eee2b1ddb: Pushed a6ab5ae423d9: Pushed 3f785cf0a0ae: Pushed latest: digest: sha256:4f4e8ae82fa22c83febc4f884b5026d01815fc704df6196431db8ed7a7def6a0 size: 3672 Verify that the image is in the image registry. The following command will get the images in the registry. Filter through the results to get only the image you pushed. Run the following command: oc get images | grep apps-was/cos-was The application image you just pushed should be listed. The hash of the image is stored alongside (indicated by the SHA-256 value). Example output: image-registry.openshift-image-registry.svc:5000/apps-was/cos-was@sha256:bc072d3b78ae6adcd843af75552965e5ed863bcce4fc3f1bc5d194570bc16953 OpenShift uses ImageStream to provide an abstraction for referencing container images from within the cluster. When an image is pushed to registry, an ImageStream is created automatically, if one doesn't already exist. Run the following command to see the ImageStream that's created: oc get imagestreams -n apps-was Example output: ``` NAME IMAGE REPOSITORY TAGS UPDATED cos-was default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was latest 2 minutes ago ``` You can also use the OpenShift console (UI) to see the ImageStream : From the panel on left-side, click on Builds > Image Streams . Then select apps-was from the Project drop-down menu. Click on cos-was from the list. Scroll down to the bottom to see the image that you pushed.","title":"Build image (Hands-on)"},{"location":"OperationalModernization/#deploy-without-operator","text":"The following steps will deploy the modernized Customer Order Services application in a traditional WebSphere Base container to a RedHat OpenShift cluster. Customer Order Services application uses DB2 as its database. You can connect to an on-prem database that already exists or migrate the database to cloud. Since migrating the database is not the focus of this particular workshop and to save time, the database needed by the application is already configured in the OpenShift cluster you are using.","title":"Deploy without operator"},{"location":"OperationalModernization/#deploy-application-without-operator-hands-on","text":"Run the following command to deploy the resources (*.yaml files) in the deploy directory: oc apply -f deploy Output: deployment.apps/cos-was created route.route.openshift.io/cos-was created secret/authdata created service/cos-was created Let's review what we just did. The directory deploy contains the following yaml files: Deployment.yaml : the specification for creating a Kubernetes deployment Service.yaml : the specification to expose the deployment as a cluster-wide Kubernetes service. Route.yaml : the specification to expose the service as a route visible outside of the cluster. Secret.yaml : the specification that the properties based configuration properties file used to configure database user/password when the container starts. The file Deployment.yaml looks like: apiVersion: apps/v1 kind: Deployment metadata: name: cos-was namespace: apps-was spec: selector: matchLabels: app: cos-was replicas: 1 template: metadata: labels: app: cos-was spec: containers: - name: cos-was image: image-registry.openshift-image-registry.svc:5000/apps-was/cos-was ports: - containerPort: 9080 livenessProbe: httpGet: path: /CustomerOrderServicesWeb/index.html port: 9080 periodSeconds: 30 failureThreshold: 6 initialDelaySeconds: 90 readinessProbe: httpGet: path: /CustomerOrderServicesWeb/index.html port: 9080 periodSeconds: 10 failureThreshold: 3 volumeMounts: - mountPath: /etc/websphere name: authdata readOnly: true volumes: - name: authdata secret: secretName: authdata Note: The liveness probe is used to tell Kubernetes when the application is live. Due to the size of the traditional WAS image, the initialDelaySeconds attribute has been set to 90 seconds to give the container time to start. The readiness probe is used to tell Kubernetes whether the application is ready to serve requests. You may store property file based configuration files such as configmaps and secrets, and bind their contents into the /etc/websphere directory. When the container starts, the server startup script will apply all the property files found in the /etc/websphere directory to reconfigure the server. For our example, the volumeMounts and volumes are used to bind the contents of the secret authdata into the directory /etc/websphere during container startup. After it is bound, it will appear as the file /etc/websphere/authdata.properties . For volumeMounts: The mountPath, /etc/websphere , specifies the directory where the files are bound. the name, authdata , specifies the name of the volume For volumes: the secretName specifies the name of the secret whose contents are to be bound. The file Secret.yaml looks like: apiVersion: v1 kind: Secret metadata: name: authdata namespace: apps-was type: Opaque stringData: authdata.props: |- # # Configuration properties file for cells/DefaultCell01|security.xml#JAASAuthData_1597094577206# # Extracted on Tue Aug 11 15:30:36 UTC 2020 # # # Section 1.0 ## Cell=!{cellName}:Security=:JAASAuthData=alias#DBUser # # # SubSection 1.0.0 # JAASAuthData Section # ResourceType=JAASAuthData ImplementingResourceType=GenericType ResourceId=Cell=!{cellName}:Security=:JAASAuthData=alias#DBUser AttributeInfo=authDataEntries # # #Properties # password=\"{xor}Oz1tNjEsK24=\" #required alias=DBUser #required userId=db2inst1 #required description= # # End of Section 1.0# Cell=!{cellName}:Security=:JAASAuthData=alias#DBUser # # # EnvironmentVariablesSection # # #Environment Variables cellName=DefaultCell01 The attribute authdata.properties contains the properties file based configure used to update the database userId and password for the JAASAuthData whose alias is DBUser. The configuration in Deployment.yaml maps it as the file /etc/websphere/authdata.properties during container startup so that the application server startup script can automatically configure the server with these entries.","title":"Deploy application without operator (Hands-on)"},{"location":"OperationalModernization/#access-the-application-without-operator-hands-on","text":"Confirm you're at the current project apps-was : oc project Example output: Using project \"apps-was\" on server \"https://c114-e.us-south.containers.cloud.ibm.com:30016\". - If it's not at the project apps-was , then switch: oc project apps-was 1. Run the following command to verify the pod is running: oc get pod If the status does not show 1/1 READY, wait a while, checking status periodically: NAME READY STATUS RESTARTS AGE cos-was-6bd4767bf6-xhr92 1/1 Running 0 120m Run the following command to get the URL of your application (the route URL plus the application contextroot): echo http://$(oc get route cos-was --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-was-apps-was.apps.demo.ibmdte.net/CustomerOrderServicesWeb Return to your Firefox browser window and go to the URL outputted by the command run in the previous step. You will be prompted to login in order to access the application. Enter the following credentials: Username: skywalker Password: force After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total . Close the browser.","title":"Access the application without operator (Hands-on)"},{"location":"OperationalModernization/#review-the-application-workload-flow-without-operator-hands-on","text":"Below is an overview diagram on the deployment you've completed from the above steps: Note: DB2 in the middle of the diagram is pre-installed through a different project db and has been up and running before your hands-on. Also it will not be impacted when you're removing the deployment in next step. Return to the Firefox browser and open the OpenShift Console to view the resources on the deployment. View the resources in the project apps-was : Select the apps-was project from the project drop down menu. View deployment details: Click on the Deployments tab under Workloads from the left menu and select cos-was Navigate to the YAML tab to view the content of yaml View pod details: Click on the Pods tab under Workloads from the left menu and select the pod with name starting with cos-was Navigate to the Logs tab to view the WebSphere Application Server log Navigate to the Terminal tab to view the files inside the container View secret details: Click on the Secrets tab under Workloads from the left menu and select the authdata secret. Scroll down to the Data section and click on the copy icon to view the content. View service details: Click on the Services tab under Networking from the left menu and select the cos-was service. Review service information including address and port mapping details. View route details: Click on the Routes tab under Networking from the left menu and select the cos-was route. 1. View the resources in the project db : - Select the db project from the project drop down menu. View deployment details: Click on the Deployments tab under Workloads from the left menu and select cos-db-was Navigate to the YAML tab to view the content of yaml View pod details: Click on the Pods tab under Workloads from the left menu and select the pod with name starting with cos-db-was Navigate to the Logs tab to view the database logs Navigate to the Terminal tab to view the files in the database container View service details: Click on the Services tab under Networking from the left menu and select the cos-db-was service.","title":"Review the application workload flow without operator (Hands-on)"},{"location":"OperationalModernization/#remove-your-deployment-standard-deployment-without-operator-hands-on","text":"To remove the deploment from the above scenario without the operator, run the command: Note: The pre-installed resources such as DB2, are not removed. oc delete -f deploy Output: deployment.apps \"cos-was\" deleted route.route.openshift.io \"cos-was\" deleted secret \"authdata\" deleted service \"cos-was\" deleted","title":"Remove your deployment (standard deployment without operator) (Hands-on)"},{"location":"OperationalModernization/#alternate-deployment-via-runtime-component-operator","text":"Another way to deploy the application is via the Runtime Component Operator. It is a generic operator used to deploy different types of application images. The operator has already been installed into your environment. For more information, see: https://github.com/application-stacks/runtime-component-operator","title":"Alternate Deployment Via Runtime Component Operator"},{"location":"OperationalModernization/#deploy-application-via-runtime-component-operator-hands-on","text":"Run the following command which uses the Runtime Component Operator to deploy the same Customer Order Service application image: oc apply -f deploy-rco Output: runtimecomponent.app.stacks/cos-was-rco created secret/authdata-rco created Let's review what we just did. First, list the contents of the deploy-rco directory: ls deploy-rco The output shows there are only two yaml files: RuntimeComponent.yaml Secret.yaml Review Secret.yaml: cat deploy-rco/Secret.yaml Note that it is the same as the Secret.yaml in the deploy directory, except the name has been changed to authdata-rco . It serves the same purpose for this new deployment - to override the database user/password. Review RuntimeComponent.yaml: cat deploy-rco/RuntimeComponent.yaml And the output: apiVersion: app.stacks/v1beta1 kind: RuntimeComponent metadata: name: cos-was-rco namespace: apps-was spec: applicationImage: image-registry.openshift-image-registry.svc:5000/apps-was/cos-was service: port: 9080 readinessProbe: httpGet: path: /CustomerOrderServicesWeb/index.html port: 9080 periodSeconds: 10 failureThreshold: 3 livenessProbe: httpGet: path: /CustomerOrderServicesWeb/index.html port: 9080 periodSeconds: 30 failureThreshold: 6 initialDelaySeconds: 90 expose: true route: termination: edge insecureEdgeTerminationPolicy: Redirect volumeMounts: - mountPath: /etc/websphere name: authdata-rco readOnly: true volumes: - name: authdata-rco secret: secretName: authdata-rco Note that: - The kind is RuntimeComponent - The expose attribute is set to true to expose a route - The attributes within the yaml file are essentially the same information that you provided for the Service , Route , and Deployment resources in the deploy directory. - The controller for the RuntimeComponent custom resource reacts to changes in the above specification, and creates the corresponding Service , Route , and Deployment objects. Issue the following commands to view what the controller has created: ``` oc get Deployment cos-was-rco -o yaml oc get Service cos-was-rco -o yaml oc get Route cos-was-rco -o yaml ```","title":"Deploy application (via Runtime Component Operator) (Hands-on)"},{"location":"OperationalModernization/#access-the-application-via-runtime-component-operator-hands-on","text":"Confirm you're at the current project apps-was : oc project Example output: Using project \"apps-was\" on server \"https://c114-e.us-south.containers.cloud.ibm.com:30016\". - If it's not at the project apps-was , then switch: oc project apps-was 1. Run the following command to verify the pod is running: oc get pod If the status does not show 1/1 READY, wait a while, checking status periodically. Note the prefix name for the pod is cos-was-rco . NAME READY STATUS RESTARTS AGE cos-was-rco-6779784fc8-pz92m 1/1 Running 0 2m59s Run the following command to get the URL of your application (the route URL plus the application contextroot): echo http://$(oc get route cos-was-rco --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-was-rco-apps-was.apps.demo.ibmdte.net/CustomerOrderServicesWeb Return to your Firefox browser window and go to the URL outputted by the command run in the previous step. The steps to access the application are the same as those used when deploying without the operator. You will be prompted to login in order to access the application. Enter the following credentials: Username: skywalker Password: force After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total . Close the browser.","title":"Access the application (via Runtime Component Operator) (Hands-on)"},{"location":"OperationalModernization/#review-the-application-workload-flow-with-runtime-component-operator-hands-on","text":"Below is an overview diagram on the deployment you've completed from the above steps using Runtime Component Operator: Note: DB2 in the middle of the diagram is pre-installed through a different project db and has been up and running before your hands-on. Return to the Firefox browser and open the OpenShift Console to view the resources on the deployment. View the resources in the project openshift-operators : Select the openshift-operators project from the project drop down menu. View operator deployment details: Click on the Deployments tab under Workloads from the left menu and select runtime-component-operator Navigate to the YAML tab to view the content of yaml View operator pod details: Click on the Pods tab under Workloads from the left menu and select the pod with name starting with runtime-component-operator Navigate to Logs to view the runtime-component-operator container log Navigate to Terminal to view the files in the container View the resources in the project apps-was : Select the apps-was project from the project drop down menu. View Runtime Component instance details: Click on the Installed Operators tab under Operators from the left menu and select Runtime Component Operator . > Note: The operator is installed at cluster level and is visible to all existing projects, but Runtime Component instance is created under the project apps-was . Navigate to the YAML tab to view the content of yaml Navigate to the Runtime Component tab and select cos-was-rco to view the deails of Runtime Component instance View deployment details: Click on the Deployments tab under Workloads from the left menu and select cos-was-rco . Navigate to the YAML tab to view the content of yaml. > Note the deployment is created through the controller of RuntimeComponent custom resource. View pod details: Click on the Pods tab under Workloads from the left menu and select the pod starting with cos-was-rco Navigate to the Logs tab to view the WebSphere Application Server log View service details: Click on the Services tab under Networking from the left menu and select cos-was-rco Navigate to the YAML tab to view the content of yaml. > Note the service is created through the controller of RuntimeComponent custom resource. View route details: Click on the Routes tab under Networking from the left menu and select cos-was-rco Navigate to the YAML tab to view the content of yaml. > Note the route is created through the controller of RuntimeComponent custom resource. View secret details: Click on the Secrets tab under Workloads from the left menu and select authdata-rco Resources in the project db : Same information as listed in the section above in Review the application workload flow without operator .","title":"Review the application workload flow with Runtime Component Operator (Hands-on)"},{"location":"OperationalModernization/#cleanup-the-deployment-with-runtime-component-operator-hands-on","text":"Run the following command in a terminal window to remove the deployment from the above secenario with Runtime Component instance: Note: The pre-installed resources such as Runtime Component Operator, DB2, are not removed. oc delete -f deploy-rco Output: runtimecomponent.app.stacks \"cos-was-rco\" deleted secret \"authdata-rco\" deleted Verify that the corresponding application Service , Route , and Deployment have also been deleted: oc get Deployment oc get Service oc get Route Output from each get command above should be: No resources found in apps-was namespace.","title":"Cleanup (the deployment with Runtime Component Operator) (Hands-on)"},{"location":"OperationalModernization/#summary","text":"Congratulations! You've completed the Operational Modernization lab. You containerized and deployed a monolith application to cloud!","title":"Summary"},{"location":"OperationalModernization/#next","text":"Please follow the link to the next lab Runtime Modernization : - Runtime Modernization","title":"Next"},{"location":"OperationalModernization/extras/WAS-analyze/","text":"Traditional WebSphere - Analyze This section covers how to use IBM Cloud Transformation Advisor to analyze an existing traditional WebSphere application. For this scenario the traditional WebSphere Application Server runtime is chosen as the target runtime and the intention is to migrate this application without code changes. Migrating to the containerized version of traditional WebSphere Application Server will prepare the organization for: moving workloads to the cloud. improving DevOps and speed-to-market. receiving the benefits of the consistency and reliability of containers. Summary This section has the following steps: Introduction to IBM Cloud Transformation Advisor Install IBM Cloud Transformation Advisor Download and run the Data Collector Upload and analyze the results Determine the migration/modernization path and next steps Introduction to IBM Cloud Transformation Advisor IBM Cloud Transformation Advisor helps you access, analyze and modernize middleware based apps into IBM Cloud(s). It categorizes Java EE apps and MQ queue managers as simple, medium and complex based on migration complexity and provides guidance for modernization. IBM Cloud Transformation Advisor can accelerate the process to move your on-premises apps to cloud, minimize migration errors and risks, and reduce time to market in five steps. You can use IBM Transformation Advisor for these activities: - Identify the Java EE programming models in an app - Determine the complexity of apps by reviewing a high-level inventory of the content and structure of each app - Highlight the Java EE programming model and WebSphere API differences between the profile types - Learn any Java EE specification implementation differences that might affect the app The tool also provides suggestions for the right-fit IBM WebSphere Application Server edition and offers advice, practices, and potential solutions to move apps to Liberty or to newer versions of WebSphere Application Server. Install IBM Cloud Transformation Advisor IBM Cloud Transformation Advisor is split in to two components (the analysis engine and the data collector ). Access Transformation Advisor from IBM WebSphere Hybrid Edition. You can also run it locally on a machine with Docker. See Installing IBM Cloud Transformation Advisor locally . Download the Data Collector Once IBM Cloud Transformation Advisor is installed, it is necessary to create a new Workspace and Collection and then download the Data Collector that will be used to examine the existing environment and applications. Open IBM Cloud Transformation Advisor in a browser and click the button to create a new Workspace Enter a Workspace name such as TransformationAdvisorDemo and click Next Enter a Collection name such as WAS855_AppSrv01 and click Let's go When the No recommendations available page is displayed, click the Data Collector button When the Data Collector page is displayed, select the Source Operating System for your environment and click the Download button to download the Data Collector. This results in a file with a name similar to transformationadvisor-Linux_TransformationAdvisorDemo_WAS855_AppSrv01.tgz being downloaded. Run the Data Collector Upload the Data Collector zip file that was downloaded from IBM Cloud Transformation Advisor in the previous step to the machine that the WebSphere ND Deployment Manager or the Standalone WebSphere Application Server is installed. The directory used arbitrary. Navigate to the directory you uploaded the transformationadvisor-Linux_TransformationAdvisorDemo_WAS855_AppSrv01.tgz file to and issue the following commands to extract the Data Collector: mkdir datacollector cd datacollector mv transformationadvisor-Linux_TransformationAdvisorDemo_WAS855_AppSrv01.tgz . tar -zxvf transformationadvisor-Linux_TransformationAdvisorDemo_WAS855_AppSrv01.tgz cd transformationadvisor-* It is necessary to modify the scan performed by the Data Collector to include the org.pwte package as the Data Collector doesn't scan org.* packages by default. Open the conf/customCmd.properties file and modify it as shown below: evaluation=--evaluate --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp --includePackages=org.pwte migration_liberty=--analyze --sourceAppServer=was855 --targetAppServer=liberty --targetCloud=dockerIBMCloud --includePackages=org.pwte --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp migration_was=--analyze --sourceAppServer=was855 --targetAppServer=was90 --targetCloud=vmIBMCloud --includePackages=org.pwte --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp #inventory=--inventory --excludeFiles=\".*/directory/LargeXMLFileName.xml\" #featureList=--featureList --excludeFiles=\".*/directory/LargeXMLFileName.xml\" #java_opt=-Xmx2g The following command assumes that WebSphere Application Server v855 is installed to /opt/IBM/WebSphere/AppServer855 with a profile named AppSrv01 and that the administration user is wasadmin with a password of wasadmin . Modify and issue the following command as necessary to execute the Data Collector against the WebSphere environment: ./bin/transformationadvisor -w /opt/IBM/WebSphere/AppServer855 -p AppSrv01 wasadmin wasadmin When prompted, accept the license agreement . The Data Collection process will now start and will analyze all of the applications installed in the WebSphere Application Server environment and will also collect the related Java EE artifacts such as Data Sources and JMS definitions. When the analysis is complete, the Data Collector will attempt to upload the collection results to IBM Cloud Transformation Advisor. If this is successful, you can skip to the Analyze the Recommendations section. If not, you will receive an error at the end of Data Collection and will find a file named AppSrv01.zip in your current directory as shown below. ~/datacollector/transformationadvisor-2.4.2# ls -la *.zip -rw-r--r-- 1 root root 625493 Jun 12 12:58 AppSrv01.zip Download this Data Collector Results zip file ready for uploading to IBM Cloud Transformation Advisor in the next section Upload the Data Collector results In this section the results from the Data Collector will be uploaded to IBM Cloud Transformation Advisor. In the IBM Cloud Transformation Advisor web browser session, click the Recomendations link in the top left corner and then click the Upload data button as shown below When the Upload data dialog is displayed, use the Drop or Add File button to select the Data Collector Results zip file that was downloaded in the previous section. Click Upload After a few moments the upload of the data collector results will be completed. Analyze the Recommendations Once the Data Collector Results have been uploaded to IBM Cloud Transformation Advisor a set of recommendations will be created and shown on the Recommendations page. In this section the recommendations will be analyzed and interpreted. The default recommendations are based on a target runtime of Liberty runtimes . In this scenario the desired target runtime is WebSphere Traditional . Use the Migration target drop down to select WebSphere traditional as shown below. The Data Collector analyzed all of the applications running on the traditional WebSphere profile a displays a row in the chart for each application. In the case of the CustomerOrderServicesApp.ear application, IBM Cloud Transformation Advisor has determined that the migration to WebSphere Traditional is of Moderate complexity and that there are four Severe Issues that have been detected. Click on the CustomerOrderServicesApp.ear application name to see more information. Review the analysis results and scroll down to the Technology Issues section. Note that IBM Cloud Transformation Advisor has detected that there are issues with JPA, specifically that the second-level cache and the JPA configuration properties must be migrated and with JAX-RS (missing Apache and Wink packages). These issues are related to a decision that was taken by IBM to allow WebSphere Application Server V9 to run in either JPA 2.0 or JPA 2.1 mode as described here and in either JAX-RS 2.0 or JAX-RS 1.1 mode as described here . In order to run in JPA 2.1 mode and JAX-RS 2.0 mode, the changes highlighted by IBM Cloud Transformation Advisor must be made to the application. However, this application can run in JPA 2.0 mode and JAX-RS 1.1 mode with no changes . In order to review the IBM Cloud Transformation Advisor results in more detail, scroll to the bottom of the analysis page and click on the Analysis Report link When the warning dialog is displayed, click OK The Detailed Migration Analysis Report will be displayed which show the results of the migration rules that were executed by the Data Collector and returned results. Scroll down to the Severe Rules section and click on the Show rule help link for each of the results. Review the recommendations. Final Analysis The intention of this traditional WebSphere V855 --> traditional WebSphere V9 scenario is to migrate the Customer Order Services application to the new runtime without code changes. IBM Cloud Transformation Advisor was used to analyze the application for compatibility with traditional WebSphere V9 and determined that code changes would be required. IBM Cloud Transformation Advisor took the approach that the application should be modified to run with the JPA 2.1 runtime and the JAX-RS 2.0 runtime instead of giving the option to use the JPA 2.0 runtime and the JAX-RS 1.1 runtime which would have resulted in not requiring any code changes to the application. The remainder of this scenario is based on the decision to use the JPA 2.0 runtime option and the JAX-RS 1.1 runtime option in traditional WebSphere V9 and as such no code changes will be required to this application.","title":"Traditional WebSphere - Analyze"},{"location":"OperationalModernization/extras/WAS-analyze/#traditional-websphere-analyze","text":"This section covers how to use IBM Cloud Transformation Advisor to analyze an existing traditional WebSphere application. For this scenario the traditional WebSphere Application Server runtime is chosen as the target runtime and the intention is to migrate this application without code changes. Migrating to the containerized version of traditional WebSphere Application Server will prepare the organization for: moving workloads to the cloud. improving DevOps and speed-to-market. receiving the benefits of the consistency and reliability of containers.","title":"Traditional WebSphere - Analyze"},{"location":"OperationalModernization/extras/WAS-analyze/#summary","text":"This section has the following steps: Introduction to IBM Cloud Transformation Advisor Install IBM Cloud Transformation Advisor Download and run the Data Collector Upload and analyze the results Determine the migration/modernization path and next steps","title":"Summary"},{"location":"OperationalModernization/extras/WAS-analyze/#introduction-to-ibm-cloud-transformation-advisor","text":"IBM Cloud Transformation Advisor helps you access, analyze and modernize middleware based apps into IBM Cloud(s). It categorizes Java EE apps and MQ queue managers as simple, medium and complex based on migration complexity and provides guidance for modernization. IBM Cloud Transformation Advisor can accelerate the process to move your on-premises apps to cloud, minimize migration errors and risks, and reduce time to market in five steps. You can use IBM Transformation Advisor for these activities: - Identify the Java EE programming models in an app - Determine the complexity of apps by reviewing a high-level inventory of the content and structure of each app - Highlight the Java EE programming model and WebSphere API differences between the profile types - Learn any Java EE specification implementation differences that might affect the app The tool also provides suggestions for the right-fit IBM WebSphere Application Server edition and offers advice, practices, and potential solutions to move apps to Liberty or to newer versions of WebSphere Application Server.","title":"Introduction to IBM Cloud Transformation Advisor"},{"location":"OperationalModernization/extras/WAS-analyze/#install-ibm-cloud-transformation-advisor","text":"IBM Cloud Transformation Advisor is split in to two components (the analysis engine and the data collector ). Access Transformation Advisor from IBM WebSphere Hybrid Edition. You can also run it locally on a machine with Docker. See Installing IBM Cloud Transformation Advisor locally .","title":"Install IBM Cloud Transformation Advisor"},{"location":"OperationalModernization/extras/WAS-analyze/#download-the-data-collector","text":"Once IBM Cloud Transformation Advisor is installed, it is necessary to create a new Workspace and Collection and then download the Data Collector that will be used to examine the existing environment and applications. Open IBM Cloud Transformation Advisor in a browser and click the button to create a new Workspace Enter a Workspace name such as TransformationAdvisorDemo and click Next Enter a Collection name such as WAS855_AppSrv01 and click Let's go When the No recommendations available page is displayed, click the Data Collector button When the Data Collector page is displayed, select the Source Operating System for your environment and click the Download button to download the Data Collector. This results in a file with a name similar to transformationadvisor-Linux_TransformationAdvisorDemo_WAS855_AppSrv01.tgz being downloaded.","title":"Download the Data Collector"},{"location":"OperationalModernization/extras/WAS-analyze/#run-the-data-collector","text":"Upload the Data Collector zip file that was downloaded from IBM Cloud Transformation Advisor in the previous step to the machine that the WebSphere ND Deployment Manager or the Standalone WebSphere Application Server is installed. The directory used arbitrary. Navigate to the directory you uploaded the transformationadvisor-Linux_TransformationAdvisorDemo_WAS855_AppSrv01.tgz file to and issue the following commands to extract the Data Collector: mkdir datacollector cd datacollector mv transformationadvisor-Linux_TransformationAdvisorDemo_WAS855_AppSrv01.tgz . tar -zxvf transformationadvisor-Linux_TransformationAdvisorDemo_WAS855_AppSrv01.tgz cd transformationadvisor-* It is necessary to modify the scan performed by the Data Collector to include the org.pwte package as the Data Collector doesn't scan org.* packages by default. Open the conf/customCmd.properties file and modify it as shown below: evaluation=--evaluate --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp --includePackages=org.pwte migration_liberty=--analyze --sourceAppServer=was855 --targetAppServer=liberty --targetCloud=dockerIBMCloud --includePackages=org.pwte --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp migration_was=--analyze --sourceAppServer=was855 --targetAppServer=was90 --targetCloud=vmIBMCloud --includePackages=org.pwte --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp #inventory=--inventory --excludeFiles=\".*/directory/LargeXMLFileName.xml\" #featureList=--featureList --excludeFiles=\".*/directory/LargeXMLFileName.xml\" #java_opt=-Xmx2g The following command assumes that WebSphere Application Server v855 is installed to /opt/IBM/WebSphere/AppServer855 with a profile named AppSrv01 and that the administration user is wasadmin with a password of wasadmin . Modify and issue the following command as necessary to execute the Data Collector against the WebSphere environment: ./bin/transformationadvisor -w /opt/IBM/WebSphere/AppServer855 -p AppSrv01 wasadmin wasadmin When prompted, accept the license agreement . The Data Collection process will now start and will analyze all of the applications installed in the WebSphere Application Server environment and will also collect the related Java EE artifacts such as Data Sources and JMS definitions. When the analysis is complete, the Data Collector will attempt to upload the collection results to IBM Cloud Transformation Advisor. If this is successful, you can skip to the Analyze the Recommendations section. If not, you will receive an error at the end of Data Collection and will find a file named AppSrv01.zip in your current directory as shown below. ~/datacollector/transformationadvisor-2.4.2# ls -la *.zip -rw-r--r-- 1 root root 625493 Jun 12 12:58 AppSrv01.zip Download this Data Collector Results zip file ready for uploading to IBM Cloud Transformation Advisor in the next section","title":"Run the Data Collector"},{"location":"OperationalModernization/extras/WAS-analyze/#upload-the-data-collector-results","text":"In this section the results from the Data Collector will be uploaded to IBM Cloud Transformation Advisor. In the IBM Cloud Transformation Advisor web browser session, click the Recomendations link in the top left corner and then click the Upload data button as shown below When the Upload data dialog is displayed, use the Drop or Add File button to select the Data Collector Results zip file that was downloaded in the previous section. Click Upload After a few moments the upload of the data collector results will be completed.","title":"Upload the Data Collector results"},{"location":"OperationalModernization/extras/WAS-analyze/#analyze-the-recommendations","text":"Once the Data Collector Results have been uploaded to IBM Cloud Transformation Advisor a set of recommendations will be created and shown on the Recommendations page. In this section the recommendations will be analyzed and interpreted. The default recommendations are based on a target runtime of Liberty runtimes . In this scenario the desired target runtime is WebSphere Traditional . Use the Migration target drop down to select WebSphere traditional as shown below. The Data Collector analyzed all of the applications running on the traditional WebSphere profile a displays a row in the chart for each application. In the case of the CustomerOrderServicesApp.ear application, IBM Cloud Transformation Advisor has determined that the migration to WebSphere Traditional is of Moderate complexity and that there are four Severe Issues that have been detected. Click on the CustomerOrderServicesApp.ear application name to see more information. Review the analysis results and scroll down to the Technology Issues section. Note that IBM Cloud Transformation Advisor has detected that there are issues with JPA, specifically that the second-level cache and the JPA configuration properties must be migrated and with JAX-RS (missing Apache and Wink packages). These issues are related to a decision that was taken by IBM to allow WebSphere Application Server V9 to run in either JPA 2.0 or JPA 2.1 mode as described here and in either JAX-RS 2.0 or JAX-RS 1.1 mode as described here . In order to run in JPA 2.1 mode and JAX-RS 2.0 mode, the changes highlighted by IBM Cloud Transformation Advisor must be made to the application. However, this application can run in JPA 2.0 mode and JAX-RS 1.1 mode with no changes . In order to review the IBM Cloud Transformation Advisor results in more detail, scroll to the bottom of the analysis page and click on the Analysis Report link When the warning dialog is displayed, click OK The Detailed Migration Analysis Report will be displayed which show the results of the migration rules that were executed by the Data Collector and returned results. Scroll down to the Severe Rules section and click on the Show rule help link for each of the results. Review the recommendations.","title":"Analyze the Recommendations"},{"location":"OperationalModernization/extras/WAS-analyze/#final-analysis","text":"The intention of this traditional WebSphere V855 --> traditional WebSphere V9 scenario is to migrate the Customer Order Services application to the new runtime without code changes. IBM Cloud Transformation Advisor was used to analyze the application for compatibility with traditional WebSphere V9 and determined that code changes would be required. IBM Cloud Transformation Advisor took the approach that the application should be modified to run with the JPA 2.1 runtime and the JAX-RS 2.0 runtime instead of giving the option to use the JPA 2.0 runtime and the JAX-RS 1.1 runtime which would have resulted in not requiring any code changes to the application. The remainder of this scenario is based on the decision to use the JPA 2.0 runtime option and the JAX-RS 1.1 runtime option in traditional WebSphere V9 and as such no code changes will be required to this application.","title":"Final Analysis"},{"location":"OperationalModernization/extras/application/","text":"Application Overview The Customer Order Services application is a simple store-front shopping application, built during the early days of the Web 2.0 movement. Users interact directly with a browser-based interface and manage their cart to submit orders. This application is built using the traditional 3-Tier Architecture model, with an HTTP server, an application server, and a supporting database. There are several components of the overall application architecture: - Starting with the database, the application leverages two SQL-based databases running on IBM DB2. The application exposes its data model through an Enterprise JavaBean layer, named CustomerOrderServices . This component leverages the Java Persistence API to expose the backend data model to calling services with minimal coding effort. This build of the application uses JavaEE6 features for EJBs and JPA. The next tier of the application, named CustomerOrderServicesWeb , exposes the necessary business APIs via REST-based web services. This component leverages the JAX-RS libraries for creating Java-based REST services with minimal coding effort. This build of the application is using JAX-RS 1.1 version of the respective capability. The application's user interface is exposed through the CustomerOrderServicesWeb component as well, in the form of a Dojo Toolkit-based JavaScript application. Delivering the user interface and business APIs in the same component is one major inhibitor our migration strategy will help to alleviate in the long-term. Finally, there is an additional integration testing component, named CustomerOrderServicesTest that is built to quickly validate an application's build and deployment to a given application server. This test component contains both JPA and JAX-RS -based tests.","title":"Application Overview"},{"location":"OperationalModernization/extras/application/#application-overview","text":"The Customer Order Services application is a simple store-front shopping application, built during the early days of the Web 2.0 movement. Users interact directly with a browser-based interface and manage their cart to submit orders. This application is built using the traditional 3-Tier Architecture model, with an HTTP server, an application server, and a supporting database. There are several components of the overall application architecture: - Starting with the database, the application leverages two SQL-based databases running on IBM DB2. The application exposes its data model through an Enterprise JavaBean layer, named CustomerOrderServices . This component leverages the Java Persistence API to expose the backend data model to calling services with minimal coding effort. This build of the application uses JavaEE6 features for EJBs and JPA. The next tier of the application, named CustomerOrderServicesWeb , exposes the necessary business APIs via REST-based web services. This component leverages the JAX-RS libraries for creating Java-based REST services with minimal coding effort. This build of the application is using JAX-RS 1.1 version of the respective capability. The application's user interface is exposed through the CustomerOrderServicesWeb component as well, in the form of a Dojo Toolkit-based JavaScript application. Delivering the user interface and business APIs in the same component is one major inhibitor our migration strategy will help to alleviate in the long-term. Finally, there is an additional integration testing component, named CustomerOrderServicesTest that is built to quickly validate an application's build and deployment to a given application server. This test component contains both JPA and JAX-RS -based tests.","title":"Application Overview"},{"location":"RuntimeModernization/","text":"Runtime Modernization Table of Contents Introduction Analysis (Hands-on) Build (Hands-on) Deploy (Hands-on) Access the Application (Hands-on) Review Deployment Cleanup (can be skipped if the next lab Application Management is included in the same session) Extra Credit Summary Next Introduction Runtime modernization moves an application to a 'built for the cloud' runtime with the least amount of effort. Open Liberty is a fast, dynamic, and easy-to-use Java application server. Ideal for the cloud, Liberty is open sourced, with fast start-up times (<2 seconds), no server restarts to pick up changes, and a simple XML configuration. This path gets the application on to a cloud-ready runtime container which is easy to use and portable. In addition to the necessary library changes, some aspects of the application were modernized. However, it has not been 'modernized' to a newer architecture such as micro-services . This lab demonstrates runtime modernization . It uses the Customer Order Services application, which originates from WebSphere ND V8.5.5. Click here to get to know the application, including its architecture and components. The application will go through the analysis , build and deploy phases. It is modernized to run on the Liberty runtime, and deployed via the IBM Cloud Pak for Applications to RedHat OpenShift Container Platform (OCP). Login to the VM If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password: \\ Password: engageibm Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window. Analysis (Hands-on) In this lab, we will demonstrate how the Transformation Advisor can be used specifically in the runtime modernization process. The steps needed to analyze the existing Customer Order Services application are: Open a Firefox browser window from within the VM. Click on the openshift console bookmark in the top left and log in with the htpasswd option. Log in to the OpenShift account using the following credentials: Username: ibmadmin Password: engageibm From the Red Hat OpenShift Container Platform console, go to the Networking tab and click on Routes . Ensure that you are in the ta project by using the project drop down and click on the Location URL next to ta-ui-route . This will open the Transformation Advisor user interface. Click Create new under Workspaces to create a new workspace. Name it RuntimeModernization and click Next . You'll be asked to create a new collection to store the data collected from the Customer Order Services application. Name it CustomerOrderServices . Click Create . The No recommendations available page is displayed. To provide data and receive recommendations, you can either download and execute the Data Collector against an existing WebSphere environment, or upload an existing data collection archive. The archive has already been created for you and the resulting data is stored here . Upload the results of the data collection (the datacollector.zip file) to IBM Cloud Transformation Advisor. When the upload is complete, you will see a list of applications analyzed from the source environment. At the top of the page, you can see the source environment and the target environment settings. Under the Migration target field, click the down arrow and select Compatible runtimes . This shows an entry for each application for each compatible destination runtime you can migrate it to. Click the CustomerOrderServicesApp.ear application with the Open Liberty migration target to open the Application details page . This lab covers runtime modernization, so the application will be re-platformed to run on Open Liberty, and will be placed in a container and deployed to OCP. Look over the migration analysis which shows a summary of the complexity of migrating this application to this target. In summary, migration of this application to Open Liberty is of Moderate complexity as some code changes may be required. (Note: there may be a severe issue related to third-party APIs, but this doesn't apply as they occur in test code.) Click on View migration plan in the top right corner of the page. This page will help you assemble an archive containing: - your application's source or binary files (you upload these here or specify Maven coordinates to download them) - the wsadmin scripts needed to configure your application and its resources (generated by Transformation Advisor and automatically included) - the deployment artifacts needed to create the container image and deploy the application to OCP (generated by Transformation Advisor and automatically included) NOTE: These artifacts have already been provided for you as part of the lab files, so you don't need to download the migration plan. However, you can do so if you wish to look around at the files. These files can also be sent to a Git repository by Transformation Advisor. Build (Hands-on) In this section, you'll learn how to build a Docker image for Customer Order Services application running on Liberty. Building this image could take around ~3 minutes so let's kick that process off and then come back to learn what you did. Open a new terminal window from the VM desktop. Login to OpenShift CLI with the oc login command from the web terminal. When prompted for the username and password, enter the following login credentials: Username: ibmadmin Password: engageibm If you have not yet cloned the GitHub repo with the lab artifacts, run the following command on your terminal: git clone https://github.com/IBM/openshift-workshop-was.git Change directory to where this lab is located: cd openshift-workshop-was cd labs/Openshift/RuntimeModernization ls Run the following command to start building the image. Make sure to copy the entire command, including the \".\" at the end (indicated as the location of current directory). While the image is building (which takes ~3 minutes), continue with rest of the lab: docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos . Modernize with MicroProfile (for reading only) Eclipse MicroProfile is a modular set of technologies designed so that you can write cloud-native microservices. However, even though our application has not been refactored into microservices, we can still take advantage of some of the technologies from MicroProfile. Determine application's availability (for reading only) In the last lab, we used /CustomerOrderServicesWeb/index.html for readiness and liveness probes, which is not the best indication that an application is ready to handle traffic or is healthy to process requests correctly within a reasonable amount of time. What if the database is down? What if the application's security layer is not yet ready/unable to handle authentication? The Pod would still be considered ready and healthy and traffic would still be sent to it. All of those requests would fail or queue up, leading to bigger problems. MicroProfile Health provides a common REST endpoint format to determine whether a microservice (or in our case a monolith application) is healthy or not. The service itself might be running but considered unhealthy if the things it requires for normal operation are unavailable. All of the checks are performed periodically, and the result is served as a simple UP or DOWN at /health/ready and /health/live which can be used for readiness and liveness probes. We implemented the following health checks: - ReadinessCheck : The application reports it is ready as long as the readiness check endpoint can be reached. Connections to other services and any other required conditions for the application to be considered ready are checked here. return HealthCheckResponse.named(\"Readiness\").up().build(); LivenessCheck : The requests should be processed within a reasonable amount of time. Monitor thread block times to identify potential deadlocks which can cause the application to hang. ThreadMXBean tBean = ManagementFactory.getThreadMXBean(); long ids[] = tBean.findMonitorDeadlockedThreads(); if (ids !=null) { ThreadInfo threadInfos[] = tBean.getThreadInfo(ids); for (ThreadInfo ti : threadInfos) { double seconds = ti.getBlockedTime() / 1000.0; if (seconds > 60) { return HealthCheckResponse.named(\"Liveness\").down().build(); } } } return HealthCheckResponse.named(\"Liveness\").up().build(); Adding metrics to application (for reading only) MicroProfile Metrics is used to gather metrics about the time it takes to add an item to cart, retrieve customer information, and count the number of times these operations are performed. @GET @Produces(MediaType.APPLICATION_JSON) @Counted @Timed(name = \"getCustomer_timed\") public Response getCustomer() { Liberty server configuration (for reading only) The Liberty runtime configuration files are based on a template provided by IBM Cloud Transformation Advisor. For this lab, instead of using a single server.xml, the configurations have been split into multiple configuration files and placed into the config/configDropins/overrides directory. You may place configuration files into configDropins/overrides directory to override pre-existing configurations. You may define separate template configurations that reflect the resources in your environment, and copy them into the configDropsins/overrides directory only for those applications that need them. Build image (Hands-on) Go back to your terminal to check the build you started earlier. You should see the following message if the image was built successfully. Please wait if it's still building: Successfully tagged default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos:latest Validate that the image is in the repository via the command line: docker images You should see the following images on the output. Notice that the base image, openliberty/open-liberty , is also listed. It was pulled as the first step of building application image. Example output: REPOSITORY TAG IMAGE ID CREATED SIZE default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos latest 4758119add3f 2 minutes ago 883MB <none> <none> 5bcb83fad548 5 minutes ago 792MB openliberty/open-liberty full-java8-openj9-ubi e6b5411076fe 5 days ago 794MB maven latest 1337d55397f7 4 weeks ago 631MB Before we push the image to OpenShift's internal image registry, create a separate project named apps . Choose one of two ways to create the project: Via the command line: oc new-project apps Example output: Now using project \"apps\" on server \"https://api.demo.ibmdte.net:64 . . . Via the OpenShift console: Open a Firefox browser window and click on the openshift console bookmark. Under the Home tab on the left menu, click Projects . Click on the Create Project button. Enter apps for the Name field and click on Create . Return to your terminal window. Switch the current project in the command line to apps oc project apps Enable monitoring by adding the necessary label to the apps namespace. Choose one of two options to label the namespace: Via the command line: oc label namespace apps app-monitoring=true Example output: namespace/apps labeled Via the OpenShift console: Under the Administration tab on the left menu, click on Namespaces . Click on the menu-options for apps namespace Click on Edit Labels . Copy and paste app-monitoring=true into the text box . Click Save . Login to the image registry via the command line: Note: From below command, a session token is obtained from the value of another command oc whoami -t and used as the password to login. docker login -u openshift -p $(oc whoami -t) default-route-openshift-image-registry.apps.demo.ibmdte.net Example output: WARNING! Using --password via the CLI is insecure. Use --password-stdin. WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded Push the image to OpenShift's internal image registry via the command line, which could take up to a minute: docker push default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos Example output: The push refers to repository [default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos] 9247390b40be: Pushed 9a21ca46f8e3: Pushed b3cee8ba43fe: Pushed 1dd2f7265f58: Pushed 33b2a4ee94ff: Pushed 2b2a8abdd0c4: Pushed 91ffc437f551: Pushed 4f04e7098d96: Pushed 248016390e0a: Pushed 0fa7eb58a57c: Pushed b5489882eed9: Pushed 2fb5caadbbb0: Pushed d06182ac791b: Pushed b39b0291530b: Pushed a04c77af4b60: Pushed 479c44e860ff: Pushed fc905c23b8a3: Pushed 161ec220381b: Pushed b7b591e3443f: Pushed ccf04fbd6e19: Pushed latest: digest: sha256:56d926b7ef64ed163ff026b7b5608ae97df4630235c1d0443a32a4fc8eb35a6c size: 4513 Verify that the image is in image registry via the command line: oc get images | grep apps/cos The application image you just pushed should be listed. Example output: sha256:56d926b7ef64ed163ff026b7b5608ae97df4630235c1d0443a32a4fc8eb35a6c image-registry.openshift-image-registry.svc:5000/apps/cos@sha256:56d926b7ef64ed163ff026b7b5608ae97df4630235c1d0443a32a4fc8eb35a6c Verify that the image stream is created via the command line: oc get imagestreams Example output: NAME IMAGE REPOSITORY TAGS UPDATED cos default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos latest 2 minutes ago You may also check the image stream via the console: Return to the OpenShift console through your Firefox browser window. Under the Builds tab in the left menu, click on Image Streams . Select project apps from the Project drop-down list. Click on cos from the list. Scroll down to the bottom to see the image that you pushed. Deploy (Hands-on) Customer Order Services application uses DB2 as its database. To deploy it to Liberty, a separate instance of the database is already pre-configured in the OpenShift cluster you are using. The database is exposed within the cluster using a Service , and the application references this database using the address of the Service . Deploy the application using the -k , or kustomize option of Openshift CLI now and we will explain how the deployment works in a later section. Preview what will be deployed: oc kustomize deploy/overlay-apps Example output of yaml: apiVersion: v1 data: DB_HOST: cos-db-liberty.db.svc kind: ConfigMap metadata: name: cos-config namespace: apps --- apiVersion: v1 data: DB_PASSWORD: ZGIyaW5zdDE= DB_USER: ZGIyaW5zdDE= kind: Secret metadata: name: db-creds namespace: apps type: Opaque --- apiVersion: v1 kind: Secret metadata: name: liberty-creds namespace: apps stringData: password: admin username: admin type: Opaque --- apiVersion: openliberty.io/v1beta1 kind: OpenLibertyApplication metadata: name: cos namespace: apps spec: applicationImage: image-registry.openshift-image-registry.svc:5000/apps/cos envFrom: - configMapRef: name: cos-config - secretRef: name: db-creds expose: true livenessProbe: httpGet: path: /health/live port: 9443 scheme: HTTPS monitoring: endpoints: - basicAuth: password: key: password name: liberty-creds username: key: username name: liberty-creds interval: 5s scheme: HTTPS tlsConfig: insecureSkipVerify: true labels: app-monitoring: \"true\" pullPolicy: Always readinessProbe: httpGet: path: /health/ready port: 9443 scheme: HTTPS route: insecureEdgeTerminationPolicy: Redirect termination: reencrypt service: annotations: service.beta.openshift.io/serving-cert-secret-name: cos-tls certificateSecretRef: cos-tls port: 9443 Run the following command to deploy the yaml files: oc apply -k deploy/overlay-apps Output of deploy command: configmap/cos-config created secret/db-creds created secret/liberty-creds created openlibertyapplication.openliberty.io/cos created Verify that the route is created for your application: oc get route cos Example output: NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD cos cos-apps.apps.demo.ibmdte.net cos 9443-tcp reencrypt/Redirect None Verify that your pod from the project apps is ready: First, confirm you're at the current project apps : oc project If it's not at project apps , then switch: oc project apps and then run the following command to view the pod status: oc get pod Example output of pod status: NAME READY STATUS RESTARTS AGE cos-596b4f849f-2fg4h 1/1 Running 0 18m - If the pod doesn't display the expected Running status (for example, after 5 minutes), then delete the pod to restart it. From the command line, run oc delete pod <pod name> (pod name is the string under NAME column from the output of oc get pod ) Access the application (Hands-on) Run the following command to get the URL of your application: echo http://$(oc get route cos --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-apps.apps.demo.ibmdte.net/CustomerOrderServicesWeb Return to your Firefox browser window and go to the URL outputted by the previous command. You'll be shown a login dialog. Login with user skywalker and password force . (The user is pre-created/registered in the basicRegistry configured in Liberty.) After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total . Close the browser. Review the application workload flow with Open Liberty Operator (Hands-on) Return to the OpenShift console through a Firefox window to view the resources on deployment. View the resources in the project openshift-operators : Select the openshift-operators project from the project drop down menu at the top of the page. View the operator's deployment details: Click on the Deployments tab under Workloads from the left menu and select open-liberty-operator Navigate to the YAML tab to view the content of yaml View the operator's pod details: Click on the Pods tab under Workloads from the left menu, and select the pod starting with open-liberty-operator Navigate to the Logs tab to view the open-liberty-operator container log Navigate to the Terminal tab to view the files in the container View the resources in the project apps : Select the apps project from the project drop down menu at the top of the page. View Open Liberty Application instance details: Click on the Installed Operators tab under Operators from the left menu and select Open Liberty Operator . Note: The operator is installed at cluster level and is visible to all existing projects, but Open Liberty Application instance is created under the project apps . Navigate to the Open Liberty Application tab and select cos to view the details of the Open Liberty Application instance Navigate to the YAML tab to view the content of yaml Navigate to the Resources tab to view the resources of Open Liberty Application instance View application deployment details: Click on the Deployments tab under Workloads from the left menu and select cos Navigate to the YAML tab to view the content of yaml. Note the deployment is created through the controller of the OpenLibertyApplication custom resource. View application pod details: Click on the Pods tab under Workloads from the left menu and select the pod starting with cos- Navigate to the Logs tab to view the liberty access log Note: by default, the Open Liberty Application instance is configured with liberty access log: View application service details: Click on the Services tab under Networking from the left menu and select cos Navigate to the YAML tab to view the content of yaml. > Note the service is created through the controller of OpenLibertyApplication custom resource. View application route details: Click on the Routes tab under Networking from the left menu and select cos Navigate to the YAML tab to view the content of yaml. >Note the route is created through the controller of OpenLibertyApplication custom resource. View application secret details: First, return to the YAML of the Open Liberty Application instance to view the configured secrets: Click on the Secrets tab under Workloads from the left menu and select the respective secrets and view the details View the resources in the project db : Select the db project from the project drop down menu. View db deployment details: Click on the Deployments tab under Workloads from the left menu and select cos-db-liberty Navigate to the YAML tab to view the content of yaml View db pod details: Click on the Pods tab under Workloads from the left menu and select the pod starting with cos-db-liberty Navigate to the Logs tab to view the database logs Navigate to the Terminal tab to view the files in the database container View db service details: Click on the Services tab under Networking from the left menu and select cos-db-liberty Review Deployment Let's review the configuration files used for our deployment. Our configuration files are structured for the -k , or kustomize option of Openshift CLI. Kustomize is a separate tool that has been integrated into Openshift CLI that allows you to customize yaml without using variables. You can define a base directory and override directories to customize the base directory Make sure you are in directory /openshift-workshop-was/labs/Openshift/RuntimeModernization or change directory with cd openshift-workshop-was/labs/Openshift/RuntimeModernization List the deploy files: ls deploy And the output shows that we have one base directory and one override directory: base overlay-apps Take a look at what's in the base directory: ls deploy/base And the output: kustomization.yaml olapp-cos.yaml Each directory used for kustomization contains one kustomization.yaml cat deploy/base/kustomization.yaml Content of yaml: - This is a simple kustomization directory that just lists the yaml files to be deployed. resources: - olapp-cos.yaml The file olapp-cos.yaml contains the custom resource definition to deploy the application and will be covered in detail later. Take a look at the files in the overlay-apps directory. ls deploy/overlay-apps And the output: configmap.yaml kustomization.yaml secret-db-creds.yaml secret-liberty-creds.yaml Take a look at the kustomization.yaml in the overlay-apps directory: cat deploy/overlay-apps/kustomization.yaml And the output: namespace: apps resources: - configmap.yaml - secret-db-creds.yaml - secret-liberty-creds.yaml bases: - ./../base Note that: - The namespace is defined. This means that all resources originating from this directory will be applied to the apps namespace. - Resources from the base directory will also be included. - You may define additional overlay directories for different environments, each with a different namespace. For example, overlay-test, overlay-prod. - The configurations in this directory contain the overrides specific to this environment. - For a real environment, DO NOT store the secret yamls into source control. It is a security expsoure. See extra credit section on how to secure your secrets. To preview the resources that will be applied for a specific override directory, use the kustomize option of the Openshift command line. For example, oc kustomize deploy/overlay-apps The output is the same as displayed in Deploy (Hands-on) section. Secrets Specifying credentials and tokens in plain text is not secure. Secrets are used to store sensitive information. The stored data can be referenced by other resources. OpenShift handles secrets with special care. For example, they will not be logged or shown anywhere. There are two secrets - one for database credentials and one for Liberty metrics credentials, which is needed to access the /metrics endpoint. The file secret-db-creds.yaml contains the credentials for the database. It is injected into the container as an environment variable via the secretRef specification for the Open Liberty Operator. View the content of secret-db-creds.yaml file: cat deploy/overlay-apps/secret-db-creds.yaml Example output: kind: Secret apiVersion: v1 metadata: name: db-creds data: DB_PASSWORD: ZGIyaW5zdDE= DB_USER: ZGIyaW5zdDE= type: Opaque The file secret-liberty-creds.yaml contains the secret to access liberty server. View the content of secret-liberty-creds.yaml file: cat deploy/overlay-apps/secret-liberty-creds.yaml Example output: kind: Secret apiVersion: v1 metadata: name: liberty-creds stringData: username: admin password: admin type: Opaque Note that the first Secret provides the credentials in base64 encoded format using the data field. The second one provides them in plain text using the stringData field. OpenShift will automatically convert the credentials to base64 format and place the information under the data field. We can see this by viewing the YAML of the liberty-creds secret: Return to the OpenShift console through a Firefox browser window. Select the apps project from the project drop down menu. Click on the Secrets tab under Workloads from the left menu and search for the liberty-creds secret. Navigate to the YAML tab. Note that the data field contains the credentials in encoded form. Configmap Configmaps allows you to store name/value pairs that can be injected into your container when it starts. For our example, the values of the configmap.yaml are injected as environment variables in the configMapRef specification on the Open Liberty Operator in the next section. cat deploy/overlay-apps/configmap.yaml Example output: apiVersion: v1 kind: ConfigMap metadata: name: cos-config data: SEC_TLS_TRUSTDEFAULTCERTS: \"true\" DB_HOST : \"cos-db-liberty.db.svc\" Open Liberty Operator We could have created Deployment, Service, and Route resources to deploy the Liberty image. However, for this lab we will use the Open Liberty Operator instead. The Open Liberty Operator provides all functionalities of Runtime Component Operator used when deploying traditional WebSphere images in a previous lab. In addition, it also offers Open Liberty specific capabilities, such as day-2 operations (gather trace & dumps) and single sign-on (SSO). The file deploy/base/olapp-cos.yaml looks like: apiVersion: openliberty.io/v1beta1 kind: OpenLibertyApplication metadata: name: cos namespace: apps spec: applicationImage: 'image-registry.openshift-image-registry.svc:5000/apps/cos' pullPolicy: Always readinessProbe: httpGet: path: /health/ready port: 9443 scheme: HTTPS livenessProbe: httpGet: path: /health/live port: 9443 scheme: HTTPS service: annotations: service.beta.openshift.io/serving-cert-secret-name: cos-tls certificateSecretRef: cos-tls port: 9443 expose: true route: termination: reencrypt insecureEdgeTerminationPolicy: Redirect env: envFrom: - configMapRef: name: cos-config - secretRef: name: db-creds monitoring: endpoints: - basicAuth: password: key: password name: liberty-creds username: key: username name: liberty-creds interval: 5s scheme: HTTPS tlsConfig: insecureSkipVerify: true labels: app-monitoring: 'true' The OpenLibertyApplication is a custom resource supported by the Open Liberty Operator, which is designed to help you with Liberty deployment. It allows you to provide Liberty specific configurations (day-2 operations, single sign-on). The application image you pushed earlier to internal image registry is specified by the applicationImage parameter. MicroProfile Health endpoints /health/ready and /health/live are used for readiness and liveness probes. The configMapRef surfaces all entries of the ConfigMap cos-config as environment variables. The secretRef surfaces the entries in the Secret db-creds as environment variables. These are the database user and password. Enabled application monitoring so that Prometheus can scrape the information provided by MicroProfile Metric's /metrics endpoint in Liberty. The /metrics endpoint is protected, hence the credentials are provided using the Secret liberty-creds you created earlier. Cleanup (Hands-on) (Skip this step if you're going to run the next lab Application Management on the same assigned cluster) The controller for the Open Liberty Operator creates the necessary Deployment, Service, and Route objects for the Customer Order Services application. To list these resources, run the commands: Reminder: Run oc project to confirm you're at the apps project before running the following commands. oc get deployment oc get service oc get route Example output: # oc get deployment NAME READY UP-TO-DATE AVAILABLE AGE cos 1/1 1 1 2d18h # oc get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cos ClusterIP 172.21.202.9 <none> 9443/TCP 2d18h # oc get route NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD cos cos-apps.apps.demo.ibmdte.net cos 9443-tcp reencrypt/Redirect None To remove these resources, run the command (Ensure you are in directory openshift-workshop-was/labs/Openshift/RuntimeModernization ) Note: The pre-installed resources such as Open Liberty Operator and DB2, are not removed. oc delete -k deploy/overlay-apps Output: configmap \"cos-config\" deleted secret \"db-creds\" deleted secret \"liberty-creds\" deleted openlibertyapplication.openliberty.io \"cos\" deleted Double check the corresponding Deployment, Service, and Route objects are deleted: oc get deployment oc get service oc get route Output from each get command above: No resources found in apps namespace. Extra Credit Read about how to protect your secrets: https://www.openshift.com/blog/gitops-secret-management Summary Congratulations! You've completed Runtime Modernization lab! This application has been modified from the initial WebSphere ND v8.5.5 version to run on modern & cloud-native runtime Open Liberty and deployed by IBM Cloud Pak for Applications to RedHat OpenShift. Next Follow the link to the next lab Application Management : - Application Management","title":"Runtime Modernization"},{"location":"RuntimeModernization/#runtime-modernization","text":"","title":"Runtime Modernization"},{"location":"RuntimeModernization/#table-of-contents","text":"Introduction Analysis (Hands-on) Build (Hands-on) Deploy (Hands-on) Access the Application (Hands-on) Review Deployment Cleanup (can be skipped if the next lab Application Management is included in the same session) Extra Credit Summary Next","title":"Table of Contents"},{"location":"RuntimeModernization/#introduction","text":"Runtime modernization moves an application to a 'built for the cloud' runtime with the least amount of effort. Open Liberty is a fast, dynamic, and easy-to-use Java application server. Ideal for the cloud, Liberty is open sourced, with fast start-up times (<2 seconds), no server restarts to pick up changes, and a simple XML configuration. This path gets the application on to a cloud-ready runtime container which is easy to use and portable. In addition to the necessary library changes, some aspects of the application were modernized. However, it has not been 'modernized' to a newer architecture such as micro-services . This lab demonstrates runtime modernization . It uses the Customer Order Services application, which originates from WebSphere ND V8.5.5. Click here to get to know the application, including its architecture and components. The application will go through the analysis , build and deploy phases. It is modernized to run on the Liberty runtime, and deployed via the IBM Cloud Pak for Applications to RedHat OpenShift Container Platform (OCP).","title":"Introduction"},{"location":"RuntimeModernization/#login-to-the-vm","text":"If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password: \\ Password: engageibm Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window.","title":"Login to the VM"},{"location":"RuntimeModernization/#analysis-hands-on","text":"In this lab, we will demonstrate how the Transformation Advisor can be used specifically in the runtime modernization process. The steps needed to analyze the existing Customer Order Services application are: Open a Firefox browser window from within the VM. Click on the openshift console bookmark in the top left and log in with the htpasswd option. Log in to the OpenShift account using the following credentials: Username: ibmadmin Password: engageibm From the Red Hat OpenShift Container Platform console, go to the Networking tab and click on Routes . Ensure that you are in the ta project by using the project drop down and click on the Location URL next to ta-ui-route . This will open the Transformation Advisor user interface. Click Create new under Workspaces to create a new workspace. Name it RuntimeModernization and click Next . You'll be asked to create a new collection to store the data collected from the Customer Order Services application. Name it CustomerOrderServices . Click Create . The No recommendations available page is displayed. To provide data and receive recommendations, you can either download and execute the Data Collector against an existing WebSphere environment, or upload an existing data collection archive. The archive has already been created for you and the resulting data is stored here . Upload the results of the data collection (the datacollector.zip file) to IBM Cloud Transformation Advisor. When the upload is complete, you will see a list of applications analyzed from the source environment. At the top of the page, you can see the source environment and the target environment settings. Under the Migration target field, click the down arrow and select Compatible runtimes . This shows an entry for each application for each compatible destination runtime you can migrate it to. Click the CustomerOrderServicesApp.ear application with the Open Liberty migration target to open the Application details page . This lab covers runtime modernization, so the application will be re-platformed to run on Open Liberty, and will be placed in a container and deployed to OCP. Look over the migration analysis which shows a summary of the complexity of migrating this application to this target. In summary, migration of this application to Open Liberty is of Moderate complexity as some code changes may be required. (Note: there may be a severe issue related to third-party APIs, but this doesn't apply as they occur in test code.) Click on View migration plan in the top right corner of the page. This page will help you assemble an archive containing: - your application's source or binary files (you upload these here or specify Maven coordinates to download them) - the wsadmin scripts needed to configure your application and its resources (generated by Transformation Advisor and automatically included) - the deployment artifacts needed to create the container image and deploy the application to OCP (generated by Transformation Advisor and automatically included) NOTE: These artifacts have already been provided for you as part of the lab files, so you don't need to download the migration plan. However, you can do so if you wish to look around at the files. These files can also be sent to a Git repository by Transformation Advisor.","title":"Analysis (Hands-on)"},{"location":"RuntimeModernization/#build-hands-on","text":"In this section, you'll learn how to build a Docker image for Customer Order Services application running on Liberty. Building this image could take around ~3 minutes so let's kick that process off and then come back to learn what you did. Open a new terminal window from the VM desktop. Login to OpenShift CLI with the oc login command from the web terminal. When prompted for the username and password, enter the following login credentials: Username: ibmadmin Password: engageibm If you have not yet cloned the GitHub repo with the lab artifacts, run the following command on your terminal: git clone https://github.com/IBM/openshift-workshop-was.git Change directory to where this lab is located: cd openshift-workshop-was cd labs/Openshift/RuntimeModernization ls Run the following command to start building the image. Make sure to copy the entire command, including the \".\" at the end (indicated as the location of current directory). While the image is building (which takes ~3 minutes), continue with rest of the lab: docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos .","title":"Build (Hands-on)"},{"location":"RuntimeModernization/#modernize-with-microprofile-for-reading-only","text":"Eclipse MicroProfile is a modular set of technologies designed so that you can write cloud-native microservices. However, even though our application has not been refactored into microservices, we can still take advantage of some of the technologies from MicroProfile.","title":"Modernize with MicroProfile (for reading only)"},{"location":"RuntimeModernization/#determine-applications-availability-for-reading-only","text":"In the last lab, we used /CustomerOrderServicesWeb/index.html for readiness and liveness probes, which is not the best indication that an application is ready to handle traffic or is healthy to process requests correctly within a reasonable amount of time. What if the database is down? What if the application's security layer is not yet ready/unable to handle authentication? The Pod would still be considered ready and healthy and traffic would still be sent to it. All of those requests would fail or queue up, leading to bigger problems. MicroProfile Health provides a common REST endpoint format to determine whether a microservice (or in our case a monolith application) is healthy or not. The service itself might be running but considered unhealthy if the things it requires for normal operation are unavailable. All of the checks are performed periodically, and the result is served as a simple UP or DOWN at /health/ready and /health/live which can be used for readiness and liveness probes. We implemented the following health checks: - ReadinessCheck : The application reports it is ready as long as the readiness check endpoint can be reached. Connections to other services and any other required conditions for the application to be considered ready are checked here. return HealthCheckResponse.named(\"Readiness\").up().build(); LivenessCheck : The requests should be processed within a reasonable amount of time. Monitor thread block times to identify potential deadlocks which can cause the application to hang. ThreadMXBean tBean = ManagementFactory.getThreadMXBean(); long ids[] = tBean.findMonitorDeadlockedThreads(); if (ids !=null) { ThreadInfo threadInfos[] = tBean.getThreadInfo(ids); for (ThreadInfo ti : threadInfos) { double seconds = ti.getBlockedTime() / 1000.0; if (seconds > 60) { return HealthCheckResponse.named(\"Liveness\").down().build(); } } } return HealthCheckResponse.named(\"Liveness\").up().build();","title":"Determine application's availability (for reading only)"},{"location":"RuntimeModernization/#adding-metrics-to-application-for-reading-only","text":"MicroProfile Metrics is used to gather metrics about the time it takes to add an item to cart, retrieve customer information, and count the number of times these operations are performed. @GET @Produces(MediaType.APPLICATION_JSON) @Counted @Timed(name = \"getCustomer_timed\") public Response getCustomer() {","title":"Adding metrics to application (for reading only)"},{"location":"RuntimeModernization/#liberty-server-configuration-for-reading-only","text":"The Liberty runtime configuration files are based on a template provided by IBM Cloud Transformation Advisor. For this lab, instead of using a single server.xml, the configurations have been split into multiple configuration files and placed into the config/configDropins/overrides directory. You may place configuration files into configDropins/overrides directory to override pre-existing configurations. You may define separate template configurations that reflect the resources in your environment, and copy them into the configDropsins/overrides directory only for those applications that need them.","title":"Liberty server configuration (for reading only)"},{"location":"RuntimeModernization/#build-image-hands-on","text":"Go back to your terminal to check the build you started earlier. You should see the following message if the image was built successfully. Please wait if it's still building: Successfully tagged default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos:latest Validate that the image is in the repository via the command line: docker images You should see the following images on the output. Notice that the base image, openliberty/open-liberty , is also listed. It was pulled as the first step of building application image. Example output: REPOSITORY TAG IMAGE ID CREATED SIZE default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos latest 4758119add3f 2 minutes ago 883MB <none> <none> 5bcb83fad548 5 minutes ago 792MB openliberty/open-liberty full-java8-openj9-ubi e6b5411076fe 5 days ago 794MB maven latest 1337d55397f7 4 weeks ago 631MB Before we push the image to OpenShift's internal image registry, create a separate project named apps . Choose one of two ways to create the project: Via the command line: oc new-project apps Example output: Now using project \"apps\" on server \"https://api.demo.ibmdte.net:64 . . . Via the OpenShift console: Open a Firefox browser window and click on the openshift console bookmark. Under the Home tab on the left menu, click Projects . Click on the Create Project button. Enter apps for the Name field and click on Create . Return to your terminal window. Switch the current project in the command line to apps oc project apps Enable monitoring by adding the necessary label to the apps namespace. Choose one of two options to label the namespace: Via the command line: oc label namespace apps app-monitoring=true Example output: namespace/apps labeled Via the OpenShift console: Under the Administration tab on the left menu, click on Namespaces . Click on the menu-options for apps namespace Click on Edit Labels . Copy and paste app-monitoring=true into the text box . Click Save . Login to the image registry via the command line: Note: From below command, a session token is obtained from the value of another command oc whoami -t and used as the password to login. docker login -u openshift -p $(oc whoami -t) default-route-openshift-image-registry.apps.demo.ibmdte.net Example output: WARNING! Using --password via the CLI is insecure. Use --password-stdin. WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded Push the image to OpenShift's internal image registry via the command line, which could take up to a minute: docker push default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos Example output: The push refers to repository [default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos] 9247390b40be: Pushed 9a21ca46f8e3: Pushed b3cee8ba43fe: Pushed 1dd2f7265f58: Pushed 33b2a4ee94ff: Pushed 2b2a8abdd0c4: Pushed 91ffc437f551: Pushed 4f04e7098d96: Pushed 248016390e0a: Pushed 0fa7eb58a57c: Pushed b5489882eed9: Pushed 2fb5caadbbb0: Pushed d06182ac791b: Pushed b39b0291530b: Pushed a04c77af4b60: Pushed 479c44e860ff: Pushed fc905c23b8a3: Pushed 161ec220381b: Pushed b7b591e3443f: Pushed ccf04fbd6e19: Pushed latest: digest: sha256:56d926b7ef64ed163ff026b7b5608ae97df4630235c1d0443a32a4fc8eb35a6c size: 4513 Verify that the image is in image registry via the command line: oc get images | grep apps/cos The application image you just pushed should be listed. Example output: sha256:56d926b7ef64ed163ff026b7b5608ae97df4630235c1d0443a32a4fc8eb35a6c image-registry.openshift-image-registry.svc:5000/apps/cos@sha256:56d926b7ef64ed163ff026b7b5608ae97df4630235c1d0443a32a4fc8eb35a6c Verify that the image stream is created via the command line: oc get imagestreams Example output: NAME IMAGE REPOSITORY TAGS UPDATED cos default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos latest 2 minutes ago You may also check the image stream via the console: Return to the OpenShift console through your Firefox browser window. Under the Builds tab in the left menu, click on Image Streams . Select project apps from the Project drop-down list. Click on cos from the list. Scroll down to the bottom to see the image that you pushed.","title":"Build image (Hands-on)"},{"location":"RuntimeModernization/#deploy-hands-on","text":"Customer Order Services application uses DB2 as its database. To deploy it to Liberty, a separate instance of the database is already pre-configured in the OpenShift cluster you are using. The database is exposed within the cluster using a Service , and the application references this database using the address of the Service . Deploy the application using the -k , or kustomize option of Openshift CLI now and we will explain how the deployment works in a later section. Preview what will be deployed: oc kustomize deploy/overlay-apps Example output of yaml: apiVersion: v1 data: DB_HOST: cos-db-liberty.db.svc kind: ConfigMap metadata: name: cos-config namespace: apps --- apiVersion: v1 data: DB_PASSWORD: ZGIyaW5zdDE= DB_USER: ZGIyaW5zdDE= kind: Secret metadata: name: db-creds namespace: apps type: Opaque --- apiVersion: v1 kind: Secret metadata: name: liberty-creds namespace: apps stringData: password: admin username: admin type: Opaque --- apiVersion: openliberty.io/v1beta1 kind: OpenLibertyApplication metadata: name: cos namespace: apps spec: applicationImage: image-registry.openshift-image-registry.svc:5000/apps/cos envFrom: - configMapRef: name: cos-config - secretRef: name: db-creds expose: true livenessProbe: httpGet: path: /health/live port: 9443 scheme: HTTPS monitoring: endpoints: - basicAuth: password: key: password name: liberty-creds username: key: username name: liberty-creds interval: 5s scheme: HTTPS tlsConfig: insecureSkipVerify: true labels: app-monitoring: \"true\" pullPolicy: Always readinessProbe: httpGet: path: /health/ready port: 9443 scheme: HTTPS route: insecureEdgeTerminationPolicy: Redirect termination: reencrypt service: annotations: service.beta.openshift.io/serving-cert-secret-name: cos-tls certificateSecretRef: cos-tls port: 9443 Run the following command to deploy the yaml files: oc apply -k deploy/overlay-apps Output of deploy command: configmap/cos-config created secret/db-creds created secret/liberty-creds created openlibertyapplication.openliberty.io/cos created Verify that the route is created for your application: oc get route cos Example output: NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD cos cos-apps.apps.demo.ibmdte.net cos 9443-tcp reencrypt/Redirect None Verify that your pod from the project apps is ready: First, confirm you're at the current project apps : oc project If it's not at project apps , then switch: oc project apps and then run the following command to view the pod status: oc get pod Example output of pod status: NAME READY STATUS RESTARTS AGE cos-596b4f849f-2fg4h 1/1 Running 0 18m - If the pod doesn't display the expected Running status (for example, after 5 minutes), then delete the pod to restart it. From the command line, run oc delete pod <pod name> (pod name is the string under NAME column from the output of oc get pod )","title":"Deploy (Hands-on)"},{"location":"RuntimeModernization/#access-the-application-hands-on","text":"Run the following command to get the URL of your application: echo http://$(oc get route cos --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-apps.apps.demo.ibmdte.net/CustomerOrderServicesWeb Return to your Firefox browser window and go to the URL outputted by the previous command. You'll be shown a login dialog. Login with user skywalker and password force . (The user is pre-created/registered in the basicRegistry configured in Liberty.) After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total . Close the browser.","title":"Access the application (Hands-on)"},{"location":"RuntimeModernization/#review-the-application-workload-flow-with-open-liberty-operator-hands-on","text":"Return to the OpenShift console through a Firefox window to view the resources on deployment. View the resources in the project openshift-operators : Select the openshift-operators project from the project drop down menu at the top of the page. View the operator's deployment details: Click on the Deployments tab under Workloads from the left menu and select open-liberty-operator Navigate to the YAML tab to view the content of yaml View the operator's pod details: Click on the Pods tab under Workloads from the left menu, and select the pod starting with open-liberty-operator Navigate to the Logs tab to view the open-liberty-operator container log Navigate to the Terminal tab to view the files in the container View the resources in the project apps : Select the apps project from the project drop down menu at the top of the page. View Open Liberty Application instance details: Click on the Installed Operators tab under Operators from the left menu and select Open Liberty Operator . Note: The operator is installed at cluster level and is visible to all existing projects, but Open Liberty Application instance is created under the project apps . Navigate to the Open Liberty Application tab and select cos to view the details of the Open Liberty Application instance Navigate to the YAML tab to view the content of yaml Navigate to the Resources tab to view the resources of Open Liberty Application instance View application deployment details: Click on the Deployments tab under Workloads from the left menu and select cos Navigate to the YAML tab to view the content of yaml. Note the deployment is created through the controller of the OpenLibertyApplication custom resource. View application pod details: Click on the Pods tab under Workloads from the left menu and select the pod starting with cos- Navigate to the Logs tab to view the liberty access log Note: by default, the Open Liberty Application instance is configured with liberty access log: View application service details: Click on the Services tab under Networking from the left menu and select cos Navigate to the YAML tab to view the content of yaml. > Note the service is created through the controller of OpenLibertyApplication custom resource. View application route details: Click on the Routes tab under Networking from the left menu and select cos Navigate to the YAML tab to view the content of yaml. >Note the route is created through the controller of OpenLibertyApplication custom resource. View application secret details: First, return to the YAML of the Open Liberty Application instance to view the configured secrets: Click on the Secrets tab under Workloads from the left menu and select the respective secrets and view the details View the resources in the project db : Select the db project from the project drop down menu. View db deployment details: Click on the Deployments tab under Workloads from the left menu and select cos-db-liberty Navigate to the YAML tab to view the content of yaml View db pod details: Click on the Pods tab under Workloads from the left menu and select the pod starting with cos-db-liberty Navigate to the Logs tab to view the database logs Navigate to the Terminal tab to view the files in the database container View db service details: Click on the Services tab under Networking from the left menu and select cos-db-liberty","title":"Review the application workload flow with Open Liberty Operator (Hands-on)"},{"location":"RuntimeModernization/#review-deployment","text":"Let's review the configuration files used for our deployment. Our configuration files are structured for the -k , or kustomize option of Openshift CLI. Kustomize is a separate tool that has been integrated into Openshift CLI that allows you to customize yaml without using variables. You can define a base directory and override directories to customize the base directory Make sure you are in directory /openshift-workshop-was/labs/Openshift/RuntimeModernization or change directory with cd openshift-workshop-was/labs/Openshift/RuntimeModernization List the deploy files: ls deploy And the output shows that we have one base directory and one override directory: base overlay-apps Take a look at what's in the base directory: ls deploy/base And the output: kustomization.yaml olapp-cos.yaml Each directory used for kustomization contains one kustomization.yaml cat deploy/base/kustomization.yaml Content of yaml: - This is a simple kustomization directory that just lists the yaml files to be deployed. resources: - olapp-cos.yaml The file olapp-cos.yaml contains the custom resource definition to deploy the application and will be covered in detail later. Take a look at the files in the overlay-apps directory. ls deploy/overlay-apps And the output: configmap.yaml kustomization.yaml secret-db-creds.yaml secret-liberty-creds.yaml Take a look at the kustomization.yaml in the overlay-apps directory: cat deploy/overlay-apps/kustomization.yaml And the output: namespace: apps resources: - configmap.yaml - secret-db-creds.yaml - secret-liberty-creds.yaml bases: - ./../base Note that: - The namespace is defined. This means that all resources originating from this directory will be applied to the apps namespace. - Resources from the base directory will also be included. - You may define additional overlay directories for different environments, each with a different namespace. For example, overlay-test, overlay-prod. - The configurations in this directory contain the overrides specific to this environment. - For a real environment, DO NOT store the secret yamls into source control. It is a security expsoure. See extra credit section on how to secure your secrets. To preview the resources that will be applied for a specific override directory, use the kustomize option of the Openshift command line. For example, oc kustomize deploy/overlay-apps The output is the same as displayed in Deploy (Hands-on) section.","title":"Review Deployment"},{"location":"RuntimeModernization/#secrets","text":"Specifying credentials and tokens in plain text is not secure. Secrets are used to store sensitive information. The stored data can be referenced by other resources. OpenShift handles secrets with special care. For example, they will not be logged or shown anywhere. There are two secrets - one for database credentials and one for Liberty metrics credentials, which is needed to access the /metrics endpoint. The file secret-db-creds.yaml contains the credentials for the database. It is injected into the container as an environment variable via the secretRef specification for the Open Liberty Operator. View the content of secret-db-creds.yaml file: cat deploy/overlay-apps/secret-db-creds.yaml Example output: kind: Secret apiVersion: v1 metadata: name: db-creds data: DB_PASSWORD: ZGIyaW5zdDE= DB_USER: ZGIyaW5zdDE= type: Opaque The file secret-liberty-creds.yaml contains the secret to access liberty server. View the content of secret-liberty-creds.yaml file: cat deploy/overlay-apps/secret-liberty-creds.yaml Example output: kind: Secret apiVersion: v1 metadata: name: liberty-creds stringData: username: admin password: admin type: Opaque Note that the first Secret provides the credentials in base64 encoded format using the data field. The second one provides them in plain text using the stringData field. OpenShift will automatically convert the credentials to base64 format and place the information under the data field. We can see this by viewing the YAML of the liberty-creds secret: Return to the OpenShift console through a Firefox browser window. Select the apps project from the project drop down menu. Click on the Secrets tab under Workloads from the left menu and search for the liberty-creds secret. Navigate to the YAML tab. Note that the data field contains the credentials in encoded form.","title":"Secrets"},{"location":"RuntimeModernization/#configmap","text":"Configmaps allows you to store name/value pairs that can be injected into your container when it starts. For our example, the values of the configmap.yaml are injected as environment variables in the configMapRef specification on the Open Liberty Operator in the next section. cat deploy/overlay-apps/configmap.yaml Example output: apiVersion: v1 kind: ConfigMap metadata: name: cos-config data: SEC_TLS_TRUSTDEFAULTCERTS: \"true\" DB_HOST : \"cos-db-liberty.db.svc\"","title":"Configmap"},{"location":"RuntimeModernization/#open-liberty-operator","text":"We could have created Deployment, Service, and Route resources to deploy the Liberty image. However, for this lab we will use the Open Liberty Operator instead. The Open Liberty Operator provides all functionalities of Runtime Component Operator used when deploying traditional WebSphere images in a previous lab. In addition, it also offers Open Liberty specific capabilities, such as day-2 operations (gather trace & dumps) and single sign-on (SSO). The file deploy/base/olapp-cos.yaml looks like: apiVersion: openliberty.io/v1beta1 kind: OpenLibertyApplication metadata: name: cos namespace: apps spec: applicationImage: 'image-registry.openshift-image-registry.svc:5000/apps/cos' pullPolicy: Always readinessProbe: httpGet: path: /health/ready port: 9443 scheme: HTTPS livenessProbe: httpGet: path: /health/live port: 9443 scheme: HTTPS service: annotations: service.beta.openshift.io/serving-cert-secret-name: cos-tls certificateSecretRef: cos-tls port: 9443 expose: true route: termination: reencrypt insecureEdgeTerminationPolicy: Redirect env: envFrom: - configMapRef: name: cos-config - secretRef: name: db-creds monitoring: endpoints: - basicAuth: password: key: password name: liberty-creds username: key: username name: liberty-creds interval: 5s scheme: HTTPS tlsConfig: insecureSkipVerify: true labels: app-monitoring: 'true' The OpenLibertyApplication is a custom resource supported by the Open Liberty Operator, which is designed to help you with Liberty deployment. It allows you to provide Liberty specific configurations (day-2 operations, single sign-on). The application image you pushed earlier to internal image registry is specified by the applicationImage parameter. MicroProfile Health endpoints /health/ready and /health/live are used for readiness and liveness probes. The configMapRef surfaces all entries of the ConfigMap cos-config as environment variables. The secretRef surfaces the entries in the Secret db-creds as environment variables. These are the database user and password. Enabled application monitoring so that Prometheus can scrape the information provided by MicroProfile Metric's /metrics endpoint in Liberty. The /metrics endpoint is protected, hence the credentials are provided using the Secret liberty-creds you created earlier.","title":"Open Liberty Operator"},{"location":"RuntimeModernization/#cleanup-hands-on-skip-this-step-if-youre-going-to-run-the-next-lab-application-management-on-the-same-assigned-cluster","text":"The controller for the Open Liberty Operator creates the necessary Deployment, Service, and Route objects for the Customer Order Services application. To list these resources, run the commands: Reminder: Run oc project to confirm you're at the apps project before running the following commands. oc get deployment oc get service oc get route Example output: # oc get deployment NAME READY UP-TO-DATE AVAILABLE AGE cos 1/1 1 1 2d18h # oc get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cos ClusterIP 172.21.202.9 <none> 9443/TCP 2d18h # oc get route NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD cos cos-apps.apps.demo.ibmdte.net cos 9443-tcp reencrypt/Redirect None To remove these resources, run the command (Ensure you are in directory openshift-workshop-was/labs/Openshift/RuntimeModernization ) Note: The pre-installed resources such as Open Liberty Operator and DB2, are not removed. oc delete -k deploy/overlay-apps Output: configmap \"cos-config\" deleted secret \"db-creds\" deleted secret \"liberty-creds\" deleted openlibertyapplication.openliberty.io \"cos\" deleted Double check the corresponding Deployment, Service, and Route objects are deleted: oc get deployment oc get service oc get route Output from each get command above: No resources found in apps namespace.","title":"Cleanup (Hands-on) (Skip this step if you're going to run the next lab Application Management on the same assigned cluster)"},{"location":"RuntimeModernization/#extra-credit","text":"Read about how to protect your secrets: https://www.openshift.com/blog/gitops-secret-management","title":"Extra Credit"},{"location":"RuntimeModernization/#summary","text":"Congratulations! You've completed Runtime Modernization lab! This application has been modified from the initial WebSphere ND v8.5.5 version to run on modern & cloud-native runtime Open Liberty and deployed by IBM Cloud Pak for Applications to RedHat OpenShift.","title":"Summary"},{"location":"RuntimeModernization/#next","text":"Follow the link to the next lab Application Management : - Application Management","title":"Next"},{"location":"RuntimeModernization/extras/application/","text":"Application Overview The Customer Order Services application is a simple store-front shopping application, built during the early days of the Web 2.0 movement. Users interact directly with a browser-based interface and manage their cart to submit orders. This application is built using the traditional 3-Tier Architecture model, with an HTTP server, an application server, and a supporting database. There are several components of the overall application architecture: - Starting with the database, the application leverages two SQL-based databases running on IBM DB2. The application exposes its data model through an Enterprise JavaBean layer, named CustomerOrderServices . This component leverages the Java Persistence API to expose the backend data model to calling services with minimal coding effort. This build of the application uses JavaEE6 features for EJBs and JPA. The next tier of the application, named CustomerOrderServicesWeb , exposes the necessary business APIs via REST-based web services. This component leverages the JAX-RS libraries for creating Java-based REST services with minimal coding effort. This build of the application is using JAX-RS 1.1 version of the respective capability. The application's user interface is exposed through the CustomerOrderServicesWeb component as well, in the form of a Dojo Toolkit-based JavaScript application. Delivering the user interface and business APIs in the same component is one major inhibitor our migration strategy will help to alleviate in the long-term. Finally, there is an additional integration testing component, named CustomerOrderServicesTest that is built to quickly validate an application's build and deployment to a given application server. This test component contains both JPA and JAX-RS -based tests.","title":"Application Overview"},{"location":"RuntimeModernization/extras/application/#application-overview","text":"The Customer Order Services application is a simple store-front shopping application, built during the early days of the Web 2.0 movement. Users interact directly with a browser-based interface and manage their cart to submit orders. This application is built using the traditional 3-Tier Architecture model, with an HTTP server, an application server, and a supporting database. There are several components of the overall application architecture: - Starting with the database, the application leverages two SQL-based databases running on IBM DB2. The application exposes its data model through an Enterprise JavaBean layer, named CustomerOrderServices . This component leverages the Java Persistence API to expose the backend data model to calling services with minimal coding effort. This build of the application uses JavaEE6 features for EJBs and JPA. The next tier of the application, named CustomerOrderServicesWeb , exposes the necessary business APIs via REST-based web services. This component leverages the JAX-RS libraries for creating Java-based REST services with minimal coding effort. This build of the application is using JAX-RS 1.1 version of the respective capability. The application's user interface is exposed through the CustomerOrderServicesWeb component as well, in the form of a Dojo Toolkit-based JavaScript application. Delivering the user interface and business APIs in the same component is one major inhibitor our migration strategy will help to alleviate in the long-term. Finally, there is an additional integration testing component, named CustomerOrderServicesTest that is built to quickly validate an application's build and deployment to a given application server. This test component contains both JPA and JAX-RS -based tests.","title":"Application Overview"},{"location":"RuntimeModernization/extras/liberty-analyze/","text":"Liberty - Analyze This section covers how to use IBM Cloud Transformation Advisor to analyze an existing traditional WebSphere application. For this scenario the Liberty runtime is chosen as the target runtime and the intention is to migrate this application with minimal code changes. Liberty is a fast, dynamic, and easy-to-use Java application server. Ideal or the cloud, Liberty is a combination of IBM technology and open source software, with fast startup times (<2 seconds), no server restarts to pick up changes, and a simple XML configuration. Summary This section has the following steps: Introduction to IBM Cloud Transformation Advisor Install IBM Cloud Transformation Advisor Download and run the Data Collector Upload and analyze the results Determine the migration/modernization path and next steps Introduction to IBM Cloud Transformation Advisor IBM Cloud Transformation Advisor helps you access, analyze and modernize middleware based apps into IBM Cloud(s). It categorizes Java EE apps and MQ queue managers as simple, medium and complex based on migration complexity and provides guidance for modernization. IBM Cloud Transformation Advisor can accelerate the process to move your on-premises apps to cloud, minimize migration errors and risks, and reduce time to market in five steps. You can use IBM Transformation Advisor for these activities: - Identify the Java EE programming models in an app - Determine the complexity of apps by reviewing a high-level inventory of the content and structure of each app - Highlight the Java EE programming model and WebSphere API differences between the profile types - Learn any Java EE specification implementation differences that might affect the app The tool also provides suggestions for the right-fit IBM WebSphere Application Server edition and offers advice, practices, and potential solutions to move apps to Liberty or to newer versions of WebSphere Application Server. Install IBM Cloud Transformation Advisor IBM Cloud Transformation Advisor is split in to two components (the analysis engine and the data collector ). Access Transformation Advisor within IBM Cloud Pak for Applications. You can also run it locally on a machine with Docker. See Installing IBM Cloud Transformation Advisor Beta Edition locally . Download the Data Collector Once IBM Cloud Transformation Advisor is installed, it is necessary to create a new Workspace and Collection and then download the Data Collector that will be used to examine the existing environment and applications. Open IBM Cloud Transformation Advisor in a browser and click the button to create a new Workspace Enter a Workspace name such as CloudPak_for_Applications and click Next Enter a Collection name such as WAS855_AppSrv01 and click Let's go When the No recommendations available page is displayed, click the Data Collector button When the Data Collector page is displayed, select the Source Operating System for your environment and click the Download button to download the Data Collector. This results in a file with a name similar to transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz being downloaded. Run the Data Collector Upload the Data Collector zip file that was downloaded from IBM Cloud Transformation Advisor in the previous step to the machine that the WebSphere ND Deployment Manager or the Standalone WebSphere Application Server is installed. The directory used arbitrary. Navigate to the directory you uploaded the transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz file to and issue the following commands to extract the Data Collector: mkdir datacollector cd datacollector mv transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz . tar -zxvf transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz cd transformationadvisor-* It is necessary to modify the scan performed by the Data Collector to include the org.pwte package as the Data Collector doesn't scan org.* packages by default. Open the conf/customCmd.properties file and modify it as shown below: evaluation=--evaluate --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp --includePackages=org.pwte migration_liberty=--analyze --sourceAppServer=was855 --targetAppServer=liberty --targetCloud=dockerIBMCloud --includePackages=org.pwte --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp migration_was=--analyze --sourceAppServer=was855 --targetAppServer=was90 --targetCloud=vmIBMCloud --includePackages=org.pwte --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp #inventory=--inventory --excludeFiles=\".*/directory/LargeXMLFileName.xml\" #featureList=--featureList --excludeFiles=\".*/directory/LargeXMLFileName.xml\" #java_opt=-Xmx2g The following command assumes that WebSphere Application Server v855 is installed to /opt/IBM/WebSphere/AppServer855 with a profile named AppSrv01 and that the administration user is wasadmin with a password of wasadmin . Modify and issue the following command as necessary to execute the Data Collector against the WebSphere environment: ./bin/transformationadvisor -w /opt/IBM/WebSphere/AppServer855 -p AppSrv01 wasadmin wasadmin When prompted, accept the license agreement . The Data Collection process will now start and will analyze all of the applications installed in the WebSphere Application Server environment and will also collect the related Java EE artifacts such as Data Sources and JMS definitions. When the analysis is complete, the Data Collector will attempt to upload the collection results to IBM Cloud Transformation Advisor. If this is successful, you can skip to the Analyze the Recommendations section. If not, you will receive an error at the end of Data Collection and will find a file named AppSrv01.zip in your current directory as shown below. ~/datacollector/transformationadvisor-1.9.6# ls -la *.zip -rw-r--r-- 1 root root 625493 Jun 12 12:58 AppSrv01.zip Download this Data Collector Results zip file ready for uploading to IBM Cloud Transformation Advisor in the next section Upload the Data Collector results In this section the results from the Data Collector will be uploaded to IBM Cloud Transformation Advisor. In the IBM Cloud Transformation Advisor web browser session, click the Recomendations link in the top left corner and then click the Upload data button as shown below When the Upload data dialog is displayed, use the Drop or Add File button to select the Data Collector Results zip file that was downloaded in the previous section. Click Upload After a few moments the upload of the data collector results will be completed. Analyze the Recommendations Once the Data Collector Results have been uploaded to IBM Cloud Transformation Advisor a set of recommendations will be created and shown on the Recommendations page. In this section the recommendations will be analyzed and interpreted. The default recommendations are based on a target runtime of Liberty on Private Cloud . The Data Collector analyzed all of the applications running on the traditional WebSphere profile a displays a row in the chart for each application. In the case of the CustomerOrderServicesApp.ear application, IBM Cloud Transformation Advisor has determined that the migration to WebSphere Traditional on Private Cloud is of Moderate complexity and that there are two Severe Issues that have been detected. Click on the CustomerOrderServicesApp.ear application name to see more information. Review the analysis results and scroll down to the Technology Issues section. Note that IBM Cloud Transformation Advisor has detected that there are issues with lookups for Enterprise JavaBeans and with accessing the Apache Wink APIs. In order to review the IBM Cloud Transformation Advisor results in more detail, scroll to the bottom of the analysis page and click on the Analysis Report link When the warning dialog is displayed, click OK The Detailed Migration Analysis Report will be displayed which show the results of the migration rules that were executed by the Data Collector and returned results. Scroll down to the Severe Rules section and click on the Show rule help link for each of the results. Review the recommendations. Behavior change on lookups for Enterprise JavaBeans In Liberty, EJB components are not bound to a server root Java Naming and Directory Interface (JNDI) namespace as they are in WebSphere Application Server traditional. The fix for this is to change the three classes that use ejblocal to use the correct URL for Liberty The user of system provided Apache Wink APIs requires configuration To use system-provided third-party APIs in Liberty applications, you must configure the applications to include the APIs. In WebSphere Application Server traditional, these APIs are available without configuration. This is a configuration only change and can be achieved by using a classloader definition in the Liberty server.xml file. Final Analysis The intention of this traditional WebSphere V855 --> Liberty (Private Cloud) scenario is to migrate the Customer Order Services application to the cloud-ready new runtime with minimal code changes. IBM Cloud Transformation Advisor was used to analyze the application for compatibility with traditional WebSphere V9 (Private Cloud) and determined that only small changes to three classes would be required. While this scenario will move the application to the cloud-ready Liberty runtime in a container, it will not modernize the application architecture and code in any way.","title":"Liberty - Analyze"},{"location":"RuntimeModernization/extras/liberty-analyze/#liberty-analyze","text":"This section covers how to use IBM Cloud Transformation Advisor to analyze an existing traditional WebSphere application. For this scenario the Liberty runtime is chosen as the target runtime and the intention is to migrate this application with minimal code changes. Liberty is a fast, dynamic, and easy-to-use Java application server. Ideal or the cloud, Liberty is a combination of IBM technology and open source software, with fast startup times (<2 seconds), no server restarts to pick up changes, and a simple XML configuration.","title":"Liberty - Analyze"},{"location":"RuntimeModernization/extras/liberty-analyze/#summary","text":"This section has the following steps: Introduction to IBM Cloud Transformation Advisor Install IBM Cloud Transformation Advisor Download and run the Data Collector Upload and analyze the results Determine the migration/modernization path and next steps","title":"Summary"},{"location":"RuntimeModernization/extras/liberty-analyze/#introduction-to-ibm-cloud-transformation-advisor","text":"IBM Cloud Transformation Advisor helps you access, analyze and modernize middleware based apps into IBM Cloud(s). It categorizes Java EE apps and MQ queue managers as simple, medium and complex based on migration complexity and provides guidance for modernization. IBM Cloud Transformation Advisor can accelerate the process to move your on-premises apps to cloud, minimize migration errors and risks, and reduce time to market in five steps. You can use IBM Transformation Advisor for these activities: - Identify the Java EE programming models in an app - Determine the complexity of apps by reviewing a high-level inventory of the content and structure of each app - Highlight the Java EE programming model and WebSphere API differences between the profile types - Learn any Java EE specification implementation differences that might affect the app The tool also provides suggestions for the right-fit IBM WebSphere Application Server edition and offers advice, practices, and potential solutions to move apps to Liberty or to newer versions of WebSphere Application Server.","title":"Introduction to IBM Cloud Transformation Advisor"},{"location":"RuntimeModernization/extras/liberty-analyze/#install-ibm-cloud-transformation-advisor","text":"IBM Cloud Transformation Advisor is split in to two components (the analysis engine and the data collector ). Access Transformation Advisor within IBM Cloud Pak for Applications. You can also run it locally on a machine with Docker. See Installing IBM Cloud Transformation Advisor Beta Edition locally .","title":"Install IBM Cloud Transformation Advisor"},{"location":"RuntimeModernization/extras/liberty-analyze/#download-the-data-collector","text":"Once IBM Cloud Transformation Advisor is installed, it is necessary to create a new Workspace and Collection and then download the Data Collector that will be used to examine the existing environment and applications. Open IBM Cloud Transformation Advisor in a browser and click the button to create a new Workspace Enter a Workspace name such as CloudPak_for_Applications and click Next Enter a Collection name such as WAS855_AppSrv01 and click Let's go When the No recommendations available page is displayed, click the Data Collector button When the Data Collector page is displayed, select the Source Operating System for your environment and click the Download button to download the Data Collector. This results in a file with a name similar to transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz being downloaded.","title":"Download the Data Collector"},{"location":"RuntimeModernization/extras/liberty-analyze/#run-the-data-collector","text":"Upload the Data Collector zip file that was downloaded from IBM Cloud Transformation Advisor in the previous step to the machine that the WebSphere ND Deployment Manager or the Standalone WebSphere Application Server is installed. The directory used arbitrary. Navigate to the directory you uploaded the transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz file to and issue the following commands to extract the Data Collector: mkdir datacollector cd datacollector mv transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz . tar -zxvf transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz cd transformationadvisor-* It is necessary to modify the scan performed by the Data Collector to include the org.pwte package as the Data Collector doesn't scan org.* packages by default. Open the conf/customCmd.properties file and modify it as shown below: evaluation=--evaluate --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp --includePackages=org.pwte migration_liberty=--analyze --sourceAppServer=was855 --targetAppServer=liberty --targetCloud=dockerIBMCloud --includePackages=org.pwte --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp migration_was=--analyze --sourceAppServer=was855 --targetAppServer=was90 --targetCloud=vmIBMCloud --includePackages=org.pwte --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp #inventory=--inventory --excludeFiles=\".*/directory/LargeXMLFileName.xml\" #featureList=--featureList --excludeFiles=\".*/directory/LargeXMLFileName.xml\" #java_opt=-Xmx2g The following command assumes that WebSphere Application Server v855 is installed to /opt/IBM/WebSphere/AppServer855 with a profile named AppSrv01 and that the administration user is wasadmin with a password of wasadmin . Modify and issue the following command as necessary to execute the Data Collector against the WebSphere environment: ./bin/transformationadvisor -w /opt/IBM/WebSphere/AppServer855 -p AppSrv01 wasadmin wasadmin When prompted, accept the license agreement . The Data Collection process will now start and will analyze all of the applications installed in the WebSphere Application Server environment and will also collect the related Java EE artifacts such as Data Sources and JMS definitions. When the analysis is complete, the Data Collector will attempt to upload the collection results to IBM Cloud Transformation Advisor. If this is successful, you can skip to the Analyze the Recommendations section. If not, you will receive an error at the end of Data Collection and will find a file named AppSrv01.zip in your current directory as shown below. ~/datacollector/transformationadvisor-1.9.6# ls -la *.zip -rw-r--r-- 1 root root 625493 Jun 12 12:58 AppSrv01.zip Download this Data Collector Results zip file ready for uploading to IBM Cloud Transformation Advisor in the next section","title":"Run the Data Collector"},{"location":"RuntimeModernization/extras/liberty-analyze/#upload-the-data-collector-results","text":"In this section the results from the Data Collector will be uploaded to IBM Cloud Transformation Advisor. In the IBM Cloud Transformation Advisor web browser session, click the Recomendations link in the top left corner and then click the Upload data button as shown below When the Upload data dialog is displayed, use the Drop or Add File button to select the Data Collector Results zip file that was downloaded in the previous section. Click Upload After a few moments the upload of the data collector results will be completed.","title":"Upload the Data Collector results"},{"location":"RuntimeModernization/extras/liberty-analyze/#analyze-the-recommendations","text":"Once the Data Collector Results have been uploaded to IBM Cloud Transformation Advisor a set of recommendations will be created and shown on the Recommendations page. In this section the recommendations will be analyzed and interpreted. The default recommendations are based on a target runtime of Liberty on Private Cloud . The Data Collector analyzed all of the applications running on the traditional WebSphere profile a displays a row in the chart for each application. In the case of the CustomerOrderServicesApp.ear application, IBM Cloud Transformation Advisor has determined that the migration to WebSphere Traditional on Private Cloud is of Moderate complexity and that there are two Severe Issues that have been detected. Click on the CustomerOrderServicesApp.ear application name to see more information. Review the analysis results and scroll down to the Technology Issues section. Note that IBM Cloud Transformation Advisor has detected that there are issues with lookups for Enterprise JavaBeans and with accessing the Apache Wink APIs. In order to review the IBM Cloud Transformation Advisor results in more detail, scroll to the bottom of the analysis page and click on the Analysis Report link When the warning dialog is displayed, click OK The Detailed Migration Analysis Report will be displayed which show the results of the migration rules that were executed by the Data Collector and returned results. Scroll down to the Severe Rules section and click on the Show rule help link for each of the results. Review the recommendations. Behavior change on lookups for Enterprise JavaBeans In Liberty, EJB components are not bound to a server root Java Naming and Directory Interface (JNDI) namespace as they are in WebSphere Application Server traditional. The fix for this is to change the three classes that use ejblocal to use the correct URL for Liberty The user of system provided Apache Wink APIs requires configuration To use system-provided third-party APIs in Liberty applications, you must configure the applications to include the APIs. In WebSphere Application Server traditional, these APIs are available without configuration. This is a configuration only change and can be achieved by using a classloader definition in the Liberty server.xml file.","title":"Analyze the Recommendations"},{"location":"RuntimeModernization/extras/liberty-analyze/#final-analysis","text":"The intention of this traditional WebSphere V855 --> Liberty (Private Cloud) scenario is to migrate the Customer Order Services application to the cloud-ready new runtime with minimal code changes. IBM Cloud Transformation Advisor was used to analyze the application for compatibility with traditional WebSphere V9 (Private Cloud) and determined that only small changes to three classes would be required. While this scenario will move the application to the cloud-ready Liberty runtime in a container, it will not modernize the application architecture and code in any way.","title":"Final Analysis"},{"location":"appmod-labs/","text":"Application Modernization Labs Operational Modernization Lab Runtime Modernization Lab Application Management Lab","title":"App Modernization Labs"},{"location":"appmod-labs/#application-modernization-labs","text":"Operational Modernization Lab Runtime Modernization Lab Application Management Lab","title":"Application Modernization Labs"},{"location":"appmod-labs/ApplicationManagement/","text":"Application Management Table of Contents Introduction Prerequisites Application Logging (Hands-on) Application Monitoring (Hands-on) Day-2 Operations (Hands-on) Summary Introduction In this lab, you'll learn about managing your running applications efficiently using various tools available to you as part of OpenShift and OperatorHub, including the Open Liberty Operator. Login to the VM If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password: Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window. Prerequisites Open a terminal window from the VM desktop. Login to OpenShift CLI with the oc login command from the web terminal. When prompted for the username and password, enter the following login credentials: Username: ibmadmin Password: engageibm If you have not yet cloned the GitHub repo with the lab artifacts, then run the following command on your terminal: git clone https://github.com/IBM/openshift-workshop-was.git Build and deploy the traditional WebSphere application (Hands-on) Change to the lab's directory: cd openshift-workshop-was/labs/Openshift/OperationalModernization Create and switch over to the project apps-was . Note: The first step oc new-project may fail if the project already exists. If so, proceed to the next command. oc new-project apps-was oc project apps-was Build and deploy the application by running the commands in the following sequence. Reminder: The . at the end of the first command is required. docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was . docker login -u openshift -p $(oc whoami -t) default-route-openshift-image-registry.apps.demo.ibmdte.net docker push default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was oc apply -f deploy Example output listing the resources that were created. deployment.apps/cos-was created route.route.openshift.io/cos-was created secret/authdata created service/cos-was created Wait for the pod to be available, check status via oc get pod command The output should be: NAME READY STATUS RESTARTS AGE cos-was-7d5ff6945-4hjzr 1/1 Running 0 88s Get the URL to the application: echo http://$(oc get route cos-was --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-was-apps-was.apps.demo.ibmdte.net/CustomerOrderServicesWeb Open a Firefox web browser window from within the VM. Go to the URL outputted by the command run in the previous step. You will be prompted to login in order to access the application. Enter the following credentials: Username: skywalker Password: force After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total . Close the browser. Build and deploy the Liberty application Please Read: Skip this section if you just finished the previous lab Runtime Modernization and did not clean up the deployment). Note: The following steps are to help you re-deploy the Liberty application if the deployment has been deleted from the previous lab Runtime Modernization. The deployment is required to generate data for the follow-on steps in monitoring. From the top level directory, change to the lab's directory RuntimeModernization folder: cd openshift-workshop-was/labs/Openshift/RuntimeModernization Create and switch over to the project apps . Also, enable monitoring for the project. Note: The first step oc new-project may fail if the project already exists. If so, proceed to the next command. oc new-project apps oc project apps oc label namespace apps app-monitoring=true Build and deploy the application by running the commands in the following sequence. Reminder: The . at the end of the first command: docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos . docker login -u openshift -p $(oc whoami -t) default-route-openshift-image-registry.apps.demo.ibmdte.net docker push default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos oc apply -k deploy/overlay-apps Example output: configmap/cos-config created secret/db-creds created secret/liberty-creds created openlibertyapplication.openliberty.io/cos created Verify the route for the application is created: oc get route cos Example output: NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD cos cos-apps.apps.demo.ibmdte.net cos 9443-tcp reencrypt/Redirect None Verify your pod is ready: oc get pod Example output: NAME READY STATUS RESTARTS AGE cos-54975b94c6-rh6kt 1/1 Running 0 3m11s Get the application URL: echo http://$(oc get route cos --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-apps.apps.demo.ibmdte.net/CustomerOrderServicesWeb Return to your Firefox browser window and go to the URL outputted by the command run in the previous step. You will be prompted to login in order to access the application. Enter the following credentials: Username: skywalker Password: force After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add multiple items to the shopping cart to trigger more logging. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total . Application Logging (Hands-on) Pod processes running in OpenShift frequently produce logs. To effectively manage this log data and ensure no loss of log data occurs when a pod terminates, a log aggregation tool should be deployed on the cluster. Log aggregation tools help users persist, search, and visualize the log data that is gathered from the pods across the cluster. Let's look at application logging with log aggregation using EFK (Elasticsearch, Fluentd, and Kibana) . Elasticsearch is a search and analytics engine. Fluentd receives, cleans and parses the log data. Kibana lets users visualize data stored in Elasticsearch with charts and graphs. Tip: If it has been a long time (more than 15 minutes) since the Liberty or WebSphere pods last started, you may want to delete each pod and let a new one start, to ensure that Liberty and WebSphere create some recent logs for Kibana to find. Launch Kibana (Hands-on) In your Firefox window, click on the openshift console bookmark in the top left, and log in with the htpasswd option. Log in to the OpenShift account using the following credentials: Username: ibmadmin Password: engageibm In the OpenShift console, from the left-panel, select Networking > Routes . From the Project drop-down list, select openshift-logging . In the kibana route, click on the URL listed under Location . This will open a new tab asking you to authorize access. Click on Allow selected permissions . In the Kibana console, you'll be prompted to create an index pattern. An index pattern tells Kibana what indices to look for in Elasticsearch. Type app so that the index pattern looks like this screenshot: You should see that your pattern matches at least one index. Then click Next step . Click the drop-down for Time Filter field name and choose @timestamp . Then click Create index pattern . You should see a number of fields populated, around 260. To check that the correct fields have been detected, type ibm in the Filter text box. You should see many fields beginning with the text ibm . If not, try clicking the refresh button (arrows in a circle) at the top right of the page. Import dashboards (Hands-on) A sample Kabana Dashboard for Liberty has been provided for this lab. Download this zip file containing dashboards to your VM and unzip it to a local directory. This will extract the grafana and kibana folders. Let's import dashboards for Liberty and WAS. From the left-panel, click on Management . Click on the Saved Objects tab and then click on Import . Click on Import from the right panel that is displayed. This will open your VM's files. Navigate to where you unzipped the dashboards file. Open the kibana folder and select the ibm-open-liberty-kibana5-problems-dashboard.json file. Click Open . Click the Import button at the bottom of the panel. When prompted to resolve pattern conflicts, select app* as the new index from the drop down menu. Click Confirm all changes . It'll take few seconds for the dashboard to import. Click Done when it finishes. Repeat the steps 3 - 7 to import the ibm-open-liberty-kibana5-traffic-dashboard.json and ibm-websphere-traditional-kibana5-dashboard.json files from the kibana folder of the unzipped file. Explore dashboards (Hands-on) In the Kibana console, from the left-panel, click on the Dashboard tab. You'll see 3 dashboards on the list. The first 2 are for Liberty. The last one is for WAS traditional. Read the description next to each dashboard. Liberty applications (Hands-on) Click on the Liberty-Problems-K5-20191122 dashboard. This dashboard visualizes message, trace and FFDC information from Liberty applications. By default, data from the last 15 minutes are rendered. Adjust the time-range (from the top-right corner), so that it includes data from when you tried the Open Liberty application. Once the data is rendered, you'll see some information about the namespace, pod, containers where events/problems occurred along with a count for each. Scroll down to Liberty Potential Problem Count section which lists the number of ERROR, FATAL, SystemErr and WARNING events. You'll likely see some WARNING events. Below that you'll see Liberty Top Message IDs . This helps to quickly identify frequently occurring events and their timeline. Click on the number above WARNING . The dashboard will change other panels to show just the events for warnings. Using this, you can determine whether the failures occurred on one or multiple pods/servers and whether they occurred around the same or different time. Scroll-down to the actual warning messages. In this case, some dojo files were not found. Even though they are warnings, it'll be good to fix them by updating the application (we won't do that as part of this workshop). Go back to the list of dashboards and click on Liberty-Traffic-K5-20191122 . This dashboard helps to identify failing or slow HTTP requests on Liberty applications. As before, adjust the time-range at the top right of the page if no data is rendered. You'll see some information about the namespace, pod, containers for the traffic along with a count for each. Scroll-down to the Liberty Error Response Code Count section which lists the number of failed requests with HTTP response codes in the 400s and 500s ranges. Scroll-down to Liberty Top URLs which lists the most frequently accessed URLs The /health and /metrics endpoints are running on the same server and are queried frequently for readiness/liveness probes and scraping metrics information. On the right-hand side, you'll see list of endpoints that had the slowest response times. Click on the number listed above 400s - Count . The dashboard will change other panels to show only the traffic with response codes in 400s. You can see the timeline and the actual messages below. These are related to warnings from last dashboard about dojo files not being found (response code 404). Traditional WebSphere applications (Hands-on) Return to the list of dashboards and click on WAS-traditional-Problems-K5-20190609 . Similar to the first dashboard for Liberty, this dashboard visualizes message and trace information for WebSphere Application Server traditional. Again, adjust the time-range as necessary if no data is rendered. Same as before, explore the panels and filter through the events to see messages corresponding to just those events. Application Monitoring (Hands-on) Building observability into applications externalizes the internal status of a system, enabling operations teams to monitor systems more effectively. It is important that applications are written to produce metrics. When the Customer Order Services application was modernized, we used MicroProfile Metrics and it provided a /metrics endpoint from where all metrics emitted by the JVM, Open Liberty server and deployed applications could be accessed. Operations teams can gather the metrics and store them in a database with tools like Prometheus. The metrics data can then be visualized and analyzed in dashboards, such as Grafana . Grafana dashboard (Hands-on) Custom resource GrafanaDashboard defines a set of dashboards for monitoring the Customer Order Services application and Open Liberty. In your terminal, run the following command to create the dashboard resource: a. Change directory to /openshift-workshop-was/labs/Openshift/ApplicationManagement if not done so already. cd /openshift-workshop-was/labs/Openshift/ApplicationManagement b. Run the following command to define the dashboards for monitoring oc apply -f dashboards/grafana/grafana-dashboard-cos.yaml Example output: grafanadashboard.integreatly.org/liberty-metrics-dashboard created The following steps to access the created dashboard are illustrated in the screen recording at the end of this section: a. In the OpenShift console, from the left-panel, select Networking > Routes . b. From the Project drop-down list, select app-monitoring . c. In the grafana-route route, click on the URL listed under the Location column. d. Click on Log in with OpenShift . Then, click on Allow selected permissions . e. In the Grafana home page, from the left-panel, hover over the dashboard icon and click on Manage . f. You should see Liberty-Metrics-Dashboard listed. Click on it. g. Explore the dashboards. The first 2 are for the Customer Order Services application. The rest are for Liberty. h. Click on Customer Order Services - Shopping Cart . By default, it'll show data from the last 15 minutes. Adjust the time-range from the top-right as necessary. i. You should see the frequency of requests, number of requests, pod information, and min/max request times. j. Scroll-down to expand the CPU section. You'll see information about process CPU time and CPU system load for pods. k. Scroll-down to expand the Servlets section. You'll see request count and response times for application servlet as well as health and metrics endpoints. l. Explore the other sections. Day-2 Operations (Hands-on) It is often nessarry to gather server traces and/or dumps for analyzing some problems with an application. Open Liberty Operator makes it easy to gather these on a server running inside a container. A storage must be configured so the generated artifacts can persist, even after the Pod is deleted. This storage can be shared by all instances of the Open Liberty applications. RedHat OpenShift on IBM Cloud utilizes the storage capabilities provided by IBM Cloud. In the lab environment, you will attach storage using the storage class backed by NFS storage that we configured for the lab environment. Let's create a request for storage. Request storage (Hands-on) In the OpenShift console, from the left-panel, select Storage > Persistent Volume Claims . From the Project drop-down list, select apps . Click on the Create Persistent Volume Claim button. Ensure that Storage Class is managed-nfs . If not, select it from the list. Enter liberty for the Persistent Volume Claim Name field. Request 1 GiB by entering 1 in the text box for Size . Click on Create . The created Persistent Volume Claim will be displayed. Wait for the Status field to change from Pending to Bound . It may take 1-2 minutes. Once bound, you should see the volume displayed under the Persistent Volume field. Enable serviceability (Hands-on) Now, let's enable the serviceability option for the Customer Order Services application. It's recommended that you do this step during initial deployment of the application - not when you encounter an issue and need to gather server traces or dumps. OpenShift cannot attach volumes to running Pods. It'll have to create a new Pod, attach the volume, and then take down the old Pod. If the problem is intermittent or hard to reproduce, you may not be able to reproduce it on the new instance of server running in the new Pod. The volume can be shared by all Liberty applications that are in the same namespace and won't be used unless you perform day-2 operation on a particular application. This makes it easy to enable serviceability with initial deployment. Specify the name of the storage request (Persistent Volume Claim) you made earlier to the spec.serviceability.volumeClaimName parameter provided by the OpenLibertyApplication custom resource. The Open Liberty Operator will attach the volume bound to the claim to each instance of the server. In your terminal, run the following command: oc patch olapp cos -n apps --patch '{\"spec\":{\"serviceability\":{\"volumeClaimName\":\"liberty\"}}}' --type=merge This patches the definition of olapp (shortname for OpenLibertyApplication ) instance cos in namespace apps (indicated by -n option). The --patch option specifies the content to patch with. In this case, we set the value of spec.serviceability.volumeClaimName field to liberty , which is the name of the Persistent Volume Claim you created earlier. The --type=merge option specifies to merge the previous content with the newly specified field and its value. Run the following command to get the status of cos application. Verify that the changes were reconciled and there are no errors: oc get olapp cos -n apps -o wide Example output: NAME IMAGE EXPOSED RECONCILED REASON MESSAGE DEPENDENCIESSATISFIED AGE cos image-registry.openshift-image-registry.svc:5000/apps/cos true True True 14h The value under RECONCILED should be True . NOTE: If the REONCILED value is False , then an error occurred. The REASON and MESSAGE columns will display the cause of the failure. A common mistake is creating the Persistent Volume Claim in another namespace. Ensure that it is created in the apps namespace. In the OpenShift console, from the left-panel, click on Workloads > Pods . Wait until there is only 1 pod on the list and its Ready column says 1/1. Click on this pod. The pod's name is needed for requesting server dump and trace in the next sections. Scroll down and copy the value under the Name field. Request server dump (Hands-on) You can request a snapshot of the server status including different types of server dumps, from an instance of Open Liberty server running inside a Pod, using Open Liberty Operator and OpenLibertyDump custom resource (CR). The following steps to request a server dump are illustrated in the screen recording below: From the left-panel of the OpenShift console, click on Operators > Installed Operators . From the Open Liberty Operator row, click on Open Liberty Dump (displayed under the Provided APIs column). Click on the Create OpenLibertyDump button. Replace Specify_Pod_Name_Here in the Pod Name text field with the pod name you copied earlier. The Include field specifies the type of server dumps to request. Let's use the default heap and thread values. Click on the Create button. Click on example-dump from the list. Scroll-down to the Conditions section and you should see Started status has value True . Wait for the operator to complete the dump operation. You should see status Completed with value True . Request server traces (Hands-on) You can also request server traces from an instance of Open Liberty server running inside a Pod using the OpenLibertyTrace custom resource (CR). The following steps to request a server trace are illustrated in the screen recording below: From the left-panel of the OpenShift console, click on Operators > Installed Operators . From the Open Liberty Operator row, click on Open Liberty Trace . Click on the Create OpenLibertyTrace button. Replace Specify_Pod_Name_Here under the Pod Name text field with the pod name you copied earlier. The Trace Specification field specifies the trace string to be used to selectively enable trace on Liberty server. Let's use the default value. Click on the Create button. Click on example-trace from the list. Scroll-down to the Conditions section and you should see Enabled status has value True . Additional notes: Once the trace has started, it can be stopped by setting the disable parameter to true. Deleting the CR will also stop the tracing. Changing the podName will first stop the tracing on the old Pod before enabling traces on the new Pod. Maximum trace file size (in MB) and the maximum number of files before rolling over can be specified using maxFileSize and maxFiles parameters. Accessing the generated files (Hands-on) The generated trace and dump files should now be in the persistent volume. You used storage from IBM Cloud, and we have to go through a number of steps using a different tool to access those files. Since the volume is attached to the Pod, we can instead use the Pod's terminal to easily verify that the trace and dump files are present. The following steps to access the files are illustrated in the screen recording below: Remote shell to your pod via one of two ways: From your terminal: oc rsh <pod-name> From OpenShift console: click on Workloads > Pods . Click on the pod and then click on Terminal tab. Enter the following command to list the files: ls -R serviceability/apps Example output: serviceability/apps: cos-dc4cf7fdd-mvbs7 serviceability/apps/cos-dc4cf7fdd-mvbs7: 2021-07-13_16:05:25.zip messages.log trace_21.07.13_16.52.34.0.log trace.log The shared volume is mounted at the serviceability folder. The sub-folder apps is the namespace of the Pod. You should see a zip file for dumps and trace log files. These are produced by the day-2 operations you performed. Summary Congratulations! You've completed Application Management lab!","title":"Application Management"},{"location":"appmod-labs/ApplicationManagement/#application-management","text":"","title":"Application Management"},{"location":"appmod-labs/ApplicationManagement/#table-of-contents","text":"Introduction Prerequisites Application Logging (Hands-on) Application Monitoring (Hands-on) Day-2 Operations (Hands-on) Summary","title":"Table of Contents"},{"location":"appmod-labs/ApplicationManagement/#introduction","text":"In this lab, you'll learn about managing your running applications efficiently using various tools available to you as part of OpenShift and OperatorHub, including the Open Liberty Operator.","title":"Introduction"},{"location":"appmod-labs/ApplicationManagement/#login-to-the-vm","text":"If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password: Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window.","title":"Login to the VM"},{"location":"appmod-labs/ApplicationManagement/#prerequisites","text":"Open a terminal window from the VM desktop. Login to OpenShift CLI with the oc login command from the web terminal. When prompted for the username and password, enter the following login credentials: Username: ibmadmin Password: engageibm If you have not yet cloned the GitHub repo with the lab artifacts, then run the following command on your terminal: git clone https://github.com/IBM/openshift-workshop-was.git","title":"Prerequisites"},{"location":"appmod-labs/ApplicationManagement/#build-and-deploy-the-traditional-websphere-application-hands-on","text":"Change to the lab's directory: cd openshift-workshop-was/labs/Openshift/OperationalModernization Create and switch over to the project apps-was . Note: The first step oc new-project may fail if the project already exists. If so, proceed to the next command. oc new-project apps-was oc project apps-was Build and deploy the application by running the commands in the following sequence. Reminder: The . at the end of the first command is required. docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was . docker login -u openshift -p $(oc whoami -t) default-route-openshift-image-registry.apps.demo.ibmdte.net docker push default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was oc apply -f deploy Example output listing the resources that were created. deployment.apps/cos-was created route.route.openshift.io/cos-was created secret/authdata created service/cos-was created Wait for the pod to be available, check status via oc get pod command The output should be: NAME READY STATUS RESTARTS AGE cos-was-7d5ff6945-4hjzr 1/1 Running 0 88s Get the URL to the application: echo http://$(oc get route cos-was --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-was-apps-was.apps.demo.ibmdte.net/CustomerOrderServicesWeb Open a Firefox web browser window from within the VM. Go to the URL outputted by the command run in the previous step. You will be prompted to login in order to access the application. Enter the following credentials: Username: skywalker Password: force After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total . Close the browser.","title":"Build and deploy the traditional WebSphere application (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/#build-and-deploy-the-liberty-application","text":"Please Read: Skip this section if you just finished the previous lab Runtime Modernization and did not clean up the deployment). Note: The following steps are to help you re-deploy the Liberty application if the deployment has been deleted from the previous lab Runtime Modernization. The deployment is required to generate data for the follow-on steps in monitoring. From the top level directory, change to the lab's directory RuntimeModernization folder: cd openshift-workshop-was/labs/Openshift/RuntimeModernization Create and switch over to the project apps . Also, enable monitoring for the project. Note: The first step oc new-project may fail if the project already exists. If so, proceed to the next command. oc new-project apps oc project apps oc label namespace apps app-monitoring=true Build and deploy the application by running the commands in the following sequence. Reminder: The . at the end of the first command: docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos . docker login -u openshift -p $(oc whoami -t) default-route-openshift-image-registry.apps.demo.ibmdte.net docker push default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos oc apply -k deploy/overlay-apps Example output: configmap/cos-config created secret/db-creds created secret/liberty-creds created openlibertyapplication.openliberty.io/cos created Verify the route for the application is created: oc get route cos Example output: NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD cos cos-apps.apps.demo.ibmdte.net cos 9443-tcp reencrypt/Redirect None Verify your pod is ready: oc get pod Example output: NAME READY STATUS RESTARTS AGE cos-54975b94c6-rh6kt 1/1 Running 0 3m11s Get the application URL: echo http://$(oc get route cos --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-apps.apps.demo.ibmdte.net/CustomerOrderServicesWeb Return to your Firefox browser window and go to the URL outputted by the command run in the previous step. You will be prompted to login in order to access the application. Enter the following credentials: Username: skywalker Password: force After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add multiple items to the shopping cart to trigger more logging. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total .","title":"Build and deploy the Liberty application"},{"location":"appmod-labs/ApplicationManagement/#application-logging-hands-on","text":"Pod processes running in OpenShift frequently produce logs. To effectively manage this log data and ensure no loss of log data occurs when a pod terminates, a log aggregation tool should be deployed on the cluster. Log aggregation tools help users persist, search, and visualize the log data that is gathered from the pods across the cluster. Let's look at application logging with log aggregation using EFK (Elasticsearch, Fluentd, and Kibana) . Elasticsearch is a search and analytics engine. Fluentd receives, cleans and parses the log data. Kibana lets users visualize data stored in Elasticsearch with charts and graphs. Tip: If it has been a long time (more than 15 minutes) since the Liberty or WebSphere pods last started, you may want to delete each pod and let a new one start, to ensure that Liberty and WebSphere create some recent logs for Kibana to find.","title":"Application Logging (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/#launch-kibana-hands-on","text":"In your Firefox window, click on the openshift console bookmark in the top left, and log in with the htpasswd option. Log in to the OpenShift account using the following credentials: Username: ibmadmin Password: engageibm In the OpenShift console, from the left-panel, select Networking > Routes . From the Project drop-down list, select openshift-logging . In the kibana route, click on the URL listed under Location . This will open a new tab asking you to authorize access. Click on Allow selected permissions . In the Kibana console, you'll be prompted to create an index pattern. An index pattern tells Kibana what indices to look for in Elasticsearch. Type app so that the index pattern looks like this screenshot: You should see that your pattern matches at least one index. Then click Next step . Click the drop-down for Time Filter field name and choose @timestamp . Then click Create index pattern . You should see a number of fields populated, around 260. To check that the correct fields have been detected, type ibm in the Filter text box. You should see many fields beginning with the text ibm . If not, try clicking the refresh button (arrows in a circle) at the top right of the page.","title":"Launch Kibana (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/#import-dashboards-hands-on","text":"A sample Kabana Dashboard for Liberty has been provided for this lab. Download this zip file containing dashboards to your VM and unzip it to a local directory. This will extract the grafana and kibana folders. Let's import dashboards for Liberty and WAS. From the left-panel, click on Management . Click on the Saved Objects tab and then click on Import . Click on Import from the right panel that is displayed. This will open your VM's files. Navigate to where you unzipped the dashboards file. Open the kibana folder and select the ibm-open-liberty-kibana5-problems-dashboard.json file. Click Open . Click the Import button at the bottom of the panel. When prompted to resolve pattern conflicts, select app* as the new index from the drop down menu. Click Confirm all changes . It'll take few seconds for the dashboard to import. Click Done when it finishes. Repeat the steps 3 - 7 to import the ibm-open-liberty-kibana5-traffic-dashboard.json and ibm-websphere-traditional-kibana5-dashboard.json files from the kibana folder of the unzipped file.","title":"Import dashboards (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/#explore-dashboards-hands-on","text":"In the Kibana console, from the left-panel, click on the Dashboard tab. You'll see 3 dashboards on the list. The first 2 are for Liberty. The last one is for WAS traditional. Read the description next to each dashboard.","title":"Explore dashboards (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/#liberty-applications-hands-on","text":"Click on the Liberty-Problems-K5-20191122 dashboard. This dashboard visualizes message, trace and FFDC information from Liberty applications. By default, data from the last 15 minutes are rendered. Adjust the time-range (from the top-right corner), so that it includes data from when you tried the Open Liberty application. Once the data is rendered, you'll see some information about the namespace, pod, containers where events/problems occurred along with a count for each. Scroll down to Liberty Potential Problem Count section which lists the number of ERROR, FATAL, SystemErr and WARNING events. You'll likely see some WARNING events. Below that you'll see Liberty Top Message IDs . This helps to quickly identify frequently occurring events and their timeline. Click on the number above WARNING . The dashboard will change other panels to show just the events for warnings. Using this, you can determine whether the failures occurred on one or multiple pods/servers and whether they occurred around the same or different time. Scroll-down to the actual warning messages. In this case, some dojo files were not found. Even though they are warnings, it'll be good to fix them by updating the application (we won't do that as part of this workshop). Go back to the list of dashboards and click on Liberty-Traffic-K5-20191122 . This dashboard helps to identify failing or slow HTTP requests on Liberty applications. As before, adjust the time-range at the top right of the page if no data is rendered. You'll see some information about the namespace, pod, containers for the traffic along with a count for each. Scroll-down to the Liberty Error Response Code Count section which lists the number of failed requests with HTTP response codes in the 400s and 500s ranges. Scroll-down to Liberty Top URLs which lists the most frequently accessed URLs The /health and /metrics endpoints are running on the same server and are queried frequently for readiness/liveness probes and scraping metrics information. On the right-hand side, you'll see list of endpoints that had the slowest response times. Click on the number listed above 400s - Count . The dashboard will change other panels to show only the traffic with response codes in 400s. You can see the timeline and the actual messages below. These are related to warnings from last dashboard about dojo files not being found (response code 404).","title":"Liberty applications (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/#traditional-websphere-applications-hands-on","text":"Return to the list of dashboards and click on WAS-traditional-Problems-K5-20190609 . Similar to the first dashboard for Liberty, this dashboard visualizes message and trace information for WebSphere Application Server traditional. Again, adjust the time-range as necessary if no data is rendered. Same as before, explore the panels and filter through the events to see messages corresponding to just those events.","title":"Traditional WebSphere applications (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/#application-monitoring-hands-on","text":"Building observability into applications externalizes the internal status of a system, enabling operations teams to monitor systems more effectively. It is important that applications are written to produce metrics. When the Customer Order Services application was modernized, we used MicroProfile Metrics and it provided a /metrics endpoint from where all metrics emitted by the JVM, Open Liberty server and deployed applications could be accessed. Operations teams can gather the metrics and store them in a database with tools like Prometheus. The metrics data can then be visualized and analyzed in dashboards, such as Grafana .","title":"Application Monitoring (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/#grafana-dashboard-hands-on","text":"Custom resource GrafanaDashboard defines a set of dashboards for monitoring the Customer Order Services application and Open Liberty. In your terminal, run the following command to create the dashboard resource: a. Change directory to /openshift-workshop-was/labs/Openshift/ApplicationManagement if not done so already. cd /openshift-workshop-was/labs/Openshift/ApplicationManagement b. Run the following command to define the dashboards for monitoring oc apply -f dashboards/grafana/grafana-dashboard-cos.yaml Example output: grafanadashboard.integreatly.org/liberty-metrics-dashboard created The following steps to access the created dashboard are illustrated in the screen recording at the end of this section: a. In the OpenShift console, from the left-panel, select Networking > Routes . b. From the Project drop-down list, select app-monitoring . c. In the grafana-route route, click on the URL listed under the Location column. d. Click on Log in with OpenShift . Then, click on Allow selected permissions . e. In the Grafana home page, from the left-panel, hover over the dashboard icon and click on Manage . f. You should see Liberty-Metrics-Dashboard listed. Click on it. g. Explore the dashboards. The first 2 are for the Customer Order Services application. The rest are for Liberty. h. Click on Customer Order Services - Shopping Cart . By default, it'll show data from the last 15 minutes. Adjust the time-range from the top-right as necessary. i. You should see the frequency of requests, number of requests, pod information, and min/max request times. j. Scroll-down to expand the CPU section. You'll see information about process CPU time and CPU system load for pods. k. Scroll-down to expand the Servlets section. You'll see request count and response times for application servlet as well as health and metrics endpoints. l. Explore the other sections.","title":"Grafana dashboard (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/#day-2-operations-hands-on","text":"It is often nessarry to gather server traces and/or dumps for analyzing some problems with an application. Open Liberty Operator makes it easy to gather these on a server running inside a container. A storage must be configured so the generated artifacts can persist, even after the Pod is deleted. This storage can be shared by all instances of the Open Liberty applications. RedHat OpenShift on IBM Cloud utilizes the storage capabilities provided by IBM Cloud. In the lab environment, you will attach storage using the storage class backed by NFS storage that we configured for the lab environment. Let's create a request for storage.","title":"Day-2 Operations (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/#request-storage-hands-on","text":"In the OpenShift console, from the left-panel, select Storage > Persistent Volume Claims . From the Project drop-down list, select apps . Click on the Create Persistent Volume Claim button. Ensure that Storage Class is managed-nfs . If not, select it from the list. Enter liberty for the Persistent Volume Claim Name field. Request 1 GiB by entering 1 in the text box for Size . Click on Create . The created Persistent Volume Claim will be displayed. Wait for the Status field to change from Pending to Bound . It may take 1-2 minutes. Once bound, you should see the volume displayed under the Persistent Volume field.","title":"Request storage (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/#enable-serviceability-hands-on","text":"Now, let's enable the serviceability option for the Customer Order Services application. It's recommended that you do this step during initial deployment of the application - not when you encounter an issue and need to gather server traces or dumps. OpenShift cannot attach volumes to running Pods. It'll have to create a new Pod, attach the volume, and then take down the old Pod. If the problem is intermittent or hard to reproduce, you may not be able to reproduce it on the new instance of server running in the new Pod. The volume can be shared by all Liberty applications that are in the same namespace and won't be used unless you perform day-2 operation on a particular application. This makes it easy to enable serviceability with initial deployment. Specify the name of the storage request (Persistent Volume Claim) you made earlier to the spec.serviceability.volumeClaimName parameter provided by the OpenLibertyApplication custom resource. The Open Liberty Operator will attach the volume bound to the claim to each instance of the server. In your terminal, run the following command: oc patch olapp cos -n apps --patch '{\"spec\":{\"serviceability\":{\"volumeClaimName\":\"liberty\"}}}' --type=merge This patches the definition of olapp (shortname for OpenLibertyApplication ) instance cos in namespace apps (indicated by -n option). The --patch option specifies the content to patch with. In this case, we set the value of spec.serviceability.volumeClaimName field to liberty , which is the name of the Persistent Volume Claim you created earlier. The --type=merge option specifies to merge the previous content with the newly specified field and its value. Run the following command to get the status of cos application. Verify that the changes were reconciled and there are no errors: oc get olapp cos -n apps -o wide Example output: NAME IMAGE EXPOSED RECONCILED REASON MESSAGE DEPENDENCIESSATISFIED AGE cos image-registry.openshift-image-registry.svc:5000/apps/cos true True True 14h The value under RECONCILED should be True . NOTE: If the REONCILED value is False , then an error occurred. The REASON and MESSAGE columns will display the cause of the failure. A common mistake is creating the Persistent Volume Claim in another namespace. Ensure that it is created in the apps namespace. In the OpenShift console, from the left-panel, click on Workloads > Pods . Wait until there is only 1 pod on the list and its Ready column says 1/1. Click on this pod. The pod's name is needed for requesting server dump and trace in the next sections. Scroll down and copy the value under the Name field.","title":"Enable serviceability (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/#request-server-dump-hands-on","text":"You can request a snapshot of the server status including different types of server dumps, from an instance of Open Liberty server running inside a Pod, using Open Liberty Operator and OpenLibertyDump custom resource (CR). The following steps to request a server dump are illustrated in the screen recording below: From the left-panel of the OpenShift console, click on Operators > Installed Operators . From the Open Liberty Operator row, click on Open Liberty Dump (displayed under the Provided APIs column). Click on the Create OpenLibertyDump button. Replace Specify_Pod_Name_Here in the Pod Name text field with the pod name you copied earlier. The Include field specifies the type of server dumps to request. Let's use the default heap and thread values. Click on the Create button. Click on example-dump from the list. Scroll-down to the Conditions section and you should see Started status has value True . Wait for the operator to complete the dump operation. You should see status Completed with value True .","title":"Request server dump (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/#request-server-traces-hands-on","text":"You can also request server traces from an instance of Open Liberty server running inside a Pod using the OpenLibertyTrace custom resource (CR). The following steps to request a server trace are illustrated in the screen recording below: From the left-panel of the OpenShift console, click on Operators > Installed Operators . From the Open Liberty Operator row, click on Open Liberty Trace . Click on the Create OpenLibertyTrace button. Replace Specify_Pod_Name_Here under the Pod Name text field with the pod name you copied earlier. The Trace Specification field specifies the trace string to be used to selectively enable trace on Liberty server. Let's use the default value. Click on the Create button. Click on example-trace from the list. Scroll-down to the Conditions section and you should see Enabled status has value True . Additional notes: Once the trace has started, it can be stopped by setting the disable parameter to true. Deleting the CR will also stop the tracing. Changing the podName will first stop the tracing on the old Pod before enabling traces on the new Pod. Maximum trace file size (in MB) and the maximum number of files before rolling over can be specified using maxFileSize and maxFiles parameters.","title":"Request server traces (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/#accessing-the-generated-files-hands-on","text":"The generated trace and dump files should now be in the persistent volume. You used storage from IBM Cloud, and we have to go through a number of steps using a different tool to access those files. Since the volume is attached to the Pod, we can instead use the Pod's terminal to easily verify that the trace and dump files are present. The following steps to access the files are illustrated in the screen recording below: Remote shell to your pod via one of two ways: From your terminal: oc rsh <pod-name> From OpenShift console: click on Workloads > Pods . Click on the pod and then click on Terminal tab. Enter the following command to list the files: ls -R serviceability/apps Example output: serviceability/apps: cos-dc4cf7fdd-mvbs7 serviceability/apps/cos-dc4cf7fdd-mvbs7: 2021-07-13_16:05:25.zip messages.log trace_21.07.13_16.52.34.0.log trace.log The shared volume is mounted at the serviceability folder. The sub-folder apps is the namespace of the Pod. You should see a zip file for dumps and trace log files. These are produced by the day-2 operations you performed.","title":"Accessing the generated files (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/#summary","text":"Congratulations! You've completed Application Management lab!","title":"Summary"},{"location":"appmod-labs/ApplicationManagement/README - Copy/","text":"Application Management Table of Contents Introduction Prerequisites Application Logging (Hands-on) Application Monitoring (Hands-on) Day-2 Operations (Hands-on) Summary Introduction In this lab, you'll learn about managing your running applications efficiently using various tools available to you as part of OpenShift and OperatorHub, including the Open Liberty Operator. Login to the VM If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password: \\ Password: engageibm Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window. Prerequisites Open a terminal window from the VM desktop. Login to OpenShift CLI with the oc login command from the web terminal. When prompted for the username and password, enter the following login credentials: Username: ibmadmin Password: engageibm If you have not yet cloned the GitHub repo with the lab artifacts, then run the following command on your terminal: git clone https://github.com/IBM/openshift-workshop-was.git Build and deploy the traditional WebSphere application (Hands-on) Change to the lab's directory: cd openshift-workshop-was/labs/Openshift/OperationalModernization Create and switch over to the project apps-was . > Note: The first step oc new-project may fail if the project already exists. If so, proceed to the next command. oc new-project apps-was oc project apps-was Build and deploy the application by running the commands in the following sequence. Reminder: the . at the end of the first command. docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was . docker login -u openshift -p $(oc whoami -t) default-route-openshift-image-registry.apps.demo.ibmdte.net docker push default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was oc apply -f deploy Example output: deployment.apps/cos-was created route.route.openshift.io/cos-was created secret/authdata created service/cos-was created Wait for the pod to be available, check status via oc get pod The output should be: NAME READY STATUS RESTARTS AGE cos-was-7d5ff6945-4hjzr 1/1 Running 0 88s Get the URL to the application: echo http://$(oc get route cos-was --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-was-apps-was.apps.demo.ibmdte.net/CustomerOrderServicesWeb Return to your Firefox browser window, and go to the URL outputted by the command run in the previous step. You will be prompted to login in order to access the application. Enter the following credentials: Username: skywalker Password: force After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total . Close the browser. Build and deploy the Liberty application (Skip this step if you just finished the previous lab Runtime Modernization and did not clean up the deployment) Note: The following steps are to help you re-deploy the Liberty application if the deployment has been deleted from the previous lab Runtime Modernization. The deployment is required to generate data for the follow-on steps in monitoring. From the top level directory, change to the lab's directory RuntimeModernization folder: cd openshift-workshop-was/labs/Openshift/RuntimeModernization Create and switch over to the project apps . Also, enable monitoring for the project. > Note: The first step oc new-project may fail if the project already exists. If so, proceed to the next command. oc new-project apps oc project apps oc label namespace apps app-monitoring=true Build and deploy the application by running the commands in the following sequence. Reminder: the . at the end of the first command: docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos . docker login -u openshift -p $(oc whoami -t) default-route-openshift-image-registry.apps.demo.ibmdte.net docker push default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos oc apply -k deploy/overlay-apps Example output: configmap/cos-config created secret/db-creds created secret/liberty-creds created openlibertyapplication.openliberty.io/cos created Verify the route for the application is created: oc get route cos Example output: NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD cos cos-apps.apps.demo.ibmdte.net cos 9443-tcp reencrypt/Redirect None Verify your pod is ready: oc get pod Example output: NAME READY STATUS RESTARTS AGE cos-54975b94c6-rh6kt 1/1 Running 0 3m11s Get the application URL: echo http://$(oc get route cos --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-apps.apps.demo.ibmdte.net/CustomerOrderServicesWeb Return to your Firefox browser window and go to the URL outputted by the command run in the previous step. You will be prompted to login in order to access the application. Enter the following credentials: Username: skywalker Password: force After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add multiple items to the shopping cart to trigger more logging. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total . Application Logging (Hands-on) Pod processes running in OpenShift frequently produce logs. To effectively manage this log data and ensure no loss of log data occurs when a pod terminates, a log aggregation tool should be deployed on the cluster. Log aggregation tools help users persist, search, and visualize the log data that is gathered from the pods across the cluster. Let's look at application logging with log aggregation using EFK (Elasticsearch, Fluentd, and Kibana). Elasticsearch is a search and analytics engine. Fluentd receives, cleans and parses the log data. Kibana lets users visualize data stored in Elasticsearch with charts and graphs. If it has been a long time (more than 15 minutes) since the Liberty or WebSphere pods last started, you may want to delete each pod and let a new one start, to ensure that Liberty and WebSphere create some recent logs for Kibana to find. Launch Kibana (Hands-on) In OpenShift console, from the left-panel, select Networking > Routes . From the Project drop-down list, select openshift-logging . Click on the route URL (listed under the Location column). Click on Log in with OpenShift . Click on Allow selected permissions . In Kibana console, you'll be prompted to create an index pattern. An index pattern tells Kibana what indices to look for in Elasticsearch. Type app so that the index pattern looks like this screenshot: You should see that your pattern matches at least one index. Then click Next step . Click the drop-down for Time Filter field name and choose @timestamp . Then click Create index pattern . You should see a number of fields populated, around 260. To check that the correct fields have been detected, type ibm in the Filter text box. You should see many fields beginning with the text ibm . If not, try clicking the refresh button (arrows in a circle) in the top right of the page. Import dashboards (Hands-on) Download this zip file containing dashboards to your computer and unzip to a local directory. (Look for the download button on the page.) Let's import dashboards for Liberty and WAS. From the left-panel, click on Management . Click on Saved Objects tab and then click on Import , then Import at the top of the panel that appears. Navigate to the kibana sub-directory and select ibm-open-liberty-kibana5-problems-dashboard.json file. Then click the Import button at the bottom of the panel. When prompted to resolve pattern conflicts, click select app* as the new index and click Confirm all changes . It'll take few seconds for the dashboard to import. Click Done when it finishes. Repeat the steps to import ibm-open-liberty-kibana5-traffic-dashboard.json and ibm-websphere-traditional-kibana5-dashboard.json . Explore dashboards (Hands-on) In Kibana console, from the left-panel, click on Dashboard . You'll see 3 dashboards on the list. The first 2 are for Liberty. The last one is for WAS traditional. Read the description next to each dashboard. Liberty applications (Hands-on) Click on Liberty-Problems-K5-20191122 . This dashboard visualizes message, trace and FFDC information from Liberty applications. By default, data from the last 15 minutes are rendered. Adjust the time-range (from the top-right corner), so that it includes data from when you tried the Open Liberty application. Once the data is rendered, you'll see some information about the namespace, pod, containers where events/problems occurred along with a count for each. Scroll down to Liberty Potential Problem Count section which lists the number of ERROR, FATAL, SystemErr and WARNING events. You'll likely see some WARNING events. Below that you'll see Liberty Top Message IDs . This helps to quickly identify most occurring events and their timeline. Scroll-up and click on the number below WARNING. Dashboard will change other panels to show just the events for warnings. Using this, you can determine: whether the failures occurred on one particular pod/server or in multiple instances, whether they occurred around the same or different time. Scroll-down to the actual warning messages. In this case some files from dojo were not found. Even though they are warnings, it'll be good to fix them by updating the application (we won't do that as part of this workshop). Go back to the list of dashboards and click on Liberty-Traffic-K5-20191122 . This dashboard helps to identify failing or slow HTTP requests on Liberty applications. As before, adjust the time-range as necessary if no data is rendered. You'll see some information about the namespace, pod, containers for the traffic along with a count for each. Scroll-down to Liberty Error Response Code Count section which lists the number of requests failed with HTTP response codes in 400s and 500s ranges. Scroll-down to Liberty Top URLs which lists the most frequently accessed URLs The /health and /metrics endpoints are running on the same server and are queried frequently for readiness/liveness probes and for scraping metrics information. It's possible to add a filter to include/exclude certain applications. On the right-hand side, you'll see list of endpoints that had the slowest response times. Scroll-up and click on the number listed below 400s. Dashboard will change other panels to show just the traffic with response code in 400s. You can see the timeline and the actual messages below. These are related to warnings from last dashboard about dojo files not being found (response code 404). Traditional WebSphere applications (Hands-on) Go back to the list of dashboards and click on WAS-traditional-Problems-K5-20190609 . Similar to the first dashboard for Liberty, this dashboard visualizes message and trace information for WebSphere Application Server traditional. As before, adjust the time-range as necessary if no data is rendered. Explore the panels and filter through the events to see messages corresponding to just those events. Application Monitoring (Hands-on) Building observability into applications externalizes the internal status of a system to enable operations teams to monitor systems more effectively. It is important that applications are written to produce metrics. When the Customer Order Services application was modernized, we used MicroProfile Metrics and it provides a /metrics endpoint from where you can access all metrics emitted by the JVM, Open Liberty server and deployed applications. Operations teams can gather the metrics and store them in a database by using tools like Prometheus. The metrics data can then be visualized and analyzed in dashboards, such as Grafana. Grafana dashboard (Hands-on) Custom resource GrafanaDashboard defines a set of dashboards for monitoring Customer Order Services application and Open Liberty. In your terminal, run the following command to create the dashboard resource: Before running the command, change directory to /openshift-workshop-was/labs/Openshift/ApplicationManagement if it's not already done. oc apply -f dashboards/grafana/grafana-dashboard-cos.yaml The following steps to access the created dashboard are illustrated in the screen recording at the end of this section: In OpenShift console, from the left-panel, select Networking > Routes . From the Project drop-down list, select app-monitoring . Click on the route URL (listed under the Location column). Click on Log in with OpenShift . Click on Allow selected permissions . In Grafana, from the left-panel, hover over the dashboard icon and click on Manage . You should see Liberty-Metrics-Dashboard on the list. Click on it. Explore the dashboards. The first 2 are for Customer Order Services application. The rest are for Liberty. Click on Customer Order Services - Shopping Cart . By default, it'll show the data for the last 15 minutes. Adjust the time-range from the top-right as necessary. You should see the frequency of requests, number of requests, pod information, min/max request times. Scroll-down to expand the CPU section. You'll see information about process CPU time, CPU system load for pods. Scroll-down to expand the Servlets section. You'll see request count and response times for application servlet as well as health and metrics endpoints. Explore the other sections. Day-2 Operations (Hands-on) You may need to gather server traces and/or dumps for analyzing some problems. Open Liberty Operator makes it easy to gather these on a server running inside a container. A storage must be configured so the generated artifacts can persist, even after the Pod is deleted. This storage can be shared by all instances of the Open Liberty applications. RedHat OpenShift on IBM Cloud utilizes the storage capabilities provided by IBM Cloud. Let's create a request for storage. Request storage (Hands-on) In OpenShift console, from the left-panel, select Storage > Persistent Volume Claims . From the Project drop-down list, select apps . Click on Create Persistent Volume Claim button. Ensure that Storage Class is managed-nfs . If not, make the selection from the list. Enter liberty for Persistent Volume Claim Name field. Request 1 GiB by entering 1 in the text box for Size . Click on Create . Created Persistent Volume Claim will be displayed. The Status field would display Pending . Wait for it to change to Bound . It may take 1-2 minutes. Once bound, you should see the volume displayed under Persistent Volume field. Enable serviceability (Hands-on) Enable serviceability option for the Customer Order Services application. In productions systems, it's recommended that you do this step with the initial deployment of the application - not when you encounter an issue and need to gather server traces or dumps. OpenShift cannot attach volumes to running Pods so it'll have to create a new Pod, attach the volume and then take down the old Pod. If the problem is intermittent or hard to reproduce, you may not be able to reproduce it on the new instance of server running in the new Pod. The volume can be shared by all Liberty applications that are in the same namespace and the volumes wouldn't be used unless you perform day-2 operation on a particular application - so that should make it easy to enable serviceability with initial deployment. Specify the name of the storage request (Persistent Volume Claim) you made earlier to spec.serviceability.volumeClaimName parameter provided by OpenLibertyApplication custom resource. Open Liberty Operator will attach the volume bound to the claim to each instance of the server. In your terminal, run the following command: oc patch olapp cos -n apps --patch '{\"spec\":{\"serviceability\":{\"volumeClaimName\":\"liberty\"}}}' --type=merge Above command patches the definition of olapp (shortname for OpenLibertyApplication ) instance cos in namespace apps (indicated by -n option). The --patch option specifies the content to patch with. In this case, we set the value of spec.serviceability.volumeClaimName field to liberty , which is the name of the Persistent Volume Claim you created earlier. The --type=merge option specifies to merge the previous content with newly specified field and its value. Run the following command to get the status of cos application, to verify that the changes were reconciled and there is no error: oc get olapp cos -n apps -o wide The value under RECONCILED column should be True . Note: If it's False then an error occurred. The REASON and MESSAGE columns will display the cause of the failure. A common mistake is creating the Persistent Volume Claim in another namespace. Ensure that it is created in the apps namespace. In OpenShift console, from the left-panel, click on Workloads > Pods . Wait until there is only 1 pod on the list and its Readiness column changed to Ready . Pod's name is needed for requesting server dump and trace in the next sections. Click on the pod and copy the value under Name field. Request server dump (Hands-on) You can request a snapshot of the server status including different types of server dumps, from an instance of Open Liberty server running inside a Pod, using Open Liberty Operator and OpenLibertyDump custom resource (CR). The following steps to request a server dump are illustrated in the screen recording below: From the left-panel, click on Operators > Installed Operators . From the Open Liberty Operator row, click on Open Liberty Dump (displayed under Provided APIs column). Click on Create OpenLibertyDump button. Replace Specify_Pod_Name_Here with the pod name you copied earlier. The include field specifies the type of server dumps to request. Heap and thread dumps are specified by default. Let's use the default values. Click on Create . Click on example-dump from the list. Scroll-down to the Conditions section and you should see Started status with True value. Wait for the operator to complete the dump operation. You should see status Completed with True value. Request server traces (Hands-on) You can also request server traces, from an instance of Open Liberty server running inside a Pod, using OpenLibertyTrace custom resource (CR). The following steps to request a server trace are illustrated in the screen recording below: From the left-panel, click on Operators > Installed Operators . From the Open Liberty Operator row, click on Open Liberty Trace . Click on Create OpenLibertyTrace button. Replace Specify_Pod_Name_Here with the pod name you copied earlier. The traceSpecification field specifies the trace string to be used to selectively enable trace on Liberty server. Let's use the default value. Click on Create . Click on example-trace from the list. Scroll-down to the Conditions section and you should see Enabled status with True value. Note: Once the trace has started, it can be stopped by setting the disable parameter to true. Deleting the CR will also stop the tracing. Changing the podName will first stop the tracing on the old Pod before enabling traces on the new Pod. Maximum trace file size (in MB) and the maximum number of files before rolling over can be specified using maxFileSize and maxFiles parameters. Accessing the generated files (Hands-on) The generated trace and dump files should now be in the persistent volume. You used storage from IBM Cloud and we have to go through a number of steps using a different tool to access those files. Since the volume is attached to the Pod, we can instead use Pod's terminal to easily verify that trace and dump files are present. The following steps to access the files are illustrated in the screen recording below: Remote shell to your pod via one of two ways: From your terminal: oc rsh <pod-name> From console: click on Workloads > Pods . Click on the pod and then click on Terminal tab. Enter ls -R serviceability/apps to list the files. The shared volume is mounted at serviceability folder. The sub-folder apps is the namespace of the Pod. You should see a zip file for dumps and trace log files. These are produced by the day-2 operations you performed. Summary Congratulations! You've completed Application Management lab!","title":"Application Management"},{"location":"appmod-labs/ApplicationManagement/README - Copy/#application-management","text":"","title":"Application Management"},{"location":"appmod-labs/ApplicationManagement/README - Copy/#table-of-contents","text":"Introduction Prerequisites Application Logging (Hands-on) Application Monitoring (Hands-on) Day-2 Operations (Hands-on) Summary","title":"Table of Contents"},{"location":"appmod-labs/ApplicationManagement/README - Copy/#introduction","text":"In this lab, you'll learn about managing your running applications efficiently using various tools available to you as part of OpenShift and OperatorHub, including the Open Liberty Operator.","title":"Introduction"},{"location":"appmod-labs/ApplicationManagement/README - Copy/#login-to-the-vm","text":"If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password: \\ Password: engageibm Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window.","title":"Login to the VM"},{"location":"appmod-labs/ApplicationManagement/README - Copy/#prerequisites","text":"Open a terminal window from the VM desktop. Login to OpenShift CLI with the oc login command from the web terminal. When prompted for the username and password, enter the following login credentials: Username: ibmadmin Password: engageibm If you have not yet cloned the GitHub repo with the lab artifacts, then run the following command on your terminal: git clone https://github.com/IBM/openshift-workshop-was.git","title":"Prerequisites"},{"location":"appmod-labs/ApplicationManagement/README - Copy/#build-and-deploy-the-traditional-websphere-application-hands-on","text":"Change to the lab's directory: cd openshift-workshop-was/labs/Openshift/OperationalModernization Create and switch over to the project apps-was . > Note: The first step oc new-project may fail if the project already exists. If so, proceed to the next command. oc new-project apps-was oc project apps-was Build and deploy the application by running the commands in the following sequence. Reminder: the . at the end of the first command. docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was . docker login -u openshift -p $(oc whoami -t) default-route-openshift-image-registry.apps.demo.ibmdte.net docker push default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was oc apply -f deploy Example output: deployment.apps/cos-was created route.route.openshift.io/cos-was created secret/authdata created service/cos-was created Wait for the pod to be available, check status via oc get pod The output should be: NAME READY STATUS RESTARTS AGE cos-was-7d5ff6945-4hjzr 1/1 Running 0 88s Get the URL to the application: echo http://$(oc get route cos-was --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-was-apps-was.apps.demo.ibmdte.net/CustomerOrderServicesWeb Return to your Firefox browser window, and go to the URL outputted by the command run in the previous step. You will be prompted to login in order to access the application. Enter the following credentials: Username: skywalker Password: force After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total . Close the browser.","title":"Build and deploy the traditional WebSphere application (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/README - Copy/#build-and-deploy-the-liberty-application","text":"(Skip this step if you just finished the previous lab Runtime Modernization and did not clean up the deployment) Note: The following steps are to help you re-deploy the Liberty application if the deployment has been deleted from the previous lab Runtime Modernization. The deployment is required to generate data for the follow-on steps in monitoring. From the top level directory, change to the lab's directory RuntimeModernization folder: cd openshift-workshop-was/labs/Openshift/RuntimeModernization Create and switch over to the project apps . Also, enable monitoring for the project. > Note: The first step oc new-project may fail if the project already exists. If so, proceed to the next command. oc new-project apps oc project apps oc label namespace apps app-monitoring=true Build and deploy the application by running the commands in the following sequence. Reminder: the . at the end of the first command: docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos . docker login -u openshift -p $(oc whoami -t) default-route-openshift-image-registry.apps.demo.ibmdte.net docker push default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos oc apply -k deploy/overlay-apps Example output: configmap/cos-config created secret/db-creds created secret/liberty-creds created openlibertyapplication.openliberty.io/cos created Verify the route for the application is created: oc get route cos Example output: NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD cos cos-apps.apps.demo.ibmdte.net cos 9443-tcp reencrypt/Redirect None Verify your pod is ready: oc get pod Example output: NAME READY STATUS RESTARTS AGE cos-54975b94c6-rh6kt 1/1 Running 0 3m11s Get the application URL: echo http://$(oc get route cos --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-apps.apps.demo.ibmdte.net/CustomerOrderServicesWeb Return to your Firefox browser window and go to the URL outputted by the command run in the previous step. You will be prompted to login in order to access the application. Enter the following credentials: Username: skywalker Password: force After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add multiple items to the shopping cart to trigger more logging. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total .","title":"Build and deploy the Liberty application"},{"location":"appmod-labs/ApplicationManagement/README - Copy/#application-logging-hands-on","text":"Pod processes running in OpenShift frequently produce logs. To effectively manage this log data and ensure no loss of log data occurs when a pod terminates, a log aggregation tool should be deployed on the cluster. Log aggregation tools help users persist, search, and visualize the log data that is gathered from the pods across the cluster. Let's look at application logging with log aggregation using EFK (Elasticsearch, Fluentd, and Kibana). Elasticsearch is a search and analytics engine. Fluentd receives, cleans and parses the log data. Kibana lets users visualize data stored in Elasticsearch with charts and graphs. If it has been a long time (more than 15 minutes) since the Liberty or WebSphere pods last started, you may want to delete each pod and let a new one start, to ensure that Liberty and WebSphere create some recent logs for Kibana to find.","title":"Application Logging (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/README - Copy/#launch-kibana-hands-on","text":"In OpenShift console, from the left-panel, select Networking > Routes . From the Project drop-down list, select openshift-logging . Click on the route URL (listed under the Location column). Click on Log in with OpenShift . Click on Allow selected permissions . In Kibana console, you'll be prompted to create an index pattern. An index pattern tells Kibana what indices to look for in Elasticsearch. Type app so that the index pattern looks like this screenshot: You should see that your pattern matches at least one index. Then click Next step . Click the drop-down for Time Filter field name and choose @timestamp . Then click Create index pattern . You should see a number of fields populated, around 260. To check that the correct fields have been detected, type ibm in the Filter text box. You should see many fields beginning with the text ibm . If not, try clicking the refresh button (arrows in a circle) in the top right of the page.","title":"Launch Kibana (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/README - Copy/#import-dashboards-hands-on","text":"Download this zip file containing dashboards to your computer and unzip to a local directory. (Look for the download button on the page.) Let's import dashboards for Liberty and WAS. From the left-panel, click on Management . Click on Saved Objects tab and then click on Import , then Import at the top of the panel that appears. Navigate to the kibana sub-directory and select ibm-open-liberty-kibana5-problems-dashboard.json file. Then click the Import button at the bottom of the panel. When prompted to resolve pattern conflicts, click select app* as the new index and click Confirm all changes . It'll take few seconds for the dashboard to import. Click Done when it finishes. Repeat the steps to import ibm-open-liberty-kibana5-traffic-dashboard.json and ibm-websphere-traditional-kibana5-dashboard.json .","title":"Import dashboards (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/README - Copy/#explore-dashboards-hands-on","text":"In Kibana console, from the left-panel, click on Dashboard . You'll see 3 dashboards on the list. The first 2 are for Liberty. The last one is for WAS traditional. Read the description next to each dashboard.","title":"Explore dashboards (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/README - Copy/#liberty-applications-hands-on","text":"Click on Liberty-Problems-K5-20191122 . This dashboard visualizes message, trace and FFDC information from Liberty applications. By default, data from the last 15 minutes are rendered. Adjust the time-range (from the top-right corner), so that it includes data from when you tried the Open Liberty application. Once the data is rendered, you'll see some information about the namespace, pod, containers where events/problems occurred along with a count for each. Scroll down to Liberty Potential Problem Count section which lists the number of ERROR, FATAL, SystemErr and WARNING events. You'll likely see some WARNING events. Below that you'll see Liberty Top Message IDs . This helps to quickly identify most occurring events and their timeline. Scroll-up and click on the number below WARNING. Dashboard will change other panels to show just the events for warnings. Using this, you can determine: whether the failures occurred on one particular pod/server or in multiple instances, whether they occurred around the same or different time. Scroll-down to the actual warning messages. In this case some files from dojo were not found. Even though they are warnings, it'll be good to fix them by updating the application (we won't do that as part of this workshop). Go back to the list of dashboards and click on Liberty-Traffic-K5-20191122 . This dashboard helps to identify failing or slow HTTP requests on Liberty applications. As before, adjust the time-range as necessary if no data is rendered. You'll see some information about the namespace, pod, containers for the traffic along with a count for each. Scroll-down to Liberty Error Response Code Count section which lists the number of requests failed with HTTP response codes in 400s and 500s ranges. Scroll-down to Liberty Top URLs which lists the most frequently accessed URLs The /health and /metrics endpoints are running on the same server and are queried frequently for readiness/liveness probes and for scraping metrics information. It's possible to add a filter to include/exclude certain applications. On the right-hand side, you'll see list of endpoints that had the slowest response times. Scroll-up and click on the number listed below 400s. Dashboard will change other panels to show just the traffic with response code in 400s. You can see the timeline and the actual messages below. These are related to warnings from last dashboard about dojo files not being found (response code 404).","title":"Liberty applications (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/README - Copy/#traditional-websphere-applications-hands-on","text":"Go back to the list of dashboards and click on WAS-traditional-Problems-K5-20190609 . Similar to the first dashboard for Liberty, this dashboard visualizes message and trace information for WebSphere Application Server traditional. As before, adjust the time-range as necessary if no data is rendered. Explore the panels and filter through the events to see messages corresponding to just those events.","title":"Traditional WebSphere applications (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/README - Copy/#application-monitoring-hands-on","text":"Building observability into applications externalizes the internal status of a system to enable operations teams to monitor systems more effectively. It is important that applications are written to produce metrics. When the Customer Order Services application was modernized, we used MicroProfile Metrics and it provides a /metrics endpoint from where you can access all metrics emitted by the JVM, Open Liberty server and deployed applications. Operations teams can gather the metrics and store them in a database by using tools like Prometheus. The metrics data can then be visualized and analyzed in dashboards, such as Grafana.","title":"Application Monitoring (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/README - Copy/#grafana-dashboard-hands-on","text":"Custom resource GrafanaDashboard defines a set of dashboards for monitoring Customer Order Services application and Open Liberty. In your terminal, run the following command to create the dashboard resource: Before running the command, change directory to /openshift-workshop-was/labs/Openshift/ApplicationManagement if it's not already done. oc apply -f dashboards/grafana/grafana-dashboard-cos.yaml The following steps to access the created dashboard are illustrated in the screen recording at the end of this section: In OpenShift console, from the left-panel, select Networking > Routes . From the Project drop-down list, select app-monitoring . Click on the route URL (listed under the Location column). Click on Log in with OpenShift . Click on Allow selected permissions . In Grafana, from the left-panel, hover over the dashboard icon and click on Manage . You should see Liberty-Metrics-Dashboard on the list. Click on it. Explore the dashboards. The first 2 are for Customer Order Services application. The rest are for Liberty. Click on Customer Order Services - Shopping Cart . By default, it'll show the data for the last 15 minutes. Adjust the time-range from the top-right as necessary. You should see the frequency of requests, number of requests, pod information, min/max request times. Scroll-down to expand the CPU section. You'll see information about process CPU time, CPU system load for pods. Scroll-down to expand the Servlets section. You'll see request count and response times for application servlet as well as health and metrics endpoints. Explore the other sections.","title":"Grafana dashboard (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/README - Copy/#day-2-operations-hands-on","text":"You may need to gather server traces and/or dumps for analyzing some problems. Open Liberty Operator makes it easy to gather these on a server running inside a container. A storage must be configured so the generated artifacts can persist, even after the Pod is deleted. This storage can be shared by all instances of the Open Liberty applications. RedHat OpenShift on IBM Cloud utilizes the storage capabilities provided by IBM Cloud. Let's create a request for storage.","title":"Day-2 Operations (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/README - Copy/#request-storage-hands-on","text":"In OpenShift console, from the left-panel, select Storage > Persistent Volume Claims . From the Project drop-down list, select apps . Click on Create Persistent Volume Claim button. Ensure that Storage Class is managed-nfs . If not, make the selection from the list. Enter liberty for Persistent Volume Claim Name field. Request 1 GiB by entering 1 in the text box for Size . Click on Create . Created Persistent Volume Claim will be displayed. The Status field would display Pending . Wait for it to change to Bound . It may take 1-2 minutes. Once bound, you should see the volume displayed under Persistent Volume field.","title":"Request storage (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/README - Copy/#enable-serviceability-hands-on","text":"Enable serviceability option for the Customer Order Services application. In productions systems, it's recommended that you do this step with the initial deployment of the application - not when you encounter an issue and need to gather server traces or dumps. OpenShift cannot attach volumes to running Pods so it'll have to create a new Pod, attach the volume and then take down the old Pod. If the problem is intermittent or hard to reproduce, you may not be able to reproduce it on the new instance of server running in the new Pod. The volume can be shared by all Liberty applications that are in the same namespace and the volumes wouldn't be used unless you perform day-2 operation on a particular application - so that should make it easy to enable serviceability with initial deployment. Specify the name of the storage request (Persistent Volume Claim) you made earlier to spec.serviceability.volumeClaimName parameter provided by OpenLibertyApplication custom resource. Open Liberty Operator will attach the volume bound to the claim to each instance of the server. In your terminal, run the following command: oc patch olapp cos -n apps --patch '{\"spec\":{\"serviceability\":{\"volumeClaimName\":\"liberty\"}}}' --type=merge Above command patches the definition of olapp (shortname for OpenLibertyApplication ) instance cos in namespace apps (indicated by -n option). The --patch option specifies the content to patch with. In this case, we set the value of spec.serviceability.volumeClaimName field to liberty , which is the name of the Persistent Volume Claim you created earlier. The --type=merge option specifies to merge the previous content with newly specified field and its value. Run the following command to get the status of cos application, to verify that the changes were reconciled and there is no error: oc get olapp cos -n apps -o wide The value under RECONCILED column should be True . Note: If it's False then an error occurred. The REASON and MESSAGE columns will display the cause of the failure. A common mistake is creating the Persistent Volume Claim in another namespace. Ensure that it is created in the apps namespace. In OpenShift console, from the left-panel, click on Workloads > Pods . Wait until there is only 1 pod on the list and its Readiness column changed to Ready . Pod's name is needed for requesting server dump and trace in the next sections. Click on the pod and copy the value under Name field.","title":"Enable serviceability (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/README - Copy/#request-server-dump-hands-on","text":"You can request a snapshot of the server status including different types of server dumps, from an instance of Open Liberty server running inside a Pod, using Open Liberty Operator and OpenLibertyDump custom resource (CR). The following steps to request a server dump are illustrated in the screen recording below: From the left-panel, click on Operators > Installed Operators . From the Open Liberty Operator row, click on Open Liberty Dump (displayed under Provided APIs column). Click on Create OpenLibertyDump button. Replace Specify_Pod_Name_Here with the pod name you copied earlier. The include field specifies the type of server dumps to request. Heap and thread dumps are specified by default. Let's use the default values. Click on Create . Click on example-dump from the list. Scroll-down to the Conditions section and you should see Started status with True value. Wait for the operator to complete the dump operation. You should see status Completed with True value.","title":"Request server dump (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/README - Copy/#request-server-traces-hands-on","text":"You can also request server traces, from an instance of Open Liberty server running inside a Pod, using OpenLibertyTrace custom resource (CR). The following steps to request a server trace are illustrated in the screen recording below: From the left-panel, click on Operators > Installed Operators . From the Open Liberty Operator row, click on Open Liberty Trace . Click on Create OpenLibertyTrace button. Replace Specify_Pod_Name_Here with the pod name you copied earlier. The traceSpecification field specifies the trace string to be used to selectively enable trace on Liberty server. Let's use the default value. Click on Create . Click on example-trace from the list. Scroll-down to the Conditions section and you should see Enabled status with True value. Note: Once the trace has started, it can be stopped by setting the disable parameter to true. Deleting the CR will also stop the tracing. Changing the podName will first stop the tracing on the old Pod before enabling traces on the new Pod. Maximum trace file size (in MB) and the maximum number of files before rolling over can be specified using maxFileSize and maxFiles parameters.","title":"Request server traces (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/README - Copy/#accessing-the-generated-files-hands-on","text":"The generated trace and dump files should now be in the persistent volume. You used storage from IBM Cloud and we have to go through a number of steps using a different tool to access those files. Since the volume is attached to the Pod, we can instead use Pod's terminal to easily verify that trace and dump files are present. The following steps to access the files are illustrated in the screen recording below: Remote shell to your pod via one of two ways: From your terminal: oc rsh <pod-name> From console: click on Workloads > Pods . Click on the pod and then click on Terminal tab. Enter ls -R serviceability/apps to list the files. The shared volume is mounted at serviceability folder. The sub-folder apps is the namespace of the Pod. You should see a zip file for dumps and trace log files. These are produced by the day-2 operations you performed.","title":"Accessing the generated files (Hands-on)"},{"location":"appmod-labs/ApplicationManagement/README - Copy/#summary","text":"Congratulations! You've completed Application Management lab!","title":"Summary"},{"location":"appmod-labs/DevopsModernization/","text":"Devops Modernization Lab Under Development","title":"Home"},{"location":"appmod-labs/DevopsModernization/#devops-modernization-lab","text":"","title":"Devops Modernization Lab"},{"location":"appmod-labs/DevopsModernization/#under-development","text":"","title":"Under Development"},{"location":"appmod-labs/OperationalModernization/","text":"Operational Modernization Table of Contents Introduction Analysis (Hands-on) Build (Hands-on) Deploy without operator (Hands-on) Access the Application without operator (Hands-on) Alternate Deployment via Runtime Component Operator (Hands-on) Summary Next Introduction Operational modernization gives an operations team the opportunity to embrace modern operations best practices without putting change requirements on the development team. Modernizing from WebSphere Network Deployment (ND) to the traditional WebSphere Application Server Base V9 runtime in a container allows the application to be moved to the cloud without code changes. This type of modernization shouldn't require any code changes and can be driven by the operations team. This path gets the application in to a container with the least amount of effort but doesn't modernize the application or the runtime. In this lab, we'll use Customer Order Services application as an example. In order to modernize, the application will go through analysis , build and deploy phases. Click here and get to know the application, its architecture and components. Login to the VM If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password. Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window. Analysis (Hands-on) IBM Cloud Transformation Advisor (TA) can be used to analyze the Customer Order Service Application running in the WebSphere ND environment. The Transformation Advisor helps you to analyze your on-premises workloads for modernization. It determines the complexity of your applications, estimates a development cost to perform the move to the cloud, and recommends the best target environment. Let's get started using TA to analyze the existing Customer Order Services application: Open a Firefox browser window from within the VM. Click on the openshift console bookmark in the top left and log in with the htpasswd option. Log in to the OpenShift account using the following credentials: Username: ibmadmin Password: engageibm From the Red Hat OpenShift Container Platform console, go to the Networking tab and click on Routes . Note: Ensure that you are in the ta project by using the project drop down and click on the Location URL next to ta-ui-route . This will open the Transformation Advisor user interface. Click Create new under Workspaces to create a new workspace. Name it OperationalModernization and click Next . You'll be asked to create a new collection to store the data collected from the Customer Order Services application. Name the new collection CustomerOrderServices . Click Create . To provide application assessment data and receive recommendations, you would typically download and execute the Data Collector against an existing WebSphere environment. The output from the data collector is a zip file containing the application and configuration metadata gathered from the WebSphere Server, and is the input to the IBM Transformation Advsor tool. However, for this lab, the data collection archive has already been created for you and the resulting data is stored here . Click the Upload button, as we already have the data collectin archive to upload. Upload the results of the data collection (the datacollector.zip file) to IBM Cloud Transformation Advisor. When the upload is complete, you will see a list of applications analyzed from the source environment. At the top of the page, you can see the source environment and the target environment settings. Under the Migration target field, click the down arrow and select Compatible runtimes . This will show you an entry for each application for each compatible destination runtime you can migrate it to. Click the CustomerOrderServicesApp.ear application with the WebSphere traditional migration target to open the Application details page . Look over the migration analysis. From the migratin analysis data, you can: view a summary of the complexity of migrating this application to this target see detailed information about issues view additional reports about the application. In summary, no code changes are required to move this application to the traditional WebSphere Base v9 runtime , so it is a good candidate to proceed with the operational modernization. Click on View migration plan in the top right corner of the page. This page will help you assemble an archive containing: your application's source or binary files (you upload these here or specify Maven coordinates to download them) any required drivers or libraries (you upload these here or specify Maven coordinates to download them) the wsadmin scripts needed to configure your application and its resources (generated by Transformation Advisor and automatically included) the deployment artifacts needed to create the container image and deploy the application to OCP (generated by Transformation Advisor and automatically included) NOTE: These artifacts have already been provided for you as part of the lab files, so you don't need to download the migration plan. However, you can do so if you wish to look around at the files. These files can also be sent to a Git repository by Transformation Advisor. Optional: For a more detailed walkthrough of the Transformation Advisor process, you may Read through this document . Please note that the lab environment DOES NOT have WebSphere installed. Therefore, you cannot actually perform the steps to scan the WebShere server as described in the \"this document\" link shown above. That additonal resource is provided as additional context for how IBM Transformation Advisor tool is used to scan WebSphere servers and collect the migration data. Build (Hands-on) In this section, you'll learn how to build a Docker image for Customer Order Services application running on traditional WebSphere Base v9. Building this image could take around ~8 minutes. So, let's kick that process off before explaining what you did. The image should be built by the time you complete this section. Open a new terminal window from the VM desktop. Login to OpenShift CLI with the oc login command from the web terminal. When prompted for the username and password, enter the following login credentials: Username: ibmadmin Password: engageibm If you have not yet cloned the GitHub repo with the lab artifacts, run the following command on your terminal: git clone https://github.com/IBM/openshift-workshop-was.git Change directory to where this lab is located: cd openshift-workshop-was/labs/Openshift/OperationalModernization ls Run the following command to create a new project named apps-was in OpenShift. oc new-project apps-was Example output: Now using project \"apps-was\" on server \"https://c115-e.us-south.containers.cloud.ibm.com:32661\". . . . Run the following command to start building the image. Make sure to copy the entire command, including the \".\" at the end (which indicates current directory). docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was . Build image (Hands-on) Review the docker build command you ran earlier: docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was . It instructs docker to build the image following the instructions in the Dockerfile in current directory (indicated by the \".\" at the end). A specific name to tag the built image is also specified after --tag The value default-route-openshift-image-registry.apps.demo.ibmdte.net in the tag is the default address of the internal image registry provided by OpenShift. Image registry is a content server that can store and serve container images. The registry is accessible within the cluster via its exposed Service . The format of a Service address: name . namespace . svc . In this case, the image registry is named image-registry and it's in namespace openshift-image-registry . Later when we push the image to OpenShift's internal image registry, we'll refer to the image by the same values. You should see the following message if the image was successfully built. Please wait if it's still building. Successfully built aa6babbb5ce9 Successfully tagged default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was:latest Validate that image is in the repository by running the command: docker images Notice that the base image, websphere-traditional, is also listed. It was pulled as the first step of building application image. Example output: REPOSITORY TAG IMAGE ID CREATED SIZE default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was latest 9394150a5a15 10 minutes ago 2.05GB ibmcom/websphere-traditional latest 898f9fd79b36 12 minutes ago 1.86GB Note that docker images only lists those images that are cached locally. The name of the image also contains the host name where the image is hosted. If there is no host name, the image is hosted on docker hub. For example, the image ibmcom/websphere-traditional has no host name. It is hosted on docker hub. The image we just built, default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was , has host name default-route-openshift-image-registry.apps.demo.ibmdte.net . It is to be hosted in the Openshift image registry for your lab cluster. If you change an image, or build a new image, the changes are only available locally. You must push the image to propagate the changes to the remote registry. Let's push the image you just built to your OpenShift cluster's built-in image registry. a. First, login to the image registry by running the following command in the terminal. Note: A session token is obtained from the value of another command oc whoami -t and used as the password to login. docker login -u $(oc whoami) -p $(oc whoami -t) default-route-openshift-image-registry.apps.demo.ibmdte.net Example output: WARNING! Using --password via the CLI is insecure. Use --password-stdin. WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded b. Now, push the image into OpenShift cluster's internal image registry, which will take 1-2 minutes: docker push default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was Example output: Using default tag: latest The push refers to repository [default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was] 470e7d3b0bec: Pushed c38e61da8211: Pushed 2a72e88fe5eb: Pushed 334c79ff1b2e: Pushed ccf8ea26529f: Pushed af0f17433f77: Pushed 4254aef2aa12: Pushed 855301ffdcce: Pushed ea252e2474a5: Pushed 68a4c9686496: Pushed 87ecb86bc8e5: Pushed 066b59214d49: Pushed 211f972e9c63: Pushed b93eee2b1ddb: Pushed a6ab5ae423d9: Pushed 3f785cf0a0ae: Pushed latest: digest: sha256:4f4e8ae82fa22c83febc4f884b5026d01815fc704df6196431db8ed7a7def6a0 size: 3672 Verify that the image is in the image registry. The following command will get the images in the registry. Filter through the results to get only the image you pushed. Run the following command: oc get images | grep apps-was/cos-was The application image you just pushed should be listed. The hash of the image is stored alongside (indicated by the SHA-256 value). Example output: image-registry.openshift-image-registry.svc:5000/apps-was/cos-was@sha256:bc072d3b78ae6adcd843af75552965e5ed863bcce4fc3f1bc5d194570bc16953 OpenShift uses ImageStream to provide an abstraction for referencing container images from within the cluster. When an image is pushed to registry, an ImageStream is created automatically, if one doesn't already exist. Run the following command to see the ImageStream that's created: oc get imagestreams -n apps-was Example output: NAME IMAGE REPOSITORY TAGS UPDATED cos-was default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was latest 2 minutes ago You can also use the OpenShift console (UI) to see the ImageStream : From the panel on left-side, click on Builds > Image Streams . Then select apps-was from the Project drop-down menu. Click on cos-was from the list. Scroll down to the bottom to see the image that you pushed. Deploy without operator The following steps will deploy the modernized Customer Order Services application in a traditional WebSphere Base container to a RedHat OpenShift cluster. Customer Order Services application uses DB2 as its database. You can connect to an on-prem database that already exists or migrate the database to cloud. Since migrating the database is not the focus of this particular workshop and to save time, the database needed by the application is already configured in the OpenShift cluster you are using. Deploy application without operator (Hands-on) Run the following command to deploy the resources (*.yaml files) in the deploy directory: oc apply -f deploy Output: deployment.apps/cos-was created route.route.openshift.io/cos-was created secret/authdata created service/cos-was created Let's review what we just did. The directory deploy contains the following yaml files: Deployment.yaml : the specification for creating a Kubernetes deployment Service.yaml : the specification to expose the deployment as a cluster-wide Kubernetes service. Route.yaml : the specification to expose the service as a route visible outside of the cluster. Secret.yaml : the specification that the properties based configuration properties file used to configure database user/password when the container starts. The Deployment.yaml looks like: apiVersion: apps/v1 kind: Deployment metadata: name: cos-was namespace: apps-was spec: selector: matchLabels: app: cos-was replicas: 1 template: metadata: labels: app: cos-was spec: containers: - name: cos-was image: image-registry.openshift-image-registry.svc:5000/apps-was/cos-was ports: - containerPort: 9080 livenessProbe: httpGet: path: /CustomerOrderServicesWeb/index.html port: 9080 periodSeconds: 30 failureThreshold: 6 initialDelaySeconds: 90 readinessProbe: httpGet: path: /CustomerOrderServicesWeb/index.html port: 9080 periodSeconds: 10 failureThreshold: 3 volumeMounts: - mountPath: /etc/websphere name: authdata readOnly: true volumes: - name: authdata secret: secretName: authdata Note the following about the deployment yaml The liveness probe is used to tell Kubernetes when the application is live. Due to the size of the traditional WAS image, the initialDelaySeconds attribute has been set to 90 seconds to give the container time to start. The readiness probe is used to tell Kubernetes whether the application is ready to serve requests. You may store property file based configuration files such as configmaps and secrets , and bind their contents into the /etc/websphere directory. When the container starts, the server startup script will apply all the property files found in the /etc/websphere directory to reconfigure the server. For our example, the volumeMounts and volumes are used to bind the contents of the secret authdata into the directory /etc/websphere during container startup. After it is bound, it will appear as the file /etc/websphere/authdata.properties . For volumeMounts: The mountPath, /etc/websphere , specifies the directory where the files are bound. the name, authdata , specifies the name of the volume For volumes: the secretName specifies the name of the secret whose contents are to be bound. The Secret.yaml looks like: apiVersion: v1 kind: Secret metadata: name: authdata namespace: apps-was type: Opaque stringData: authdata.props: |- # # Configuration properties file for cells/DefaultCell01|security.xml#JAASAuthData_1597094577206# # Extracted on Tue Aug 11 15:30:36 UTC 2020 # # # Section 1.0 ## Cell=!{cellName}:Security=:JAASAuthData=alias#DBUser # # # SubSection 1.0.0 # JAASAuthData Section # ResourceType=JAASAuthData ImplementingResourceType=GenericType ResourceId=Cell=!{cellName}:Security=:JAASAuthData=alias#DBUser AttributeInfo=authDataEntries # # #Properties # password=\"{xor}Oz1tNjEsK24=\" #required alias=DBUser #required userId=db2inst1 #required description= # # End of Section 1.0# Cell=!{cellName}:Security=:JAASAuthData=alias#DBUser # # # EnvironmentVariablesSection # # #Environment Variables cellName=DefaultCell01 The attribute authdata.properties contains the properties file based configure used to update the database userId and password for the JAASAuthData whose alias is DBUser. The configuration in Deployment.yaml maps it as the file /etc/websphere/authdata.properties during container startup so that the application server startup script can automatically configure the server with these entries. Access the application without operator (Hands-on) Confirm you're at the current project apps-was : oc project Example output: Using project \"apps-was\" on server \"https://c114-e.us-south.containers.cloud.ibm.com:30016\". If it's not at the project apps-was , then switch: oc project apps-was Run the following command to verify the pod is running: oc get pod If the status does not show 1/1 READY, wait a while, checking status periodically: NAME READY STATUS RESTARTS AGE cos-was-6bd4767bf6-xhr92 1/1 Running 0 120m Run the following command to get the URL of your application (the route URL plus the application contextroot): echo http://$(oc get route cos-was --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-was-apps-was.apps.demo.ibmdte.net/CustomerOrderServicesWeb Return to your Firefox browser window and go to the URL outputted by the command run in the previous step. You will be prompted to login in order to access the application. Enter the following credentials: Username: skywalker Password: force After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total . Close the browser. Review the application workload flow without operator (Hands-on) Below is an overview diagram on the deployment you've completed from the above steps: Note: DB2 in the middle of the diagram is pre-installed through a different project db and has been up and running before your hands-on. Also it will not be impacted when you're removing the deployment in next step. Return to the Firefox browser and open the OpenShift Console to view the resources on the deployment. View the resources in the project apps-was : Select the apps-was project from the project drop down menu. View deployment details: Click on the Deployments tab under Workloads from the left menu and select cos-was Navigate to the YAML tab to view the content of yaml View pod details: Click on the Pods tab under Workloads from the left menu and select the pod with name starting with cos-was Navigate to the Logs tab to view the WebSphere Application Server log Navigate to the Terminal tab to view the files inside the container View secret details: Click on the Secrets tab under Workloads from the left menu and select the authdata secret. Scroll down to the Data section and click on the copy icon to view the content. View service details: Click on the Services tab under Networking from the left menu and select the cos-was service. Review service information including address and port mapping details. View route details: Click on the Routes tab under Networking from the left menu and select the cos-was route. View the resources in the project that contains the DB2 databse used by the application` View the resources in the project db : Select the db project from the project drop down menu. View deployment details: Click on the Deployments tab under Workloads from the left menu and select cos-db-was Navigate to the YAML tab to view the content of yaml View pod details: Click on the Pods tab under Workloads from the left menu and select the pod with name starting with cos-db-was Navigate to the Logs tab to view the database logs Navigate to the Terminal tab to view the files in the database container View service details: Click on the Services tab under Networking from the left menu and select the cos-db-was service. Remove your deployment (standard deployment without operator) (Hands-on) To remove the deploment from the above scenario without the operator, run the command: Note: The pre-installed resources such as DB2, are not removed. oc delete -f deploy Output: deployment.apps \"cos-was\" deleted route.route.openshift.io \"cos-was\" deleted secret \"authdata\" deleted service \"cos-was\" deleted Alternate Deployment Via Runtime Component Operator Another way to deploy the application is via the Runtime Component Operator . It is a generic operator used to deploy different types of application images. The operator has already been installed into your environment. For more information, see: https://github.com/application-stacks/runtime-component-operator Deploy application (via Runtime Component Operator) (Hands-on) Run the following command which uses the Runtime Component Operator to deploy the same Customer Order Service application image: oc apply -f deploy-rco Output: runtimecomponent.app.stacks/cos-was-rco created secret/authdata-rco created Let's review what we just did. First, list the contents of the deploy-rco directory: ls deploy-rco The output shows there are only two yaml files: RuntimeComponent.yaml Secret.yaml Review Secret.yaml : cat deploy-rco/Secret.yaml Note that it is the same as the Secret.yaml in the deploy directory, except the name has been changed to authdata-rco . It serves the same purpose for this new deployment - to override the database user/password. Review RuntimeComponent.yaml : cat deploy-rco/RuntimeComponent.yaml And the output: apiVersion: app.stacks/v1beta1 kind: RuntimeComponent metadata: name: cos-was-rco namespace: apps-was spec: applicationImage: image-registry.openshift-image-registry.svc:5000/apps-was/cos-was service: port: 9080 readinessProbe: httpGet: path: /CustomerOrderServicesWeb/index.html port: 9080 periodSeconds: 10 failureThreshold: 3 livenessProbe: httpGet: path: /CustomerOrderServicesWeb/index.html port: 9080 periodSeconds: 30 failureThreshold: 6 initialDelaySeconds: 90 expose: true route: termination: edge insecureEdgeTerminationPolicy: Redirect volumeMounts: - mountPath: /etc/websphere name: authdata-rco readOnly: true volumes: - name: authdata-rco secret: secretName: authdata-rco Note that: The kind is RuntimeComponent The expose attribute is set to true to expose a route The attributes within the yaml file are essentially the same information that you provided for the Service , Route , and Deployment resources in the deploy directory. The controller for the RuntimeComponent custom resource reacts to changes in the above specification, and creates the corresponding Service , Route , and Deployment objects. Issue the following commands to view what the controller has created: oc get Deployment cos-was-rco -o yaml oc get Service cos-was-rco -o yaml oc get Route cos-was-rco -o yaml Access the application (via Runtime Component Operator) (Hands-on) Ensure you are at the project apps-was : oc project apps-was Example output: Using project \"apps-was\" on server \"https://c114-e.us-south.containers.cloud.ibm.com:30016\". Run the following command to verify the pod is running: oc get pod If the status does not show 1/1 READY, wait a while, checking status periodically. Note the prefix name for the pod is cos-was-rco . NAME READY STATUS RESTARTS AGE cos-was-rco-6779784fc8-pz92m 1/1 Running 0 2m59s Run the following command to get the URL of your application (the route URL plus the application contextroot): echo http://$(oc get route cos-was-rco --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-was-rco-apps-was.apps.demo.ibmdte.net/CustomerOrderServicesWeb Return to your Firefox browser window and go to the URL outputted by the command run in the previous step. Note: The steps to access the application are the same as those used when deploying without the operator. You will be prompted to login in order to access the application. Enter the following credentials: Username: skywalker Password: force After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total . Close the browser. Review the application workload flow with Runtime Component Operator (Hands-on) Below is an overview diagram on the deployment you've completed from the above steps using Runtime Component Operator: Note: DB2 in the middle of the diagram is pre-installed through a different project db and has been up and running before your hands-on. Return to the Firefox browser and open the OpenShift Console to view the resources on the deployment. Review the resources in the project openshift-operators View the resources in the project openshift-operators : Select the openshift-operators project from the project drop down menu. View operator deployment details: Click on the Deployments tab under Workloads from the left menu and select runtime-component-operator Navigate to the YAML tab to view the content of yaml View operator pod details: Click on the Pods tab under Workloads from the left menu and select the pod with name starting with runtime-component-operator Navigate to Logs to view the runtime-component-operator container log Navigate to Terminal to view the files in the container View the resources in the project apps-was View the resources in the project apps-was : Select the apps-was project from the project drop down menu. View Runtime Component instance details: Click on the Installed Operators tab under Operators from the left menu and select Runtime Component Operator . Note: The operator is installed at cluster level and is visible to all existing projects, but Runtime Component instance is created under the project apps-was . Navigate to the YAML tab to view the content of yaml Navigate to the Runtime Component tab and select cos-was-rco to view the deails of Runtime Component instance View deployment details: Click on the Deployments tab under Workloads from the left menu and select cos-was-rco . Navigate to the YAML tab to view the content of yaml. Note the deployment is created through the controller of RuntimeComponent custom resource. View pod details: Click on the Pods tab under Workloads from the left menu and select the pod starting with cos-was-rco Navigate to the Logs tab to view the WebSphere Application Server log View service details: Click on the Services tab under Networking from the left menu and select cos-was-rco Navigate to the YAML tab to view the content of yaml. Note the service is created through the controller of RuntimeComponent custom resource. View route details: Click on the Routes tab under Networking from the left menu and select cos-was-rco Navigate to the YAML tab to view the content of yaml. Note the route is created through the controller of RuntimeComponent custom resource. View secret details: Click on the Secrets tab under Workloads from the left menu and select authdata-rco Resources in the project db : The Database resources are the Same as listed in the section above in Review the application workload flow without operator . Cleanup (the deployment with Runtime Component Operator) (Hands-on) Run the following command in a terminal window to remove the deployment from the above secenario with Runtime Component instance: Note: The pre-installed resources such as Runtime Component Operator, DB2, are not removed. oc delete -f deploy-rco Output: runtimecomponent.app.stacks \"cos-was-rco\" deleted secret \"authdata-rco\" deleted Verify that the corresponding application Service , Route , and Deployment have also been deleted: oc get Deployment oc get Service oc get Route Output from each get command above should be: No resources found in apps-was namespace. Summary Congratulations! You've completed the Operational Modernization lab. You containerized and deployed a monolith application to cloud! Next Please follow the link to the next lab Runtime Modernization : - Runtime Modernization","title":"Operational Modernization"},{"location":"appmod-labs/OperationalModernization/#operational-modernization","text":"","title":"Operational Modernization"},{"location":"appmod-labs/OperationalModernization/#table-of-contents","text":"Introduction Analysis (Hands-on) Build (Hands-on) Deploy without operator (Hands-on) Access the Application without operator (Hands-on) Alternate Deployment via Runtime Component Operator (Hands-on) Summary Next","title":"Table of Contents"},{"location":"appmod-labs/OperationalModernization/#introduction","text":"Operational modernization gives an operations team the opportunity to embrace modern operations best practices without putting change requirements on the development team. Modernizing from WebSphere Network Deployment (ND) to the traditional WebSphere Application Server Base V9 runtime in a container allows the application to be moved to the cloud without code changes. This type of modernization shouldn't require any code changes and can be driven by the operations team. This path gets the application in to a container with the least amount of effort but doesn't modernize the application or the runtime. In this lab, we'll use Customer Order Services application as an example. In order to modernize, the application will go through analysis , build and deploy phases. Click here and get to know the application, its architecture and components.","title":"Introduction"},{"location":"appmod-labs/OperationalModernization/#login-to-the-vm","text":"If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password. Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window.","title":"Login to the VM"},{"location":"appmod-labs/OperationalModernization/#analysis-hands-on","text":"IBM Cloud Transformation Advisor (TA) can be used to analyze the Customer Order Service Application running in the WebSphere ND environment. The Transformation Advisor helps you to analyze your on-premises workloads for modernization. It determines the complexity of your applications, estimates a development cost to perform the move to the cloud, and recommends the best target environment.","title":"Analysis (Hands-on)"},{"location":"appmod-labs/OperationalModernization/#lets-get-started-using-ta-to-analyze-the-existing-customer-order-services-application","text":"Open a Firefox browser window from within the VM. Click on the openshift console bookmark in the top left and log in with the htpasswd option. Log in to the OpenShift account using the following credentials: Username: ibmadmin Password: engageibm From the Red Hat OpenShift Container Platform console, go to the Networking tab and click on Routes . Note: Ensure that you are in the ta project by using the project drop down and click on the Location URL next to ta-ui-route . This will open the Transformation Advisor user interface. Click Create new under Workspaces to create a new workspace. Name it OperationalModernization and click Next . You'll be asked to create a new collection to store the data collected from the Customer Order Services application. Name the new collection CustomerOrderServices . Click Create . To provide application assessment data and receive recommendations, you would typically download and execute the Data Collector against an existing WebSphere environment. The output from the data collector is a zip file containing the application and configuration metadata gathered from the WebSphere Server, and is the input to the IBM Transformation Advsor tool. However, for this lab, the data collection archive has already been created for you and the resulting data is stored here . Click the Upload button, as we already have the data collectin archive to upload. Upload the results of the data collection (the datacollector.zip file) to IBM Cloud Transformation Advisor. When the upload is complete, you will see a list of applications analyzed from the source environment. At the top of the page, you can see the source environment and the target environment settings. Under the Migration target field, click the down arrow and select Compatible runtimes . This will show you an entry for each application for each compatible destination runtime you can migrate it to. Click the CustomerOrderServicesApp.ear application with the WebSphere traditional migration target to open the Application details page . Look over the migration analysis. From the migratin analysis data, you can: view a summary of the complexity of migrating this application to this target see detailed information about issues view additional reports about the application. In summary, no code changes are required to move this application to the traditional WebSphere Base v9 runtime , so it is a good candidate to proceed with the operational modernization. Click on View migration plan in the top right corner of the page. This page will help you assemble an archive containing: your application's source or binary files (you upload these here or specify Maven coordinates to download them) any required drivers or libraries (you upload these here or specify Maven coordinates to download them) the wsadmin scripts needed to configure your application and its resources (generated by Transformation Advisor and automatically included) the deployment artifacts needed to create the container image and deploy the application to OCP (generated by Transformation Advisor and automatically included) NOTE: These artifacts have already been provided for you as part of the lab files, so you don't need to download the migration plan. However, you can do so if you wish to look around at the files. These files can also be sent to a Git repository by Transformation Advisor. Optional: For a more detailed walkthrough of the Transformation Advisor process, you may Read through this document . Please note that the lab environment DOES NOT have WebSphere installed. Therefore, you cannot actually perform the steps to scan the WebShere server as described in the \"this document\" link shown above. That additonal resource is provided as additional context for how IBM Transformation Advisor tool is used to scan WebSphere servers and collect the migration data.","title":"Let's get started using TA to analyze the existing Customer Order Services application:"},{"location":"appmod-labs/OperationalModernization/#build-hands-on","text":"In this section, you'll learn how to build a Docker image for Customer Order Services application running on traditional WebSphere Base v9. Building this image could take around ~8 minutes. So, let's kick that process off before explaining what you did. The image should be built by the time you complete this section. Open a new terminal window from the VM desktop. Login to OpenShift CLI with the oc login command from the web terminal. When prompted for the username and password, enter the following login credentials: Username: ibmadmin Password: engageibm If you have not yet cloned the GitHub repo with the lab artifacts, run the following command on your terminal: git clone https://github.com/IBM/openshift-workshop-was.git Change directory to where this lab is located: cd openshift-workshop-was/labs/Openshift/OperationalModernization ls Run the following command to create a new project named apps-was in OpenShift. oc new-project apps-was Example output: Now using project \"apps-was\" on server \"https://c115-e.us-south.containers.cloud.ibm.com:32661\". . . . Run the following command to start building the image. Make sure to copy the entire command, including the \".\" at the end (which indicates current directory). docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was .","title":"Build (Hands-on)"},{"location":"appmod-labs/OperationalModernization/#build-image-hands-on","text":"Review the docker build command you ran earlier: docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was . It instructs docker to build the image following the instructions in the Dockerfile in current directory (indicated by the \".\" at the end). A specific name to tag the built image is also specified after --tag The value default-route-openshift-image-registry.apps.demo.ibmdte.net in the tag is the default address of the internal image registry provided by OpenShift. Image registry is a content server that can store and serve container images. The registry is accessible within the cluster via its exposed Service . The format of a Service address: name . namespace . svc . In this case, the image registry is named image-registry and it's in namespace openshift-image-registry . Later when we push the image to OpenShift's internal image registry, we'll refer to the image by the same values. You should see the following message if the image was successfully built. Please wait if it's still building. Successfully built aa6babbb5ce9 Successfully tagged default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was:latest Validate that image is in the repository by running the command: docker images Notice that the base image, websphere-traditional, is also listed. It was pulled as the first step of building application image. Example output: REPOSITORY TAG IMAGE ID CREATED SIZE default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was latest 9394150a5a15 10 minutes ago 2.05GB ibmcom/websphere-traditional latest 898f9fd79b36 12 minutes ago 1.86GB Note that docker images only lists those images that are cached locally. The name of the image also contains the host name where the image is hosted. If there is no host name, the image is hosted on docker hub. For example, the image ibmcom/websphere-traditional has no host name. It is hosted on docker hub. The image we just built, default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was , has host name default-route-openshift-image-registry.apps.demo.ibmdte.net . It is to be hosted in the Openshift image registry for your lab cluster. If you change an image, or build a new image, the changes are only available locally. You must push the image to propagate the changes to the remote registry. Let's push the image you just built to your OpenShift cluster's built-in image registry. a. First, login to the image registry by running the following command in the terminal. Note: A session token is obtained from the value of another command oc whoami -t and used as the password to login. docker login -u $(oc whoami) -p $(oc whoami -t) default-route-openshift-image-registry.apps.demo.ibmdte.net Example output: WARNING! Using --password via the CLI is insecure. Use --password-stdin. WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded b. Now, push the image into OpenShift cluster's internal image registry, which will take 1-2 minutes: docker push default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was Example output: Using default tag: latest The push refers to repository [default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was] 470e7d3b0bec: Pushed c38e61da8211: Pushed 2a72e88fe5eb: Pushed 334c79ff1b2e: Pushed ccf8ea26529f: Pushed af0f17433f77: Pushed 4254aef2aa12: Pushed 855301ffdcce: Pushed ea252e2474a5: Pushed 68a4c9686496: Pushed 87ecb86bc8e5: Pushed 066b59214d49: Pushed 211f972e9c63: Pushed b93eee2b1ddb: Pushed a6ab5ae423d9: Pushed 3f785cf0a0ae: Pushed latest: digest: sha256:4f4e8ae82fa22c83febc4f884b5026d01815fc704df6196431db8ed7a7def6a0 size: 3672 Verify that the image is in the image registry. The following command will get the images in the registry. Filter through the results to get only the image you pushed. Run the following command: oc get images | grep apps-was/cos-was The application image you just pushed should be listed. The hash of the image is stored alongside (indicated by the SHA-256 value). Example output: image-registry.openshift-image-registry.svc:5000/apps-was/cos-was@sha256:bc072d3b78ae6adcd843af75552965e5ed863bcce4fc3f1bc5d194570bc16953 OpenShift uses ImageStream to provide an abstraction for referencing container images from within the cluster. When an image is pushed to registry, an ImageStream is created automatically, if one doesn't already exist. Run the following command to see the ImageStream that's created: oc get imagestreams -n apps-was Example output: NAME IMAGE REPOSITORY TAGS UPDATED cos-was default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was latest 2 minutes ago You can also use the OpenShift console (UI) to see the ImageStream : From the panel on left-side, click on Builds > Image Streams . Then select apps-was from the Project drop-down menu. Click on cos-was from the list. Scroll down to the bottom to see the image that you pushed.","title":"Build image (Hands-on)"},{"location":"appmod-labs/OperationalModernization/#deploy-without-operator","text":"The following steps will deploy the modernized Customer Order Services application in a traditional WebSphere Base container to a RedHat OpenShift cluster. Customer Order Services application uses DB2 as its database. You can connect to an on-prem database that already exists or migrate the database to cloud. Since migrating the database is not the focus of this particular workshop and to save time, the database needed by the application is already configured in the OpenShift cluster you are using.","title":"Deploy without operator"},{"location":"appmod-labs/OperationalModernization/#deploy-application-without-operator-hands-on","text":"Run the following command to deploy the resources (*.yaml files) in the deploy directory: oc apply -f deploy Output: deployment.apps/cos-was created route.route.openshift.io/cos-was created secret/authdata created service/cos-was created Let's review what we just did. The directory deploy contains the following yaml files: Deployment.yaml : the specification for creating a Kubernetes deployment Service.yaml : the specification to expose the deployment as a cluster-wide Kubernetes service. Route.yaml : the specification to expose the service as a route visible outside of the cluster. Secret.yaml : the specification that the properties based configuration properties file used to configure database user/password when the container starts. The Deployment.yaml looks like: apiVersion: apps/v1 kind: Deployment metadata: name: cos-was namespace: apps-was spec: selector: matchLabels: app: cos-was replicas: 1 template: metadata: labels: app: cos-was spec: containers: - name: cos-was image: image-registry.openshift-image-registry.svc:5000/apps-was/cos-was ports: - containerPort: 9080 livenessProbe: httpGet: path: /CustomerOrderServicesWeb/index.html port: 9080 periodSeconds: 30 failureThreshold: 6 initialDelaySeconds: 90 readinessProbe: httpGet: path: /CustomerOrderServicesWeb/index.html port: 9080 periodSeconds: 10 failureThreshold: 3 volumeMounts: - mountPath: /etc/websphere name: authdata readOnly: true volumes: - name: authdata secret: secretName: authdata Note the following about the deployment yaml The liveness probe is used to tell Kubernetes when the application is live. Due to the size of the traditional WAS image, the initialDelaySeconds attribute has been set to 90 seconds to give the container time to start. The readiness probe is used to tell Kubernetes whether the application is ready to serve requests. You may store property file based configuration files such as configmaps and secrets , and bind their contents into the /etc/websphere directory. When the container starts, the server startup script will apply all the property files found in the /etc/websphere directory to reconfigure the server. For our example, the volumeMounts and volumes are used to bind the contents of the secret authdata into the directory /etc/websphere during container startup. After it is bound, it will appear as the file /etc/websphere/authdata.properties . For volumeMounts: The mountPath, /etc/websphere , specifies the directory where the files are bound. the name, authdata , specifies the name of the volume For volumes: the secretName specifies the name of the secret whose contents are to be bound. The Secret.yaml looks like: apiVersion: v1 kind: Secret metadata: name: authdata namespace: apps-was type: Opaque stringData: authdata.props: |- # # Configuration properties file for cells/DefaultCell01|security.xml#JAASAuthData_1597094577206# # Extracted on Tue Aug 11 15:30:36 UTC 2020 # # # Section 1.0 ## Cell=!{cellName}:Security=:JAASAuthData=alias#DBUser # # # SubSection 1.0.0 # JAASAuthData Section # ResourceType=JAASAuthData ImplementingResourceType=GenericType ResourceId=Cell=!{cellName}:Security=:JAASAuthData=alias#DBUser AttributeInfo=authDataEntries # # #Properties # password=\"{xor}Oz1tNjEsK24=\" #required alias=DBUser #required userId=db2inst1 #required description= # # End of Section 1.0# Cell=!{cellName}:Security=:JAASAuthData=alias#DBUser # # # EnvironmentVariablesSection # # #Environment Variables cellName=DefaultCell01 The attribute authdata.properties contains the properties file based configure used to update the database userId and password for the JAASAuthData whose alias is DBUser. The configuration in Deployment.yaml maps it as the file /etc/websphere/authdata.properties during container startup so that the application server startup script can automatically configure the server with these entries.","title":"Deploy application without operator (Hands-on)"},{"location":"appmod-labs/OperationalModernization/#access-the-application-without-operator-hands-on","text":"Confirm you're at the current project apps-was : oc project Example output: Using project \"apps-was\" on server \"https://c114-e.us-south.containers.cloud.ibm.com:30016\". If it's not at the project apps-was , then switch: oc project apps-was Run the following command to verify the pod is running: oc get pod If the status does not show 1/1 READY, wait a while, checking status periodically: NAME READY STATUS RESTARTS AGE cos-was-6bd4767bf6-xhr92 1/1 Running 0 120m Run the following command to get the URL of your application (the route URL plus the application contextroot): echo http://$(oc get route cos-was --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-was-apps-was.apps.demo.ibmdte.net/CustomerOrderServicesWeb Return to your Firefox browser window and go to the URL outputted by the command run in the previous step. You will be prompted to login in order to access the application. Enter the following credentials: Username: skywalker Password: force After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total . Close the browser.","title":"Access the application without operator (Hands-on)"},{"location":"appmod-labs/OperationalModernization/#review-the-application-workload-flow-without-operator-hands-on","text":"Below is an overview diagram on the deployment you've completed from the above steps: Note: DB2 in the middle of the diagram is pre-installed through a different project db and has been up and running before your hands-on. Also it will not be impacted when you're removing the deployment in next step. Return to the Firefox browser and open the OpenShift Console to view the resources on the deployment. View the resources in the project apps-was : Select the apps-was project from the project drop down menu. View deployment details: Click on the Deployments tab under Workloads from the left menu and select cos-was Navigate to the YAML tab to view the content of yaml View pod details: Click on the Pods tab under Workloads from the left menu and select the pod with name starting with cos-was Navigate to the Logs tab to view the WebSphere Application Server log Navigate to the Terminal tab to view the files inside the container View secret details: Click on the Secrets tab under Workloads from the left menu and select the authdata secret. Scroll down to the Data section and click on the copy icon to view the content. View service details: Click on the Services tab under Networking from the left menu and select the cos-was service. Review service information including address and port mapping details. View route details: Click on the Routes tab under Networking from the left menu and select the cos-was route.","title":"Review the application workload flow without operator (Hands-on)"},{"location":"appmod-labs/OperationalModernization/#view-the-resources-in-the-project-that-contains-the-db2-databse-used-by-the-application","text":"View the resources in the project db : Select the db project from the project drop down menu. View deployment details: Click on the Deployments tab under Workloads from the left menu and select cos-db-was Navigate to the YAML tab to view the content of yaml View pod details: Click on the Pods tab under Workloads from the left menu and select the pod with name starting with cos-db-was Navigate to the Logs tab to view the database logs Navigate to the Terminal tab to view the files in the database container View service details: Click on the Services tab under Networking from the left menu and select the cos-db-was service.","title":"View the resources in the project that contains the DB2 databse used by the application`"},{"location":"appmod-labs/OperationalModernization/#remove-your-deployment-standard-deployment-without-operator-hands-on","text":"To remove the deploment from the above scenario without the operator, run the command: Note: The pre-installed resources such as DB2, are not removed. oc delete -f deploy Output: deployment.apps \"cos-was\" deleted route.route.openshift.io \"cos-was\" deleted secret \"authdata\" deleted service \"cos-was\" deleted","title":"Remove your deployment (standard deployment without operator) (Hands-on)"},{"location":"appmod-labs/OperationalModernization/#alternate-deployment-via-runtime-component-operator","text":"Another way to deploy the application is via the Runtime Component Operator . It is a generic operator used to deploy different types of application images. The operator has already been installed into your environment. For more information, see: https://github.com/application-stacks/runtime-component-operator","title":"Alternate Deployment Via Runtime Component Operator"},{"location":"appmod-labs/OperationalModernization/#deploy-application-via-runtime-component-operator-hands-on","text":"Run the following command which uses the Runtime Component Operator to deploy the same Customer Order Service application image: oc apply -f deploy-rco Output: runtimecomponent.app.stacks/cos-was-rco created secret/authdata-rco created Let's review what we just did. First, list the contents of the deploy-rco directory: ls deploy-rco The output shows there are only two yaml files: RuntimeComponent.yaml Secret.yaml Review Secret.yaml : cat deploy-rco/Secret.yaml Note that it is the same as the Secret.yaml in the deploy directory, except the name has been changed to authdata-rco . It serves the same purpose for this new deployment - to override the database user/password. Review RuntimeComponent.yaml : cat deploy-rco/RuntimeComponent.yaml And the output: apiVersion: app.stacks/v1beta1 kind: RuntimeComponent metadata: name: cos-was-rco namespace: apps-was spec: applicationImage: image-registry.openshift-image-registry.svc:5000/apps-was/cos-was service: port: 9080 readinessProbe: httpGet: path: /CustomerOrderServicesWeb/index.html port: 9080 periodSeconds: 10 failureThreshold: 3 livenessProbe: httpGet: path: /CustomerOrderServicesWeb/index.html port: 9080 periodSeconds: 30 failureThreshold: 6 initialDelaySeconds: 90 expose: true route: termination: edge insecureEdgeTerminationPolicy: Redirect volumeMounts: - mountPath: /etc/websphere name: authdata-rco readOnly: true volumes: - name: authdata-rco secret: secretName: authdata-rco Note that: The kind is RuntimeComponent The expose attribute is set to true to expose a route The attributes within the yaml file are essentially the same information that you provided for the Service , Route , and Deployment resources in the deploy directory. The controller for the RuntimeComponent custom resource reacts to changes in the above specification, and creates the corresponding Service , Route , and Deployment objects. Issue the following commands to view what the controller has created: oc get Deployment cos-was-rco -o yaml oc get Service cos-was-rco -o yaml oc get Route cos-was-rco -o yaml","title":"Deploy application (via Runtime Component Operator) (Hands-on)"},{"location":"appmod-labs/OperationalModernization/#access-the-application-via-runtime-component-operator-hands-on","text":"Ensure you are at the project apps-was : oc project apps-was Example output: Using project \"apps-was\" on server \"https://c114-e.us-south.containers.cloud.ibm.com:30016\". Run the following command to verify the pod is running: oc get pod If the status does not show 1/1 READY, wait a while, checking status periodically. Note the prefix name for the pod is cos-was-rco . NAME READY STATUS RESTARTS AGE cos-was-rco-6779784fc8-pz92m 1/1 Running 0 2m59s Run the following command to get the URL of your application (the route URL plus the application contextroot): echo http://$(oc get route cos-was-rco --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-was-rco-apps-was.apps.demo.ibmdte.net/CustomerOrderServicesWeb Return to your Firefox browser window and go to the URL outputted by the command run in the previous step. Note: The steps to access the application are the same as those used when deploying without the operator. You will be prompted to login in order to access the application. Enter the following credentials: Username: skywalker Password: force After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total . Close the browser.","title":"Access the application (via Runtime Component Operator) (Hands-on)"},{"location":"appmod-labs/OperationalModernization/#review-the-application-workload-flow-with-runtime-component-operator-hands-on","text":"Below is an overview diagram on the deployment you've completed from the above steps using Runtime Component Operator: Note: DB2 in the middle of the diagram is pre-installed through a different project db and has been up and running before your hands-on. Return to the Firefox browser and open the OpenShift Console to view the resources on the deployment.","title":"Review the application workload flow with Runtime Component Operator (Hands-on)"},{"location":"appmod-labs/OperationalModernization/#review-the-resources-in-the-project-openshift-operators","text":"View the resources in the project openshift-operators : Select the openshift-operators project from the project drop down menu. View operator deployment details: Click on the Deployments tab under Workloads from the left menu and select runtime-component-operator Navigate to the YAML tab to view the content of yaml View operator pod details: Click on the Pods tab under Workloads from the left menu and select the pod with name starting with runtime-component-operator Navigate to Logs to view the runtime-component-operator container log Navigate to Terminal to view the files in the container","title":"Review the resources in the project openshift-operators"},{"location":"appmod-labs/OperationalModernization/#view-the-resources-in-the-project-apps-was","text":"View the resources in the project apps-was : Select the apps-was project from the project drop down menu. View Runtime Component instance details: Click on the Installed Operators tab under Operators from the left menu and select Runtime Component Operator . Note: The operator is installed at cluster level and is visible to all existing projects, but Runtime Component instance is created under the project apps-was . Navigate to the YAML tab to view the content of yaml Navigate to the Runtime Component tab and select cos-was-rco to view the deails of Runtime Component instance View deployment details: Click on the Deployments tab under Workloads from the left menu and select cos-was-rco . Navigate to the YAML tab to view the content of yaml. Note the deployment is created through the controller of RuntimeComponent custom resource. View pod details: Click on the Pods tab under Workloads from the left menu and select the pod starting with cos-was-rco Navigate to the Logs tab to view the WebSphere Application Server log View service details: Click on the Services tab under Networking from the left menu and select cos-was-rco Navigate to the YAML tab to view the content of yaml. Note the service is created through the controller of RuntimeComponent custom resource. View route details: Click on the Routes tab under Networking from the left menu and select cos-was-rco Navigate to the YAML tab to view the content of yaml. Note the route is created through the controller of RuntimeComponent custom resource. View secret details: Click on the Secrets tab under Workloads from the left menu and select authdata-rco Resources in the project db : The Database resources are the Same as listed in the section above in Review the application workload flow without operator .","title":"View the resources in the project apps-was"},{"location":"appmod-labs/OperationalModernization/#cleanup-the-deployment-with-runtime-component-operator-hands-on","text":"Run the following command in a terminal window to remove the deployment from the above secenario with Runtime Component instance: Note: The pre-installed resources such as Runtime Component Operator, DB2, are not removed. oc delete -f deploy-rco Output: runtimecomponent.app.stacks \"cos-was-rco\" deleted secret \"authdata-rco\" deleted Verify that the corresponding application Service , Route , and Deployment have also been deleted: oc get Deployment oc get Service oc get Route Output from each get command above should be: No resources found in apps-was namespace.","title":"Cleanup (the deployment with Runtime Component Operator) (Hands-on)"},{"location":"appmod-labs/OperationalModernization/#summary","text":"Congratulations! You've completed the Operational Modernization lab. You containerized and deployed a monolith application to cloud!","title":"Summary"},{"location":"appmod-labs/OperationalModernization/#next","text":"Please follow the link to the next lab Runtime Modernization : - Runtime Modernization","title":"Next"},{"location":"appmod-labs/OperationalModernization/README - Copy/","text":"Operational Modernization Table of Contents Introduction Analysis (Hands-on) Build (Hands-on) Deploy without operator (Hands-on) Access the Application without operator (Hands-on) Alternate Deployment via Runtime Component Operator (Hands-on) Summary Next Introduction Operational modernization gives an operations team the opportunity to embrace modern operations best practices without putting change requirements on the development team. Modernizing from WebSphere Network Deployment (ND) to the traditional WebSphere Application Server Base V9 runtime in a container allows the application to be moved to the cloud without code changes. This type of modernization shouldn't require any code changes and can be driven by the operations team. This path gets the application in to a container with the least amount of effort but doesn't modernize the application or the runtime. In this lab, we'll use Customer Order Services application as an example. In order to modernize, the application will go through analysis , build and deploy phases. Click here and get to know the application, its architecture and components. Login to the VM If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password: \\ Password: engageibm Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window. Analysis (Hands-on) IBM Cloud Transformation Advisor can be used to analyze the Customer Order Service Application running in the WebSphere ND environment. The Transformation Advisor helps you to analyze your on-premises workloads for modernization. It determines the complexity of your applications, estimates a development cost to perform the move to the cloud, and recommends the best target environment. The steps needed to analyze the existing Customer Order Services application are: 1. Open a Firefox browser window from within the VM. Click on the openshift console bookmark in the top left and log in with the htpasswd option. Log in to the OpenShift account using the following credentials: Username: ibmadmin Password: engageibm From the Red Hat OpenShift Container Platform console, go to the Networking tab and click on Routes . Ensure that you are in the ta project by using the project drop down and click on the Location URL next to ta-ui-route . This will open the Transformation Advisor user interface. Click Create new under Workspaces to create a new workspace. Name it OperationalModernization and click Next . You'll be asked to create a new collection to store the data collected from the Customer Order Services application. Name it CustomerOrderServices . Click Create . To provide data and receive recommendations, you can either download and execute the Data Collector against an existing WebSphere environment or upload an existing data collection archive. The archive has already been created for you and the resulting data is stored here . Upload the results of the data collection (the datacollector.zip file) to IBM Cloud Transformation Advisor. When the upload is complete, you will see a list of applications analyzed from the source environment. At the top of the page, you can see the source environment and the target environment settings. Under the Migration target field, click the down arrow and select Compatible runtimes . This will show you an entry for each application for each compatible destination runtime you can migrate it to. Click the CustomerOrderServicesApp.ear application with the WebSphere traditional migration target to open the Application details page . Look over the migration analysis. You can view a summary of the complexity of migrating this application to this target, see detailed information about issues, and view additional reports about the application. In summary, no code changes are required to move this application to the traditional WebSphere Base v9 runtime, so it is a good candidate to proceed with the operational modernization. Click on View migration plan in the top right corner of the page. This page will help you assemble an archive containing: - your application's source or binary files (you upload these here or specify Maven coordinates to download them) - any required drivers or libraries (you upload these here or specify Maven coordinates to download them) - the wsadmin scripts needed to configure your application and its resources (generated by Transformation Advisor and automatically included) - the deployment artifacts needed to create the container image and deploy the application to OCP (generated by Transformation Advisor and automatically included) NOTE: These artifacts have already been provided for you as part of the lab files, so you don't need to download the migration plan. However, you can do so if you wish to look around at the files. These files can also be sent to a Git repository by Transformation Advisor. For a more detailed walkthrough of the Transformation Advisor process, see this document . Build (Hands-on) In this section, you'll learn how to build a Docker image for Customer Order Services application running on traditional WebSphere Base v9. Building this image could take around ~8 minutes. So, let's kick that process off before explaining what you did. The image should be built by the time you complete this section. Open a new terminal window from the VM desktop. Login to OpenShift CLI with the oc login command from the web terminal. When prompted for the username and password, enter the following login credentials: Username: ibmadmin Password: engageibm If you have not yet cloned the GitHub repo with the lab artifacts, run the following command on your terminal: git clone https://github.com/IBM/openshift-workshop-was.git Change directory to where this lab is located: cd openshift-workshop-was/labs/Openshift/OperationalModernization ls Run the following command to create a new project named apps-was in OpenShift. oc new-project apps-was Example output: Now using project \"apps-was\" on server \"https://c115-e.us-south.containers.cloud.ibm.com:32661\". . . . Run the following command to start building the image. Make sure to copy the entire command, including the \".\" at the end (which indicates current directory). docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was . Build image (Hands-on) Review the command you ran earlier: docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was . It instructs docker to build the image following the instructions in the Dockerfile in current directory (indicated by the \".\" at the end). A specific name to tag the built image is also specified after --tag . The value default-route-openshift-image-registry.apps.demo.ibmdte.net in the tag is the default address of the internal image registry provided by OpenShift. Image registry is a content server that can store and serve container images. The registry is accessible within the cluster via its exposed Service . The format of a Service address: name . namespace . svc . In this case, the image registry is named image-registry and it's in namespace openshift-image-registry . Later when we push the image to OpenShift's internal image registry, we'll refer to the image by the same values. You should see the following message if the image was successfully built. Please wait if it's still building. Successfully built aa6babbb5ce9 Successfully tagged default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was:latest Validate that image is in the repository by running the command: docker images Notice that the base image, websphere-traditional, is also listed. It was pulled as the first step of building application image. Example output: REPOSITORY TAG IMAGE ID CREATED SIZE default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was latest 9394150a5a15 10 minutes ago 2.05GB ibmcom/websphere-traditional latest 898f9fd79b36 12 minutes ago 1.86GB Note that docker images only lists those images that are cached locally. The name of the image also contains the host name where the image is hosted. If there is no host name, the image is hosted on docker hub. For example, the image ibmcom/websphere-traditional has no host name. It is hosted on docker hub. The image we just built, default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was , has host name default-route-openshift-image-registry.apps.demo.ibmdte.net . It is to be hosted in the Openshift image registry for your lab cluster. If you change an image, or build a new image, the changes are only available locally. You must push the image to propagate the changes to the remote registry. Let's push the image you just built to your OpenShift cluster's built-in image registry. First, login to the image registry by running the following command in the terminal. Note: A session token is obtained from the value of another command oc whoami -t and used as the password to login. docker login -u $(oc whoami) -p $(oc whoami -t) default-route-openshift-image-registry.apps.demo.ibmdte.net Example output: WARNING! Using --password via the CLI is insecure. Use --password-stdin. WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded Now, push the image into OpenShift cluster's internal image registry, which will take 1-2 minutes: docker push default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was Example output: Using default tag: latest The push refers to repository [default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was] 470e7d3b0bec: Pushed c38e61da8211: Pushed 2a72e88fe5eb: Pushed 334c79ff1b2e: Pushed ccf8ea26529f: Pushed af0f17433f77: Pushed 4254aef2aa12: Pushed 855301ffdcce: Pushed ea252e2474a5: Pushed 68a4c9686496: Pushed 87ecb86bc8e5: Pushed 066b59214d49: Pushed 211f972e9c63: Pushed b93eee2b1ddb: Pushed a6ab5ae423d9: Pushed 3f785cf0a0ae: Pushed latest: digest: sha256:4f4e8ae82fa22c83febc4f884b5026d01815fc704df6196431db8ed7a7def6a0 size: 3672 Verify that the image is in the image registry. The following command will get the images in the registry. Filter through the results to get only the image you pushed. Run the following command: oc get images | grep apps-was/cos-was The application image you just pushed should be listed. The hash of the image is stored alongside (indicated by the SHA-256 value). Example output: image-registry.openshift-image-registry.svc:5000/apps-was/cos-was@sha256:bc072d3b78ae6adcd843af75552965e5ed863bcce4fc3f1bc5d194570bc16953 OpenShift uses ImageStream to provide an abstraction for referencing container images from within the cluster. When an image is pushed to registry, an ImageStream is created automatically, if one doesn't already exist. Run the following command to see the ImageStream that's created: oc get imagestreams -n apps-was Example output: ``` NAME IMAGE REPOSITORY TAGS UPDATED cos-was default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was latest 2 minutes ago ``` You can also use the OpenShift console (UI) to see the ImageStream : From the panel on left-side, click on Builds > Image Streams . Then select apps-was from the Project drop-down menu. Click on cos-was from the list. Scroll down to the bottom to see the image that you pushed. Deploy without operator The following steps will deploy the modernized Customer Order Services application in a traditional WebSphere Base container to a RedHat OpenShift cluster. Customer Order Services application uses DB2 as its database. You can connect to an on-prem database that already exists or migrate the database to cloud. Since migrating the database is not the focus of this particular workshop and to save time, the database needed by the application is already configured in the OpenShift cluster you are using. Deploy application without operator (Hands-on) Run the following command to deploy the resources (*.yaml files) in the deploy directory: oc apply -f deploy Output: deployment.apps/cos-was created route.route.openshift.io/cos-was created secret/authdata created service/cos-was created Let's review what we just did. The directory deploy contains the following yaml files: Deployment.yaml : the specification for creating a Kubernetes deployment Service.yaml : the specification to expose the deployment as a cluster-wide Kubernetes service. Route.yaml : the specification to expose the service as a route visible outside of the cluster. Secret.yaml : the specification that the properties based configuration properties file used to configure database user/password when the container starts. The file Deployment.yaml looks like: apiVersion: apps/v1 kind: Deployment metadata: name: cos-was namespace: apps-was spec: selector: matchLabels: app: cos-was replicas: 1 template: metadata: labels: app: cos-was spec: containers: - name: cos-was image: image-registry.openshift-image-registry.svc:5000/apps-was/cos-was ports: - containerPort: 9080 livenessProbe: httpGet: path: /CustomerOrderServicesWeb/index.html port: 9080 periodSeconds: 30 failureThreshold: 6 initialDelaySeconds: 90 readinessProbe: httpGet: path: /CustomerOrderServicesWeb/index.html port: 9080 periodSeconds: 10 failureThreshold: 3 volumeMounts: - mountPath: /etc/websphere name: authdata readOnly: true volumes: - name: authdata secret: secretName: authdata Note: The liveness probe is used to tell Kubernetes when the application is live. Due to the size of the traditional WAS image, the initialDelaySeconds attribute has been set to 90 seconds to give the container time to start. The readiness probe is used to tell Kubernetes whether the application is ready to serve requests. You may store property file based configuration files such as configmaps and secrets, and bind their contents into the /etc/websphere directory. When the container starts, the server startup script will apply all the property files found in the /etc/websphere directory to reconfigure the server. For our example, the volumeMounts and volumes are used to bind the contents of the secret authdata into the directory /etc/websphere during container startup. After it is bound, it will appear as the file /etc/websphere/authdata.properties . For volumeMounts: The mountPath, /etc/websphere , specifies the directory where the files are bound. the name, authdata , specifies the name of the volume For volumes: the secretName specifies the name of the secret whose contents are to be bound. The file Secret.yaml looks like: apiVersion: v1 kind: Secret metadata: name: authdata namespace: apps-was type: Opaque stringData: authdata.props: |- # # Configuration properties file for cells/DefaultCell01|security.xml#JAASAuthData_1597094577206# # Extracted on Tue Aug 11 15:30:36 UTC 2020 # # # Section 1.0 ## Cell=!{cellName}:Security=:JAASAuthData=alias#DBUser # # # SubSection 1.0.0 # JAASAuthData Section # ResourceType=JAASAuthData ImplementingResourceType=GenericType ResourceId=Cell=!{cellName}:Security=:JAASAuthData=alias#DBUser AttributeInfo=authDataEntries # # #Properties # password=\"{xor}Oz1tNjEsK24=\" #required alias=DBUser #required userId=db2inst1 #required description= # # End of Section 1.0# Cell=!{cellName}:Security=:JAASAuthData=alias#DBUser # # # EnvironmentVariablesSection # # #Environment Variables cellName=DefaultCell01 The attribute authdata.properties contains the properties file based configure used to update the database userId and password for the JAASAuthData whose alias is DBUser. The configuration in Deployment.yaml maps it as the file /etc/websphere/authdata.properties during container startup so that the application server startup script can automatically configure the server with these entries. Access the application without operator (Hands-on) Confirm you're at the current project apps-was : oc project Example output: Using project \"apps-was\" on server \"https://c114-e.us-south.containers.cloud.ibm.com:30016\". - If it's not at the project apps-was , then switch: oc project apps-was 1. Run the following command to verify the pod is running: oc get pod If the status does not show 1/1 READY, wait a while, checking status periodically: NAME READY STATUS RESTARTS AGE cos-was-6bd4767bf6-xhr92 1/1 Running 0 120m Run the following command to get the URL of your application (the route URL plus the application contextroot): echo http://$(oc get route cos-was --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-was-apps-was.apps.demo.ibmdte.net/CustomerOrderServicesWeb Return to your Firefox browser window and go to the URL outputted by the command run in the previous step. You will be prompted to login in order to access the application. Enter the following credentials: Username: skywalker Password: force After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total . Close the browser. Review the application workload flow without operator (Hands-on) Below is an overview diagram on the deployment you've completed from the above steps: Note: DB2 in the middle of the diagram is pre-installed through a different project db and has been up and running before your hands-on. Also it will not be impacted when you're removing the deployment in next step. Return to the Firefox browser and open the OpenShift Console to view the resources on the deployment. View the resources in the project apps-was : Select the apps-was project from the project drop down menu. View deployment details: Click on the Deployments tab under Workloads from the left menu and select cos-was Navigate to the YAML tab to view the content of yaml View pod details: Click on the Pods tab under Workloads from the left menu and select the pod with name starting with cos-was Navigate to the Logs tab to view the WebSphere Application Server log Navigate to the Terminal tab to view the files inside the container View secret details: Click on the Secrets tab under Workloads from the left menu and select the authdata secret. Scroll down to the Data section and click on the copy icon to view the content. View service details: Click on the Services tab under Networking from the left menu and select the cos-was service. Review service information including address and port mapping details. View route details: Click on the Routes tab under Networking from the left menu and select the cos-was route. 1. View the resources in the project db : - Select the db project from the project drop down menu. View deployment details: Click on the Deployments tab under Workloads from the left menu and select cos-db-was Navigate to the YAML tab to view the content of yaml View pod details: Click on the Pods tab under Workloads from the left menu and select the pod with name starting with cos-db-was Navigate to the Logs tab to view the database logs Navigate to the Terminal tab to view the files in the database container View service details: Click on the Services tab under Networking from the left menu and select the cos-db-was service. Remove your deployment (standard deployment without operator) (Hands-on) To remove the deploment from the above scenario without the operator, run the command: Note: The pre-installed resources such as DB2, are not removed. oc delete -f deploy Output: deployment.apps \"cos-was\" deleted route.route.openshift.io \"cos-was\" deleted secret \"authdata\" deleted service \"cos-was\" deleted Alternate Deployment Via Runtime Component Operator Another way to deploy the application is via the Runtime Component Operator. It is a generic operator used to deploy different types of application images. The operator has already been installed into your environment. For more information, see: https://github.com/application-stacks/runtime-component-operator Deploy application (via Runtime Component Operator) (Hands-on) Run the following command which uses the Runtime Component Operator to deploy the same Customer Order Service application image: oc apply -f deploy-rco Output: runtimecomponent.app.stacks/cos-was-rco created secret/authdata-rco created Let's review what we just did. First, list the contents of the deploy-rco directory: ls deploy-rco The output shows there are only two yaml files: RuntimeComponent.yaml Secret.yaml Review Secret.yaml: cat deploy-rco/Secret.yaml Note that it is the same as the Secret.yaml in the deploy directory, except the name has been changed to authdata-rco . It serves the same purpose for this new deployment - to override the database user/password. Review RuntimeComponent.yaml: cat deploy-rco/RuntimeComponent.yaml And the output: apiVersion: app.stacks/v1beta1 kind: RuntimeComponent metadata: name: cos-was-rco namespace: apps-was spec: applicationImage: image-registry.openshift-image-registry.svc:5000/apps-was/cos-was service: port: 9080 readinessProbe: httpGet: path: /CustomerOrderServicesWeb/index.html port: 9080 periodSeconds: 10 failureThreshold: 3 livenessProbe: httpGet: path: /CustomerOrderServicesWeb/index.html port: 9080 periodSeconds: 30 failureThreshold: 6 initialDelaySeconds: 90 expose: true route: termination: edge insecureEdgeTerminationPolicy: Redirect volumeMounts: - mountPath: /etc/websphere name: authdata-rco readOnly: true volumes: - name: authdata-rco secret: secretName: authdata-rco Note that: - The kind is RuntimeComponent - The expose attribute is set to true to expose a route - The attributes within the yaml file are essentially the same information that you provided for the Service , Route , and Deployment resources in the deploy directory. - The controller for the RuntimeComponent custom resource reacts to changes in the above specification, and creates the corresponding Service , Route , and Deployment objects. Issue the following commands to view what the controller has created: ``` oc get Deployment cos-was-rco -o yaml oc get Service cos-was-rco -o yaml oc get Route cos-was-rco -o yaml ``` Access the application (via Runtime Component Operator) (Hands-on) Confirm you're at the current project apps-was : oc project Example output: Using project \"apps-was\" on server \"https://c114-e.us-south.containers.cloud.ibm.com:30016\". - If it's not at the project apps-was , then switch: oc project apps-was 1. Run the following command to verify the pod is running: oc get pod If the status does not show 1/1 READY, wait a while, checking status periodically. Note the prefix name for the pod is cos-was-rco . NAME READY STATUS RESTARTS AGE cos-was-rco-6779784fc8-pz92m 1/1 Running 0 2m59s Run the following command to get the URL of your application (the route URL plus the application contextroot): echo http://$(oc get route cos-was-rco --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-was-rco-apps-was.apps.demo.ibmdte.net/CustomerOrderServicesWeb Return to your Firefox browser window and go to the URL outputted by the command run in the previous step. The steps to access the application are the same as those used when deploying without the operator. You will be prompted to login in order to access the application. Enter the following credentials: Username: skywalker Password: force After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total . Close the browser. Review the application workload flow with Runtime Component Operator (Hands-on) Below is an overview diagram on the deployment you've completed from the above steps using Runtime Component Operator: Note: DB2 in the middle of the diagram is pre-installed through a different project db and has been up and running before your hands-on. Return to the Firefox browser and open the OpenShift Console to view the resources on the deployment. View the resources in the project openshift-operators : Select the openshift-operators project from the project drop down menu. View operator deployment details: Click on the Deployments tab under Workloads from the left menu and select runtime-component-operator Navigate to the YAML tab to view the content of yaml View operator pod details: Click on the Pods tab under Workloads from the left menu and select the pod with name starting with runtime-component-operator Navigate to Logs to view the runtime-component-operator container log Navigate to Terminal to view the files in the container View the resources in the project apps-was : Select the apps-was project from the project drop down menu. View Runtime Component instance details: Click on the Installed Operators tab under Operators from the left menu and select Runtime Component Operator . > Note: The operator is installed at cluster level and is visible to all existing projects, but Runtime Component instance is created under the project apps-was . Navigate to the YAML tab to view the content of yaml Navigate to the Runtime Component tab and select cos-was-rco to view the deails of Runtime Component instance View deployment details: Click on the Deployments tab under Workloads from the left menu and select cos-was-rco . Navigate to the YAML tab to view the content of yaml. > Note the deployment is created through the controller of RuntimeComponent custom resource. View pod details: Click on the Pods tab under Workloads from the left menu and select the pod starting with cos-was-rco Navigate to the Logs tab to view the WebSphere Application Server log View service details: Click on the Services tab under Networking from the left menu and select cos-was-rco Navigate to the YAML tab to view the content of yaml. > Note the service is created through the controller of RuntimeComponent custom resource. View route details: Click on the Routes tab under Networking from the left menu and select cos-was-rco Navigate to the YAML tab to view the content of yaml. > Note the route is created through the controller of RuntimeComponent custom resource. View secret details: Click on the Secrets tab under Workloads from the left menu and select authdata-rco Resources in the project db : Same information as listed in the section above in Review the application workload flow without operator . Cleanup (the deployment with Runtime Component Operator) (Hands-on) Run the following command in a terminal window to remove the deployment from the above secenario with Runtime Component instance: Note: The pre-installed resources such as Runtime Component Operator, DB2, are not removed. oc delete -f deploy-rco Output: runtimecomponent.app.stacks \"cos-was-rco\" deleted secret \"authdata-rco\" deleted Verify that the corresponding application Service , Route , and Deployment have also been deleted: oc get Deployment oc get Service oc get Route Output from each get command above should be: No resources found in apps-was namespace. Summary Congratulations! You've completed the Operational Modernization lab. You containerized and deployed a monolith application to cloud! Next Please follow the link to the next lab Runtime Modernization : - Runtime Modernization","title":"Operational Modernization"},{"location":"appmod-labs/OperationalModernization/README - Copy/#operational-modernization","text":"","title":"Operational Modernization"},{"location":"appmod-labs/OperationalModernization/README - Copy/#table-of-contents","text":"Introduction Analysis (Hands-on) Build (Hands-on) Deploy without operator (Hands-on) Access the Application without operator (Hands-on) Alternate Deployment via Runtime Component Operator (Hands-on) Summary Next","title":"Table of Contents"},{"location":"appmod-labs/OperationalModernization/README - Copy/#introduction","text":"Operational modernization gives an operations team the opportunity to embrace modern operations best practices without putting change requirements on the development team. Modernizing from WebSphere Network Deployment (ND) to the traditional WebSphere Application Server Base V9 runtime in a container allows the application to be moved to the cloud without code changes. This type of modernization shouldn't require any code changes and can be driven by the operations team. This path gets the application in to a container with the least amount of effort but doesn't modernize the application or the runtime. In this lab, we'll use Customer Order Services application as an example. In order to modernize, the application will go through analysis , build and deploy phases. Click here and get to know the application, its architecture and components.","title":"Introduction"},{"location":"appmod-labs/OperationalModernization/README - Copy/#login-to-the-vm","text":"If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password: \\ Password: engageibm Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window.","title":"Login to the VM"},{"location":"appmod-labs/OperationalModernization/README - Copy/#analysis-hands-on","text":"IBM Cloud Transformation Advisor can be used to analyze the Customer Order Service Application running in the WebSphere ND environment. The Transformation Advisor helps you to analyze your on-premises workloads for modernization. It determines the complexity of your applications, estimates a development cost to perform the move to the cloud, and recommends the best target environment. The steps needed to analyze the existing Customer Order Services application are: 1. Open a Firefox browser window from within the VM. Click on the openshift console bookmark in the top left and log in with the htpasswd option. Log in to the OpenShift account using the following credentials: Username: ibmadmin Password: engageibm From the Red Hat OpenShift Container Platform console, go to the Networking tab and click on Routes . Ensure that you are in the ta project by using the project drop down and click on the Location URL next to ta-ui-route . This will open the Transformation Advisor user interface. Click Create new under Workspaces to create a new workspace. Name it OperationalModernization and click Next . You'll be asked to create a new collection to store the data collected from the Customer Order Services application. Name it CustomerOrderServices . Click Create . To provide data and receive recommendations, you can either download and execute the Data Collector against an existing WebSphere environment or upload an existing data collection archive. The archive has already been created for you and the resulting data is stored here . Upload the results of the data collection (the datacollector.zip file) to IBM Cloud Transformation Advisor. When the upload is complete, you will see a list of applications analyzed from the source environment. At the top of the page, you can see the source environment and the target environment settings. Under the Migration target field, click the down arrow and select Compatible runtimes . This will show you an entry for each application for each compatible destination runtime you can migrate it to. Click the CustomerOrderServicesApp.ear application with the WebSphere traditional migration target to open the Application details page . Look over the migration analysis. You can view a summary of the complexity of migrating this application to this target, see detailed information about issues, and view additional reports about the application. In summary, no code changes are required to move this application to the traditional WebSphere Base v9 runtime, so it is a good candidate to proceed with the operational modernization. Click on View migration plan in the top right corner of the page. This page will help you assemble an archive containing: - your application's source or binary files (you upload these here or specify Maven coordinates to download them) - any required drivers or libraries (you upload these here or specify Maven coordinates to download them) - the wsadmin scripts needed to configure your application and its resources (generated by Transformation Advisor and automatically included) - the deployment artifacts needed to create the container image and deploy the application to OCP (generated by Transformation Advisor and automatically included) NOTE: These artifacts have already been provided for you as part of the lab files, so you don't need to download the migration plan. However, you can do so if you wish to look around at the files. These files can also be sent to a Git repository by Transformation Advisor. For a more detailed walkthrough of the Transformation Advisor process, see this document .","title":"Analysis (Hands-on)"},{"location":"appmod-labs/OperationalModernization/README - Copy/#build-hands-on","text":"In this section, you'll learn how to build a Docker image for Customer Order Services application running on traditional WebSphere Base v9. Building this image could take around ~8 minutes. So, let's kick that process off before explaining what you did. The image should be built by the time you complete this section. Open a new terminal window from the VM desktop. Login to OpenShift CLI with the oc login command from the web terminal. When prompted for the username and password, enter the following login credentials: Username: ibmadmin Password: engageibm If you have not yet cloned the GitHub repo with the lab artifacts, run the following command on your terminal: git clone https://github.com/IBM/openshift-workshop-was.git Change directory to where this lab is located: cd openshift-workshop-was/labs/Openshift/OperationalModernization ls Run the following command to create a new project named apps-was in OpenShift. oc new-project apps-was Example output: Now using project \"apps-was\" on server \"https://c115-e.us-south.containers.cloud.ibm.com:32661\". . . . Run the following command to start building the image. Make sure to copy the entire command, including the \".\" at the end (which indicates current directory). docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was .","title":"Build (Hands-on)"},{"location":"appmod-labs/OperationalModernization/README - Copy/#build-image-hands-on","text":"Review the command you ran earlier: docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was . It instructs docker to build the image following the instructions in the Dockerfile in current directory (indicated by the \".\" at the end). A specific name to tag the built image is also specified after --tag . The value default-route-openshift-image-registry.apps.demo.ibmdte.net in the tag is the default address of the internal image registry provided by OpenShift. Image registry is a content server that can store and serve container images. The registry is accessible within the cluster via its exposed Service . The format of a Service address: name . namespace . svc . In this case, the image registry is named image-registry and it's in namespace openshift-image-registry . Later when we push the image to OpenShift's internal image registry, we'll refer to the image by the same values. You should see the following message if the image was successfully built. Please wait if it's still building. Successfully built aa6babbb5ce9 Successfully tagged default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was:latest Validate that image is in the repository by running the command: docker images Notice that the base image, websphere-traditional, is also listed. It was pulled as the first step of building application image. Example output: REPOSITORY TAG IMAGE ID CREATED SIZE default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was latest 9394150a5a15 10 minutes ago 2.05GB ibmcom/websphere-traditional latest 898f9fd79b36 12 minutes ago 1.86GB Note that docker images only lists those images that are cached locally. The name of the image also contains the host name where the image is hosted. If there is no host name, the image is hosted on docker hub. For example, the image ibmcom/websphere-traditional has no host name. It is hosted on docker hub. The image we just built, default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was , has host name default-route-openshift-image-registry.apps.demo.ibmdte.net . It is to be hosted in the Openshift image registry for your lab cluster. If you change an image, or build a new image, the changes are only available locally. You must push the image to propagate the changes to the remote registry. Let's push the image you just built to your OpenShift cluster's built-in image registry. First, login to the image registry by running the following command in the terminal. Note: A session token is obtained from the value of another command oc whoami -t and used as the password to login. docker login -u $(oc whoami) -p $(oc whoami -t) default-route-openshift-image-registry.apps.demo.ibmdte.net Example output: WARNING! Using --password via the CLI is insecure. Use --password-stdin. WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded Now, push the image into OpenShift cluster's internal image registry, which will take 1-2 minutes: docker push default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was Example output: Using default tag: latest The push refers to repository [default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was] 470e7d3b0bec: Pushed c38e61da8211: Pushed 2a72e88fe5eb: Pushed 334c79ff1b2e: Pushed ccf8ea26529f: Pushed af0f17433f77: Pushed 4254aef2aa12: Pushed 855301ffdcce: Pushed ea252e2474a5: Pushed 68a4c9686496: Pushed 87ecb86bc8e5: Pushed 066b59214d49: Pushed 211f972e9c63: Pushed b93eee2b1ddb: Pushed a6ab5ae423d9: Pushed 3f785cf0a0ae: Pushed latest: digest: sha256:4f4e8ae82fa22c83febc4f884b5026d01815fc704df6196431db8ed7a7def6a0 size: 3672 Verify that the image is in the image registry. The following command will get the images in the registry. Filter through the results to get only the image you pushed. Run the following command: oc get images | grep apps-was/cos-was The application image you just pushed should be listed. The hash of the image is stored alongside (indicated by the SHA-256 value). Example output: image-registry.openshift-image-registry.svc:5000/apps-was/cos-was@sha256:bc072d3b78ae6adcd843af75552965e5ed863bcce4fc3f1bc5d194570bc16953 OpenShift uses ImageStream to provide an abstraction for referencing container images from within the cluster. When an image is pushed to registry, an ImageStream is created automatically, if one doesn't already exist. Run the following command to see the ImageStream that's created: oc get imagestreams -n apps-was Example output: ``` NAME IMAGE REPOSITORY TAGS UPDATED cos-was default-route-openshift-image-registry.apps.demo.ibmdte.net/apps-was/cos-was latest 2 minutes ago ``` You can also use the OpenShift console (UI) to see the ImageStream : From the panel on left-side, click on Builds > Image Streams . Then select apps-was from the Project drop-down menu. Click on cos-was from the list. Scroll down to the bottom to see the image that you pushed.","title":"Build image (Hands-on)"},{"location":"appmod-labs/OperationalModernization/README - Copy/#deploy-without-operator","text":"The following steps will deploy the modernized Customer Order Services application in a traditional WebSphere Base container to a RedHat OpenShift cluster. Customer Order Services application uses DB2 as its database. You can connect to an on-prem database that already exists or migrate the database to cloud. Since migrating the database is not the focus of this particular workshop and to save time, the database needed by the application is already configured in the OpenShift cluster you are using.","title":"Deploy without operator"},{"location":"appmod-labs/OperationalModernization/README - Copy/#deploy-application-without-operator-hands-on","text":"Run the following command to deploy the resources (*.yaml files) in the deploy directory: oc apply -f deploy Output: deployment.apps/cos-was created route.route.openshift.io/cos-was created secret/authdata created service/cos-was created Let's review what we just did. The directory deploy contains the following yaml files: Deployment.yaml : the specification for creating a Kubernetes deployment Service.yaml : the specification to expose the deployment as a cluster-wide Kubernetes service. Route.yaml : the specification to expose the service as a route visible outside of the cluster. Secret.yaml : the specification that the properties based configuration properties file used to configure database user/password when the container starts. The file Deployment.yaml looks like: apiVersion: apps/v1 kind: Deployment metadata: name: cos-was namespace: apps-was spec: selector: matchLabels: app: cos-was replicas: 1 template: metadata: labels: app: cos-was spec: containers: - name: cos-was image: image-registry.openshift-image-registry.svc:5000/apps-was/cos-was ports: - containerPort: 9080 livenessProbe: httpGet: path: /CustomerOrderServicesWeb/index.html port: 9080 periodSeconds: 30 failureThreshold: 6 initialDelaySeconds: 90 readinessProbe: httpGet: path: /CustomerOrderServicesWeb/index.html port: 9080 periodSeconds: 10 failureThreshold: 3 volumeMounts: - mountPath: /etc/websphere name: authdata readOnly: true volumes: - name: authdata secret: secretName: authdata Note: The liveness probe is used to tell Kubernetes when the application is live. Due to the size of the traditional WAS image, the initialDelaySeconds attribute has been set to 90 seconds to give the container time to start. The readiness probe is used to tell Kubernetes whether the application is ready to serve requests. You may store property file based configuration files such as configmaps and secrets, and bind their contents into the /etc/websphere directory. When the container starts, the server startup script will apply all the property files found in the /etc/websphere directory to reconfigure the server. For our example, the volumeMounts and volumes are used to bind the contents of the secret authdata into the directory /etc/websphere during container startup. After it is bound, it will appear as the file /etc/websphere/authdata.properties . For volumeMounts: The mountPath, /etc/websphere , specifies the directory where the files are bound. the name, authdata , specifies the name of the volume For volumes: the secretName specifies the name of the secret whose contents are to be bound. The file Secret.yaml looks like: apiVersion: v1 kind: Secret metadata: name: authdata namespace: apps-was type: Opaque stringData: authdata.props: |- # # Configuration properties file for cells/DefaultCell01|security.xml#JAASAuthData_1597094577206# # Extracted on Tue Aug 11 15:30:36 UTC 2020 # # # Section 1.0 ## Cell=!{cellName}:Security=:JAASAuthData=alias#DBUser # # # SubSection 1.0.0 # JAASAuthData Section # ResourceType=JAASAuthData ImplementingResourceType=GenericType ResourceId=Cell=!{cellName}:Security=:JAASAuthData=alias#DBUser AttributeInfo=authDataEntries # # #Properties # password=\"{xor}Oz1tNjEsK24=\" #required alias=DBUser #required userId=db2inst1 #required description= # # End of Section 1.0# Cell=!{cellName}:Security=:JAASAuthData=alias#DBUser # # # EnvironmentVariablesSection # # #Environment Variables cellName=DefaultCell01 The attribute authdata.properties contains the properties file based configure used to update the database userId and password for the JAASAuthData whose alias is DBUser. The configuration in Deployment.yaml maps it as the file /etc/websphere/authdata.properties during container startup so that the application server startup script can automatically configure the server with these entries.","title":"Deploy application without operator (Hands-on)"},{"location":"appmod-labs/OperationalModernization/README - Copy/#access-the-application-without-operator-hands-on","text":"Confirm you're at the current project apps-was : oc project Example output: Using project \"apps-was\" on server \"https://c114-e.us-south.containers.cloud.ibm.com:30016\". - If it's not at the project apps-was , then switch: oc project apps-was 1. Run the following command to verify the pod is running: oc get pod If the status does not show 1/1 READY, wait a while, checking status periodically: NAME READY STATUS RESTARTS AGE cos-was-6bd4767bf6-xhr92 1/1 Running 0 120m Run the following command to get the URL of your application (the route URL plus the application contextroot): echo http://$(oc get route cos-was --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-was-apps-was.apps.demo.ibmdte.net/CustomerOrderServicesWeb Return to your Firefox browser window and go to the URL outputted by the command run in the previous step. You will be prompted to login in order to access the application. Enter the following credentials: Username: skywalker Password: force After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total . Close the browser.","title":"Access the application without operator (Hands-on)"},{"location":"appmod-labs/OperationalModernization/README - Copy/#review-the-application-workload-flow-without-operator-hands-on","text":"Below is an overview diagram on the deployment you've completed from the above steps: Note: DB2 in the middle of the diagram is pre-installed through a different project db and has been up and running before your hands-on. Also it will not be impacted when you're removing the deployment in next step. Return to the Firefox browser and open the OpenShift Console to view the resources on the deployment. View the resources in the project apps-was : Select the apps-was project from the project drop down menu. View deployment details: Click on the Deployments tab under Workloads from the left menu and select cos-was Navigate to the YAML tab to view the content of yaml View pod details: Click on the Pods tab under Workloads from the left menu and select the pod with name starting with cos-was Navigate to the Logs tab to view the WebSphere Application Server log Navigate to the Terminal tab to view the files inside the container View secret details: Click on the Secrets tab under Workloads from the left menu and select the authdata secret. Scroll down to the Data section and click on the copy icon to view the content. View service details: Click on the Services tab under Networking from the left menu and select the cos-was service. Review service information including address and port mapping details. View route details: Click on the Routes tab under Networking from the left menu and select the cos-was route. 1. View the resources in the project db : - Select the db project from the project drop down menu. View deployment details: Click on the Deployments tab under Workloads from the left menu and select cos-db-was Navigate to the YAML tab to view the content of yaml View pod details: Click on the Pods tab under Workloads from the left menu and select the pod with name starting with cos-db-was Navigate to the Logs tab to view the database logs Navigate to the Terminal tab to view the files in the database container View service details: Click on the Services tab under Networking from the left menu and select the cos-db-was service.","title":"Review the application workload flow without operator (Hands-on)"},{"location":"appmod-labs/OperationalModernization/README - Copy/#remove-your-deployment-standard-deployment-without-operator-hands-on","text":"To remove the deploment from the above scenario without the operator, run the command: Note: The pre-installed resources such as DB2, are not removed. oc delete -f deploy Output: deployment.apps \"cos-was\" deleted route.route.openshift.io \"cos-was\" deleted secret \"authdata\" deleted service \"cos-was\" deleted","title":"Remove your deployment (standard deployment without operator) (Hands-on)"},{"location":"appmod-labs/OperationalModernization/README - Copy/#alternate-deployment-via-runtime-component-operator","text":"Another way to deploy the application is via the Runtime Component Operator. It is a generic operator used to deploy different types of application images. The operator has already been installed into your environment. For more information, see: https://github.com/application-stacks/runtime-component-operator","title":"Alternate Deployment Via Runtime Component Operator"},{"location":"appmod-labs/OperationalModernization/README - Copy/#deploy-application-via-runtime-component-operator-hands-on","text":"Run the following command which uses the Runtime Component Operator to deploy the same Customer Order Service application image: oc apply -f deploy-rco Output: runtimecomponent.app.stacks/cos-was-rco created secret/authdata-rco created Let's review what we just did. First, list the contents of the deploy-rco directory: ls deploy-rco The output shows there are only two yaml files: RuntimeComponent.yaml Secret.yaml Review Secret.yaml: cat deploy-rco/Secret.yaml Note that it is the same as the Secret.yaml in the deploy directory, except the name has been changed to authdata-rco . It serves the same purpose for this new deployment - to override the database user/password. Review RuntimeComponent.yaml: cat deploy-rco/RuntimeComponent.yaml And the output: apiVersion: app.stacks/v1beta1 kind: RuntimeComponent metadata: name: cos-was-rco namespace: apps-was spec: applicationImage: image-registry.openshift-image-registry.svc:5000/apps-was/cos-was service: port: 9080 readinessProbe: httpGet: path: /CustomerOrderServicesWeb/index.html port: 9080 periodSeconds: 10 failureThreshold: 3 livenessProbe: httpGet: path: /CustomerOrderServicesWeb/index.html port: 9080 periodSeconds: 30 failureThreshold: 6 initialDelaySeconds: 90 expose: true route: termination: edge insecureEdgeTerminationPolicy: Redirect volumeMounts: - mountPath: /etc/websphere name: authdata-rco readOnly: true volumes: - name: authdata-rco secret: secretName: authdata-rco Note that: - The kind is RuntimeComponent - The expose attribute is set to true to expose a route - The attributes within the yaml file are essentially the same information that you provided for the Service , Route , and Deployment resources in the deploy directory. - The controller for the RuntimeComponent custom resource reacts to changes in the above specification, and creates the corresponding Service , Route , and Deployment objects. Issue the following commands to view what the controller has created: ``` oc get Deployment cos-was-rco -o yaml oc get Service cos-was-rco -o yaml oc get Route cos-was-rco -o yaml ```","title":"Deploy application (via Runtime Component Operator) (Hands-on)"},{"location":"appmod-labs/OperationalModernization/README - Copy/#access-the-application-via-runtime-component-operator-hands-on","text":"Confirm you're at the current project apps-was : oc project Example output: Using project \"apps-was\" on server \"https://c114-e.us-south.containers.cloud.ibm.com:30016\". - If it's not at the project apps-was , then switch: oc project apps-was 1. Run the following command to verify the pod is running: oc get pod If the status does not show 1/1 READY, wait a while, checking status periodically. Note the prefix name for the pod is cos-was-rco . NAME READY STATUS RESTARTS AGE cos-was-rco-6779784fc8-pz92m 1/1 Running 0 2m59s Run the following command to get the URL of your application (the route URL plus the application contextroot): echo http://$(oc get route cos-was-rco --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-was-rco-apps-was.apps.demo.ibmdte.net/CustomerOrderServicesWeb Return to your Firefox browser window and go to the URL outputted by the command run in the previous step. The steps to access the application are the same as those used when deploying without the operator. You will be prompted to login in order to access the application. Enter the following credentials: Username: skywalker Password: force After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total . Close the browser.","title":"Access the application (via Runtime Component Operator) (Hands-on)"},{"location":"appmod-labs/OperationalModernization/README - Copy/#review-the-application-workload-flow-with-runtime-component-operator-hands-on","text":"Below is an overview diagram on the deployment you've completed from the above steps using Runtime Component Operator: Note: DB2 in the middle of the diagram is pre-installed through a different project db and has been up and running before your hands-on. Return to the Firefox browser and open the OpenShift Console to view the resources on the deployment. View the resources in the project openshift-operators : Select the openshift-operators project from the project drop down menu. View operator deployment details: Click on the Deployments tab under Workloads from the left menu and select runtime-component-operator Navigate to the YAML tab to view the content of yaml View operator pod details: Click on the Pods tab under Workloads from the left menu and select the pod with name starting with runtime-component-operator Navigate to Logs to view the runtime-component-operator container log Navigate to Terminal to view the files in the container View the resources in the project apps-was : Select the apps-was project from the project drop down menu. View Runtime Component instance details: Click on the Installed Operators tab under Operators from the left menu and select Runtime Component Operator . > Note: The operator is installed at cluster level and is visible to all existing projects, but Runtime Component instance is created under the project apps-was . Navigate to the YAML tab to view the content of yaml Navigate to the Runtime Component tab and select cos-was-rco to view the deails of Runtime Component instance View deployment details: Click on the Deployments tab under Workloads from the left menu and select cos-was-rco . Navigate to the YAML tab to view the content of yaml. > Note the deployment is created through the controller of RuntimeComponent custom resource. View pod details: Click on the Pods tab under Workloads from the left menu and select the pod starting with cos-was-rco Navigate to the Logs tab to view the WebSphere Application Server log View service details: Click on the Services tab under Networking from the left menu and select cos-was-rco Navigate to the YAML tab to view the content of yaml. > Note the service is created through the controller of RuntimeComponent custom resource. View route details: Click on the Routes tab under Networking from the left menu and select cos-was-rco Navigate to the YAML tab to view the content of yaml. > Note the route is created through the controller of RuntimeComponent custom resource. View secret details: Click on the Secrets tab under Workloads from the left menu and select authdata-rco Resources in the project db : Same information as listed in the section above in Review the application workload flow without operator .","title":"Review the application workload flow with Runtime Component Operator (Hands-on)"},{"location":"appmod-labs/OperationalModernization/README - Copy/#cleanup-the-deployment-with-runtime-component-operator-hands-on","text":"Run the following command in a terminal window to remove the deployment from the above secenario with Runtime Component instance: Note: The pre-installed resources such as Runtime Component Operator, DB2, are not removed. oc delete -f deploy-rco Output: runtimecomponent.app.stacks \"cos-was-rco\" deleted secret \"authdata-rco\" deleted Verify that the corresponding application Service , Route , and Deployment have also been deleted: oc get Deployment oc get Service oc get Route Output from each get command above should be: No resources found in apps-was namespace.","title":"Cleanup (the deployment with Runtime Component Operator) (Hands-on)"},{"location":"appmod-labs/OperationalModernization/README - Copy/#summary","text":"Congratulations! You've completed the Operational Modernization lab. You containerized and deployed a monolith application to cloud!","title":"Summary"},{"location":"appmod-labs/OperationalModernization/README - Copy/#next","text":"Please follow the link to the next lab Runtime Modernization : - Runtime Modernization","title":"Next"},{"location":"appmod-labs/OperationalModernization/extras/WAS-analyze/","text":"Traditional WebSphere - Analyze This section covers how to use IBM Cloud Transformation Advisor to analyze an existing traditional WebSphere application. For this scenario the traditional WebSphere Application Server runtime is chosen as the target runtime and the intention is to migrate this application without code changes. Migrating to the containerized version of traditional WebSphere Application Server will prepare the organization for: moving workloads to the cloud. improving DevOps and speed-to-market. receiving the benefits of the consistency and reliability of containers. Summary This section has the following steps: Introduction to IBM Cloud Transformation Advisor Install IBM Cloud Transformation Advisor Download and run the Data Collector Upload and analyze the results Determine the migration/modernization path and next steps Introduction to IBM Cloud Transformation Advisor IBM Cloud Transformation Advisor helps you access, analyze and modernize middleware based apps into IBM Cloud(s). It categorizes Java EE apps and MQ queue managers as simple, medium and complex based on migration complexity and provides guidance for modernization. IBM Cloud Transformation Advisor can accelerate the process to move your on-premises apps to cloud, minimize migration errors and risks, and reduce time to market in five steps. You can use IBM Transformation Advisor for these activities: - Identify the Java EE programming models in an app - Determine the complexity of apps by reviewing a high-level inventory of the content and structure of each app - Highlight the Java EE programming model and WebSphere API differences between the profile types - Learn any Java EE specification implementation differences that might affect the app The tool also provides suggestions for the right-fit IBM WebSphere Application Server edition and offers advice, practices, and potential solutions to move apps to Liberty or to newer versions of WebSphere Application Server. Install IBM Cloud Transformation Advisor IBM Cloud Transformation Advisor is split in to two components (the analysis engine and the data collector ). Access Transformation Advisor from IBM WebSphere Hybrid Edition. You can also run it locally on a machine with Docker. See Installing IBM Cloud Transformation Advisor locally . Download the Data Collector Once IBM Cloud Transformation Advisor is installed, it is necessary to create a new Workspace and Collection and then download the Data Collector that will be used to examine the existing environment and applications. Open IBM Cloud Transformation Advisor in a browser and click the button to create a new Workspace Enter a Workspace name such as TransformationAdvisorDemo and click Next Enter a Collection name such as WAS855_AppSrv01 and click Let's go When the No recommendations available page is displayed, click the Data Collector button When the Data Collector page is displayed, select the Source Operating System for your environment and click the Download button to download the Data Collector. This results in a file with a name similar to transformationadvisor-Linux_TransformationAdvisorDemo_WAS855_AppSrv01.tgz being downloaded. Run the Data Collector Upload the Data Collector zip file that was downloaded from IBM Cloud Transformation Advisor in the previous step to the machine that the WebSphere ND Deployment Manager or the Standalone WebSphere Application Server is installed. The directory used arbitrary. Navigate to the directory you uploaded the transformationadvisor-Linux_TransformationAdvisorDemo_WAS855_AppSrv01.tgz file to and issue the following commands to extract the Data Collector: mkdir datacollector cd datacollector mv transformationadvisor-Linux_TransformationAdvisorDemo_WAS855_AppSrv01.tgz . tar -zxvf transformationadvisor-Linux_TransformationAdvisorDemo_WAS855_AppSrv01.tgz cd transformationadvisor-* It is necessary to modify the scan performed by the Data Collector to include the org.pwte package as the Data Collector doesn't scan org.* packages by default. Open the conf/customCmd.properties file and modify it as shown below: evaluation=--evaluate --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp --includePackages=org.pwte migration_liberty=--analyze --sourceAppServer=was855 --targetAppServer=liberty --targetCloud=dockerIBMCloud --includePackages=org.pwte --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp migration_was=--analyze --sourceAppServer=was855 --targetAppServer=was90 --targetCloud=vmIBMCloud --includePackages=org.pwte --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp #inventory=--inventory --excludeFiles=\".*/directory/LargeXMLFileName.xml\" #featureList=--featureList --excludeFiles=\".*/directory/LargeXMLFileName.xml\" #java_opt=-Xmx2g The following command assumes that WebSphere Application Server v855 is installed to /opt/IBM/WebSphere/AppServer855 with a profile named AppSrv01 and that the administration user is wasadmin with a password of wasadmin . Modify and issue the following command as necessary to execute the Data Collector against the WebSphere environment: ./bin/transformationadvisor -w /opt/IBM/WebSphere/AppServer855 -p AppSrv01 wasadmin wasadmin When prompted, accept the license agreement . The Data Collection process will now start and will analyze all of the applications installed in the WebSphere Application Server environment and will also collect the related Java EE artifacts such as Data Sources and JMS definitions. When the analysis is complete, the Data Collector will attempt to upload the collection results to IBM Cloud Transformation Advisor. If this is successful, you can skip to the Analyze the Recommendations section. If not, you will receive an error at the end of Data Collection and will find a file named AppSrv01.zip in your current directory as shown below. ~/datacollector/transformationadvisor-2.4.2# ls -la *.zip -rw-r--r-- 1 root root 625493 Jun 12 12:58 AppSrv01.zip Download this Data Collector Results zip file ready for uploading to IBM Cloud Transformation Advisor in the next section Upload the Data Collector results In this section the results from the Data Collector will be uploaded to IBM Cloud Transformation Advisor. In the IBM Cloud Transformation Advisor web browser session, click the Recomendations link in the top left corner and then click the Upload data button as shown below When the Upload data dialog is displayed, use the Drop or Add File button to select the Data Collector Results zip file that was downloaded in the previous section. Click Upload After a few moments the upload of the data collector results will be completed. Analyze the Recommendations Once the Data Collector Results have been uploaded to IBM Cloud Transformation Advisor a set of recommendations will be created and shown on the Recommendations page. In this section the recommendations will be analyzed and interpreted. The default recommendations are based on a target runtime of Liberty runtimes . In this scenario the desired target runtime is WebSphere Traditional . Use the Migration target drop down to select WebSphere traditional as shown below. The Data Collector analyzed all of the applications running on the traditional WebSphere profile a displays a row in the chart for each application. In the case of the CustomerOrderServicesApp.ear application, IBM Cloud Transformation Advisor has determined that the migration to WebSphere Traditional is of Moderate complexity and that there are four Severe Issues that have been detected. Click on the CustomerOrderServicesApp.ear application name to see more information. Review the analysis results and scroll down to the Technology Issues section. Note that IBM Cloud Transformation Advisor has detected that there are issues with JPA, specifically that the second-level cache and the JPA configuration properties must be migrated and with JAX-RS (missing Apache and Wink packages). These issues are related to a decision that was taken by IBM to allow WebSphere Application Server V9 to run in either JPA 2.0 or JPA 2.1 mode as described here and in either JAX-RS 2.0 or JAX-RS 1.1 mode as described here . In order to run in JPA 2.1 mode and JAX-RS 2.0 mode, the changes highlighted by IBM Cloud Transformation Advisor must be made to the application. However, this application can run in JPA 2.0 mode and JAX-RS 1.1 mode with no changes . In order to review the IBM Cloud Transformation Advisor results in more detail, scroll to the bottom of the analysis page and click on the Analysis Report link When the warning dialog is displayed, click OK The Detailed Migration Analysis Report will be displayed which show the results of the migration rules that were executed by the Data Collector and returned results. Scroll down to the Severe Rules section and click on the Show rule help link for each of the results. Review the recommendations. Final Analysis The intention of this traditional WebSphere V855 --> traditional WebSphere V9 scenario is to migrate the Customer Order Services application to the new runtime without code changes. IBM Cloud Transformation Advisor was used to analyze the application for compatibility with traditional WebSphere V9 and determined that code changes would be required. IBM Cloud Transformation Advisor took the approach that the application should be modified to run with the JPA 2.1 runtime and the JAX-RS 2.0 runtime instead of giving the option to use the JPA 2.0 runtime and the JAX-RS 1.1 runtime which would have resulted in not requiring any code changes to the application. The remainder of this scenario is based on the decision to use the JPA 2.0 runtime option and the JAX-RS 1.1 runtime option in traditional WebSphere V9 and as such no code changes will be required to this application.","title":"Traditional WebSphere - Analyze"},{"location":"appmod-labs/OperationalModernization/extras/WAS-analyze/#traditional-websphere-analyze","text":"This section covers how to use IBM Cloud Transformation Advisor to analyze an existing traditional WebSphere application. For this scenario the traditional WebSphere Application Server runtime is chosen as the target runtime and the intention is to migrate this application without code changes. Migrating to the containerized version of traditional WebSphere Application Server will prepare the organization for: moving workloads to the cloud. improving DevOps and speed-to-market. receiving the benefits of the consistency and reliability of containers.","title":"Traditional WebSphere - Analyze"},{"location":"appmod-labs/OperationalModernization/extras/WAS-analyze/#summary","text":"This section has the following steps: Introduction to IBM Cloud Transformation Advisor Install IBM Cloud Transformation Advisor Download and run the Data Collector Upload and analyze the results Determine the migration/modernization path and next steps","title":"Summary"},{"location":"appmod-labs/OperationalModernization/extras/WAS-analyze/#introduction-to-ibm-cloud-transformation-advisor","text":"IBM Cloud Transformation Advisor helps you access, analyze and modernize middleware based apps into IBM Cloud(s). It categorizes Java EE apps and MQ queue managers as simple, medium and complex based on migration complexity and provides guidance for modernization. IBM Cloud Transformation Advisor can accelerate the process to move your on-premises apps to cloud, minimize migration errors and risks, and reduce time to market in five steps. You can use IBM Transformation Advisor for these activities: - Identify the Java EE programming models in an app - Determine the complexity of apps by reviewing a high-level inventory of the content and structure of each app - Highlight the Java EE programming model and WebSphere API differences between the profile types - Learn any Java EE specification implementation differences that might affect the app The tool also provides suggestions for the right-fit IBM WebSphere Application Server edition and offers advice, practices, and potential solutions to move apps to Liberty or to newer versions of WebSphere Application Server.","title":"Introduction to IBM Cloud Transformation Advisor"},{"location":"appmod-labs/OperationalModernization/extras/WAS-analyze/#install-ibm-cloud-transformation-advisor","text":"IBM Cloud Transformation Advisor is split in to two components (the analysis engine and the data collector ). Access Transformation Advisor from IBM WebSphere Hybrid Edition. You can also run it locally on a machine with Docker. See Installing IBM Cloud Transformation Advisor locally .","title":"Install IBM Cloud Transformation Advisor"},{"location":"appmod-labs/OperationalModernization/extras/WAS-analyze/#download-the-data-collector","text":"Once IBM Cloud Transformation Advisor is installed, it is necessary to create a new Workspace and Collection and then download the Data Collector that will be used to examine the existing environment and applications. Open IBM Cloud Transformation Advisor in a browser and click the button to create a new Workspace Enter a Workspace name such as TransformationAdvisorDemo and click Next Enter a Collection name such as WAS855_AppSrv01 and click Let's go When the No recommendations available page is displayed, click the Data Collector button When the Data Collector page is displayed, select the Source Operating System for your environment and click the Download button to download the Data Collector. This results in a file with a name similar to transformationadvisor-Linux_TransformationAdvisorDemo_WAS855_AppSrv01.tgz being downloaded.","title":"Download the Data Collector"},{"location":"appmod-labs/OperationalModernization/extras/WAS-analyze/#run-the-data-collector","text":"Upload the Data Collector zip file that was downloaded from IBM Cloud Transformation Advisor in the previous step to the machine that the WebSphere ND Deployment Manager or the Standalone WebSphere Application Server is installed. The directory used arbitrary. Navigate to the directory you uploaded the transformationadvisor-Linux_TransformationAdvisorDemo_WAS855_AppSrv01.tgz file to and issue the following commands to extract the Data Collector: mkdir datacollector cd datacollector mv transformationadvisor-Linux_TransformationAdvisorDemo_WAS855_AppSrv01.tgz . tar -zxvf transformationadvisor-Linux_TransformationAdvisorDemo_WAS855_AppSrv01.tgz cd transformationadvisor-* It is necessary to modify the scan performed by the Data Collector to include the org.pwte package as the Data Collector doesn't scan org.* packages by default. Open the conf/customCmd.properties file and modify it as shown below: evaluation=--evaluate --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp --includePackages=org.pwte migration_liberty=--analyze --sourceAppServer=was855 --targetAppServer=liberty --targetCloud=dockerIBMCloud --includePackages=org.pwte --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp migration_was=--analyze --sourceAppServer=was855 --targetAppServer=was90 --targetCloud=vmIBMCloud --includePackages=org.pwte --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp #inventory=--inventory --excludeFiles=\".*/directory/LargeXMLFileName.xml\" #featureList=--featureList --excludeFiles=\".*/directory/LargeXMLFileName.xml\" #java_opt=-Xmx2g The following command assumes that WebSphere Application Server v855 is installed to /opt/IBM/WebSphere/AppServer855 with a profile named AppSrv01 and that the administration user is wasadmin with a password of wasadmin . Modify and issue the following command as necessary to execute the Data Collector against the WebSphere environment: ./bin/transformationadvisor -w /opt/IBM/WebSphere/AppServer855 -p AppSrv01 wasadmin wasadmin When prompted, accept the license agreement . The Data Collection process will now start and will analyze all of the applications installed in the WebSphere Application Server environment and will also collect the related Java EE artifacts such as Data Sources and JMS definitions. When the analysis is complete, the Data Collector will attempt to upload the collection results to IBM Cloud Transformation Advisor. If this is successful, you can skip to the Analyze the Recommendations section. If not, you will receive an error at the end of Data Collection and will find a file named AppSrv01.zip in your current directory as shown below. ~/datacollector/transformationadvisor-2.4.2# ls -la *.zip -rw-r--r-- 1 root root 625493 Jun 12 12:58 AppSrv01.zip Download this Data Collector Results zip file ready for uploading to IBM Cloud Transformation Advisor in the next section","title":"Run the Data Collector"},{"location":"appmod-labs/OperationalModernization/extras/WAS-analyze/#upload-the-data-collector-results","text":"In this section the results from the Data Collector will be uploaded to IBM Cloud Transformation Advisor. In the IBM Cloud Transformation Advisor web browser session, click the Recomendations link in the top left corner and then click the Upload data button as shown below When the Upload data dialog is displayed, use the Drop or Add File button to select the Data Collector Results zip file that was downloaded in the previous section. Click Upload After a few moments the upload of the data collector results will be completed.","title":"Upload the Data Collector results"},{"location":"appmod-labs/OperationalModernization/extras/WAS-analyze/#analyze-the-recommendations","text":"Once the Data Collector Results have been uploaded to IBM Cloud Transformation Advisor a set of recommendations will be created and shown on the Recommendations page. In this section the recommendations will be analyzed and interpreted. The default recommendations are based on a target runtime of Liberty runtimes . In this scenario the desired target runtime is WebSphere Traditional . Use the Migration target drop down to select WebSphere traditional as shown below. The Data Collector analyzed all of the applications running on the traditional WebSphere profile a displays a row in the chart for each application. In the case of the CustomerOrderServicesApp.ear application, IBM Cloud Transformation Advisor has determined that the migration to WebSphere Traditional is of Moderate complexity and that there are four Severe Issues that have been detected. Click on the CustomerOrderServicesApp.ear application name to see more information. Review the analysis results and scroll down to the Technology Issues section. Note that IBM Cloud Transformation Advisor has detected that there are issues with JPA, specifically that the second-level cache and the JPA configuration properties must be migrated and with JAX-RS (missing Apache and Wink packages). These issues are related to a decision that was taken by IBM to allow WebSphere Application Server V9 to run in either JPA 2.0 or JPA 2.1 mode as described here and in either JAX-RS 2.0 or JAX-RS 1.1 mode as described here . In order to run in JPA 2.1 mode and JAX-RS 2.0 mode, the changes highlighted by IBM Cloud Transformation Advisor must be made to the application. However, this application can run in JPA 2.0 mode and JAX-RS 1.1 mode with no changes . In order to review the IBM Cloud Transformation Advisor results in more detail, scroll to the bottom of the analysis page and click on the Analysis Report link When the warning dialog is displayed, click OK The Detailed Migration Analysis Report will be displayed which show the results of the migration rules that were executed by the Data Collector and returned results. Scroll down to the Severe Rules section and click on the Show rule help link for each of the results. Review the recommendations.","title":"Analyze the Recommendations"},{"location":"appmod-labs/OperationalModernization/extras/WAS-analyze/#final-analysis","text":"The intention of this traditional WebSphere V855 --> traditional WebSphere V9 scenario is to migrate the Customer Order Services application to the new runtime without code changes. IBM Cloud Transformation Advisor was used to analyze the application for compatibility with traditional WebSphere V9 and determined that code changes would be required. IBM Cloud Transformation Advisor took the approach that the application should be modified to run with the JPA 2.1 runtime and the JAX-RS 2.0 runtime instead of giving the option to use the JPA 2.0 runtime and the JAX-RS 1.1 runtime which would have resulted in not requiring any code changes to the application. The remainder of this scenario is based on the decision to use the JPA 2.0 runtime option and the JAX-RS 1.1 runtime option in traditional WebSphere V9 and as such no code changes will be required to this application.","title":"Final Analysis"},{"location":"appmod-labs/OperationalModernization/extras/application/","text":"Application Overview The Customer Order Services application is a simple store-front shopping application, built during the early days of the Web 2.0 movement. Users interact directly with a browser-based interface and manage their cart to submit orders. This application is built using the traditional 3-Tier Architecture model, with an HTTP server, an application server, and a supporting database. There are several components of the overall application architecture: - Starting with the database, the application leverages two SQL-based databases running on IBM DB2. The application exposes its data model through an Enterprise JavaBean layer, named CustomerOrderServices . This component leverages the Java Persistence API to expose the backend data model to calling services with minimal coding effort. This build of the application uses JavaEE6 features for EJBs and JPA. The next tier of the application, named CustomerOrderServicesWeb , exposes the necessary business APIs via REST-based web services. This component leverages the JAX-RS libraries for creating Java-based REST services with minimal coding effort. This build of the application is using JAX-RS 1.1 version of the respective capability. The application's user interface is exposed through the CustomerOrderServicesWeb component as well, in the form of a Dojo Toolkit-based JavaScript application. Delivering the user interface and business APIs in the same component is one major inhibitor our migration strategy will help to alleviate in the long-term. Finally, there is an additional integration testing component, named CustomerOrderServicesTest that is built to quickly validate an application's build and deployment to a given application server. This test component contains both JPA and JAX-RS -based tests.","title":"Application Overview"},{"location":"appmod-labs/OperationalModernization/extras/application/#application-overview","text":"The Customer Order Services application is a simple store-front shopping application, built during the early days of the Web 2.0 movement. Users interact directly with a browser-based interface and manage their cart to submit orders. This application is built using the traditional 3-Tier Architecture model, with an HTTP server, an application server, and a supporting database. There are several components of the overall application architecture: - Starting with the database, the application leverages two SQL-based databases running on IBM DB2. The application exposes its data model through an Enterprise JavaBean layer, named CustomerOrderServices . This component leverages the Java Persistence API to expose the backend data model to calling services with minimal coding effort. This build of the application uses JavaEE6 features for EJBs and JPA. The next tier of the application, named CustomerOrderServicesWeb , exposes the necessary business APIs via REST-based web services. This component leverages the JAX-RS libraries for creating Java-based REST services with minimal coding effort. This build of the application is using JAX-RS 1.1 version of the respective capability. The application's user interface is exposed through the CustomerOrderServicesWeb component as well, in the form of a Dojo Toolkit-based JavaScript application. Delivering the user interface and business APIs in the same component is one major inhibitor our migration strategy will help to alleviate in the long-term. Finally, there is an additional integration testing component, named CustomerOrderServicesTest that is built to quickly validate an application's build and deployment to a given application server. This test component contains both JPA and JAX-RS -based tests.","title":"Application Overview"},{"location":"appmod-labs/RuntimeModernization/","text":"Runtime Modernization Table of Contents Introduction Analysis (Hands-on) Build (Hands-on) Deploy (Hands-on) Access the Application (Hands-on) Review Deployment Cleanup (can be skipped if the next lab Application Management is included in the same session) Extra Credit Summary Next Introduction Runtime modernization moves an application to a 'built for the cloud' runtime with the least amount of effort. Open Liberty is a fast, dynamic, and easy-to-use Java application server. Ideal for the cloud, Liberty is open sourced, with fast start-up times (<2 seconds), no server restarts to pick up changes, and a simple XML configuration. This path gets the application on to a cloud-ready runtime container which is easy to use and portable. In addition to the necessary library changes, some aspects of the application were modernized. However, it has not been 'modernized' to a newer architecture such as micro-services . This lab demonstrates runtime modernization . It uses the Customer Order Services application, which originates from WebSphere ND V8.5.5. Click here to get to know the application, including its architecture and components. The application will go through the analysis , build and deploy phases. It is modernized to run on the Liberty runtime, and deployed via the IBM Cloud Pak for Applications to RedHat OpenShift Container Platform (OCP). Login to the VM If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password: Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window. Analysis (Hands-on) In this lab, we will demonstrate how the Transformation Advisor can be used specifically in the runtime modernization process, using Liberty in containers on OpenShift. The steps needed to analyze the existing Customer Order Services application are: Open a Firefox browser window from within the VM. Click on the openshift console bookmark in the top left and log in with the htpasswd option. Log in to the OpenShift account using the following credentials: Username: ibmadmin Password: engageibm From the Red Hat OpenShift Container Platform console, go to the Networking tab and click on Routes . Ensure that you are in the ta project by using the project drop down and click on the Location URL next to ta-ui-route . This will open the Transformation Advisor user interface. Click Create new under Workspaces to create a new workspace. Name it RuntimeModernization and click Next . You'll be asked to create a new collection to store the data collected from the Customer Order Services application. Name the new collection CustomerOrderServices . Click Create . To provide application assessment data and receive recommendations, you would typically download and execute the Data Collector against an existing WebSphere environment. The output from the data collector is a zip file containing the application and configuration metadata gathered from the WebSphere Server, and is the input to the IBM Transformation Advsor tool. However, for this lab, the data collection archive has already been created for you and the resulting data is stored here . Click the Upload button, as we already have the data collectin archive to upload. Upload the results of the data collection (the datacollector.zip file) to IBM Cloud Transformation Advisor. When the upload is complete, you will see a list of applications analyzed from the source environment. At the top of the page, you can see the source environment and the target environment settings. Under the Migration target field, click the down arrow and select Compatible runtimes . This shows an entry for each application for each compatible destination runtime you can migrate it to. Click the CustomerOrderServicesApp.ear application with the Open Liberty migration target to open the Application details page . This lab covers runtime modernization, so the application will be re-platformed to run on Open Liberty, and will be placed in a container and deployed to OCP. Look over the migration analysis which shows a summary of the complexity of migrating this application to this target. In summary, migration of this application to Open Liberty is of Moderate complexity as some code changes may be required. Note: There may be a severe issue related to third-party APIs, but this doesn't apply as they occur in test code. Click on View migration plan in the top right corner of the page. This page will help you assemble an archive containing the following resources: your application's source or binary files (you upload these here or specify Maven coordinates to download them) a Dockerfile to build the Liberty container image with the application installed (generated by Transformation Advisor and automatically included) a jvm.options file that is used for configuring the Liberty runetime (generated by Transformation Advisor and automatically included) a server.xml file to configure the Liberty server for the application (generated by Transformation Advisor and automatically included) a pom.xml file that is used to build the applcation binaries (Jar, WAR, EAR) (generated by Transformation Advisor and automatically included) the deployment artifacts needed to create the container image and deploy the application to OCP (generated by Transformation Advisor and automatically included) Note: These artifacts have already been provided for you as part of the lab files, so you don't need to download the migration plan. However, you can do so if you wish to look around at the files. These files can also be sent to a Git repository by Transformation Advisor. Build (Hands-on) In this section, you'll learn how to build a Docker image for Customer Order Services application running on Liberty. Building this image could take around ~3 minutes so let's kick that process off and then come back to learn what you did. Open a new terminal window from the VM desktop. Login to OpenShift CLI with the oc login command from the web terminal. When prompted for the username and password, enter the following login credentials: Username: ibmadmin Password: engageibm If you have not yet cloned the GitHub repo with the lab artifacts, run the following command on your terminal: git clone https://github.com/IBM/openshift-workshop-was.git Change directory to where this lab is located: cd openshift-workshop-was cd labs/Openshift/RuntimeModernization ls 5 Run the following command to start building the image. Make sure to copy the entire command, including the \".\" at the end (indicated as the location of current directory). While the image is building (which takes ~3 minutes), continue with rest of the lab: docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos . Modernize with MicroProfile (for reading only) Eclipse MicroProfile is a modular set of technologies designed so that you can write cloud-native microservices. However, even though our application has not been refactored into microservices, we can still take advantage of some of the technologies from MicroProfile. Determine application's availability (for reading only) In the last lab, we used /CustomerOrderServicesWeb/index.html for readiness and liveness probes, which is not the best indication that an application is ready to handle traffic or is healthy to process requests correctly within a reasonable amount of time. What if the database is down? What if the application's security layer is not yet ready/unable to handle authentication? The Pod would still be considered ready and healthy and traffic would still be sent to it. All of those requests would fail or queue up, leading to bigger problems. MicroProfile Health provides a common REST endpoint format to determine whether a microservice (or in our case a monolith application) is healthy or not. The service itself might be running but considered unhealthy if the things it requires for normal operation are unavailable. All of the checks are performed periodically, and the result is served as a simple UP or DOWN at /health/ready and /health/live which can be used for readiness and liveness probes. We implemented the following health checks: ReadinessCheck : The application reports it is ready as long as the readiness check endpoint can be reached. Connections to other services and any other required conditions for the application to be considered ready are checked here. return HealthCheckResponse.named(\"Readiness\").up().build(); LivenessCheck : The requests should be processed within a reasonable amount of time. Monitor thread block times to identify potential deadlocks which can cause the application to hang. ThreadMXBean tBean = ManagementFactory.getThreadMXBean(); long ids[] = tBean.findMonitorDeadlockedThreads(); if (ids !=null) { ThreadInfo threadInfos[] = tBean.getThreadInfo(ids); for (ThreadInfo ti : threadInfos) { double seconds = ti.getBlockedTime() / 1000.0; if (seconds > 60) { return HealthCheckResponse.named(\"Liveness\").down().build(); } } } return HealthCheckResponse.named(\"Liveness\").up().build(); Adding metrics to application (for reading only) MicroProfile Metrics is used to gather metrics about the time it takes to add an item to cart, retrieve customer information, and count the number of times these operations are performed. @GET @Produces(MediaType.APPLICATION_JSON) @Counted @Timed(name = \"getCustomer_timed\") public Response getCustomer() { Liberty server configuration (for reading only) The Liberty runtime configuration files are based on a template provided by IBM Cloud Transformation Advisor. For this lab, instead of using a single server.xml, the configurations have been split into multiple configuration files and placed into the config/configDropins/overrides directory. You may place configuration files into configDropins/overrides directory to override pre-existing configurations. You may define separate template configurations that reflect the resources in your environment, and copy them into the configDropsins/overrides directory only for those applications that need them. Build image (Hands-on) Go back to your terminal to check the build you started earlier. You should see the following message if the image was built successfully. Please wait if it's still building: Successfully tagged default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos:latest Validate that the image is in the repository via the command line: docker images You should see the following images on the output. Notice that the base image, openliberty/open-liberty , is also listed. It was pulled as the first step of building application image. Example output: REPOSITORY TAG IMAGE ID CREATED SIZE default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos latest 4758119add3f 2 minutes ago 883MB <none> <none> 5bcb83fad548 5 minutes ago 792MB openliberty/open-liberty full-java8-openj9-ubi e6b5411076fe 5 days ago 794MB maven latest 1337d55397f7 4 weeks ago 631MB Before we push the image to OpenShift's internal image registry, create a separate project named apps . Choose one of two ways to create the project, command line or OpenShift console : Via the command line: oc new-project apps Example output: Now using project \"apps\" on server \"https://api.demo.ibmdte.net:64 . . . Via the OpenShift console: Open a Firefox browser window and click on the openshift console bookmark. Under the Home tab on the left menu, click Projects . Click on the Create Project button. Enter apps for the Name field and click on Create . Return to your terminal window. Switch the current project in the command line to apps oc project apps Enable monitoring by adding the necessary label to the apps namespace. Choose one of two options to label the namespace: Use Command Line or OpenShift Console Via the command line: oc label namespace apps app-monitoring=true Example output: namespace/apps labeled Via the OpenShift console: Under the Administration tab on the left menu, click on Namespaces . Click on the menu-options for apps namespace Click on Edit Labels . Copy and paste app-monitoring=true into the text box . Click Save . Login to the image registry via the command line: Note: From below command, a session token is obtained from the value of another command oc whoami -t and used as the password to login. docker login -u openshift -p $(oc whoami -t) default-route-openshift-image-registry.apps.demo.ibmdte.net Example output: WARNING! Using --password via the CLI is insecure. Use --password-stdin. WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded Push the image to OpenShift's internal image registry via the command line, which could take up to a minute: docker push default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos Example output: The push refers to repository [default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos] 9247390b40be: Pushed 9a21ca46f8e3: Pushed b3cee8ba43fe: Pushed 1dd2f7265f58: Pushed 33b2a4ee94ff: Pushed 2b2a8abdd0c4: Pushed 91ffc437f551: Pushed 4f04e7098d96: Pushed 248016390e0a: Pushed 0fa7eb58a57c: Pushed b5489882eed9: Pushed 2fb5caadbbb0: Pushed d06182ac791b: Pushed b39b0291530b: Pushed a04c77af4b60: Pushed 479c44e860ff: Pushed fc905c23b8a3: Pushed 161ec220381b: Pushed b7b591e3443f: Pushed ccf04fbd6e19: Pushed latest: digest: sha256:56d926b7ef64ed163ff026b7b5608ae97df4630235c1d0443a32a4fc8eb35a6c size: 4513 Verify that the image is in image registry via the command line: oc get images | grep apps/cos The application image you just pushed should be listed. Example output: sha256:56d926b7ef64ed163ff026b7b5608ae97df4630235c1d0443a32a4fc8eb35a6c image-registry.openshift-image-registry.svc:5000/apps/cos@sha256:56d926b7ef64ed163ff026b7b5608ae97df4630235c1d0443a32a4fc8eb35a6c Verify that the image stream is created via the command line: oc get imagestreams Example output: NAME IMAGE REPOSITORY TAGS UPDATED cos default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos latest 2 minutes ago You may also check the image stream via the OpenShift console: Return to the OpenShift console through your Firefox browser window. Under the Builds tab in the left menu, click on Image Streams . Select project apps from the Project drop-down list. Click on cos from the list. Scroll down to the bottom to see the image that you pushed. Deploy (Hands-on) Customer Order Services application uses DB2 as its database. To deploy it to Liberty, a separate instance of the database is already pre-configured in the OpenShift cluster you are using. The database is exposed within the cluster using a Service , and the application references this database using the address of the Service . Preview what will be deployed, using the -k , or kustomize option of Openshift CLI Note: We will explain how the deployment works in a later section. oc kustomize deploy/overlay-apps Example output of yaml that will be used to deploy the application resources. apiVersion: v1 data: DB_HOST: cos-db-liberty.db.svc kind: ConfigMap metadata: name: cos-config namespace: apps --- apiVersion: v1 data: DB_PASSWORD: ZGIyaW5zdDE= DB_USER: ZGIyaW5zdDE= kind: Secret metadata: name: db-creds namespace: apps type: Opaque --- apiVersion: v1 kind: Secret metadata: name: liberty-creds namespace: apps stringData: password: admin username: admin type: Opaque --- apiVersion: openliberty.io/v1beta1 kind: OpenLibertyApplication metadata: name: cos namespace: apps spec: applicationImage: image-registry.openshift-image-registry.svc:5000/apps/cos envFrom: - configMapRef: name: cos-config - secretRef: name: db-creds expose: true livenessProbe: httpGet: path: /health/live port: 9443 scheme: HTTPS monitoring: endpoints: - basicAuth: password: key: password name: liberty-creds username: key: username name: liberty-creds interval: 5s scheme: HTTPS tlsConfig: insecureSkipVerify: true labels: app-monitoring: \"true\" pullPolicy: Always readinessProbe: httpGet: path: /health/ready port: 9443 scheme: HTTPS route: insecureEdgeTerminationPolicy: Redirect termination: reencrypt service: annotations: service.beta.openshift.io/serving-cert-secret-name: cos-tls certificateSecretRef: cos-tls port: 9443 Run the following command to deploy the yaml files: oc apply -k deploy/overlay-apps Output of deploy command: configmap/cos-config created secret/db-creds created secret/liberty-creds created openlibertyapplication.openliberty.io/cos created Verify that the route is created for your application: oc get route cos Example output: NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD cos cos-apps.apps.demo.ibmdte.net cos 9443-tcp reencrypt/Redirect None Verify that your pod from the project apps is ready: First, confirm you're at the current project apps : oc project If it's not at project apps , then switch: oc project apps Run the following command to view the pod status: oc get pod Example output of pod status: NAME READY STATUS RESTARTS AGE cos-596b4f849f-2fg4h 1/1 Running 0 18m TROUBESHOOTING: If the pod doesn't display the expected Running status (for example, after 5 minutes), then delete the pod to restart it. oc delete pod <pod name> Note: pod name is the string under NAME column from the output of `oc get pod` Access the application (Hands-on) Run the following command to get the URL of your application: echo http://$(oc get route cos --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-apps.apps.demo.ibmdte.net/CustomerOrderServicesWeb Return to your Firefox browser window and go to the URL outputted by the previous command. You'll be shown a login dialog. Login with user skywalker and password force . (The user is pre-created/registered in the basicRegistry configured in Liberty.) After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total . Close the browser. Review the application workload flow with Open Liberty Operator (Hands-on) Return to the OpenShift console through a Firefox window to view the resources on deployment. View the resources in the project openshift-operators : a. Select the openshift-operators project from the project drop down menu at the top of the page. b. View the operator's deployment details: Click on the Deployments tab under Workloads from the left menu and select open-liberty-operator Navigate to the YAML tab to view the content of yaml c. View the operator's pod details: Click on the Pods tab under Workloads from the left menu, and select the pod starting with open-liberty-operator Navigate to the Logs tab to view the open-liberty-operator container log Navigate to the Terminal tab to view the files in the container View the resources in the project apps : a. Select the apps project from the project drop down menu at the top of the page. b. View Open Liberty Application instance details: Click on the Installed Operators tab under Operators from the left menu and select Open Liberty Operator . Note: The operator is installed at cluster level and is visible to all existing projects, but Open Liberty Application instance is created under the project apps . Navigate to the Open Liberty Application tab and select cos to view the details of the Open Liberty Application instance Navigate to the YAML tab to view the content of yaml Navigate to the Resources tab to view the resources of Open Liberty Application instance c. View application deployment details: Click on the Deployments tab under Workloads from the left menu and select cos Navigate to the YAML tab to view the content of yaml. Note: the deployment is created through the controller of the OpenLibertyApplication custom resource. d. View application pod details: Click on the Pods tab under Workloads from the left menu and select the pod starting with cos- Navigate to the Logs tab to view the liberty access log Note: by default, the Open Liberty Application instance is configured with liberty access log: e. View application service details: Click on the Services tab under Networking from the left menu and select cos Navigate to the YAML tab to view the content of yaml. **Note** the service is created through the controller of OpenLibertyApplication custom resource. f. View application route details: Click on the Routes tab under Networking from the left menu and select cos Navigate to the YAML tab to view the content of yaml. **Note** the route is created through the controller of OpenLibertyApplication custom resource. g. View application secret details: First, return to the YAML of the Open Liberty Application instance to view the configured secrets: Click on the Secrets tab under Workloads from the left menu and select the respective secrets and view the details View the resources in the project db : a. Select the db project from the project drop down menu. b. View db deployment details: Click on the Deployments tab under Workloads from the left menu and select cos-db-liberty Navigate to the YAML tab to view the content of yaml c. View db pod details: Click on the Pods tab under Workloads from the left menu and select the pod starting with cos-db-liberty Navigate to the Logs tab to view the database logs Navigate to the Terminal tab to view the files in the database container d. View db service details: Click on the Services tab under Networking from the left menu and select cos-db-liberty Review Deployment Let's review the configuration files used for our deployment. Our configuration files are structured for the -k , or kustomize option of Openshift CLI. Kustomize is a separate tool that has been integrated into Openshift CLI that allows you to customize yaml without using variables. You can define a base directory and override directories to customize the base directory Make sure you are in directory /openshift-workshop-was/labs/Openshift/RuntimeModernization Use the following command to change directory with cd openshift-workshop-was/labs/Openshift/RuntimeModernization List the deploy files: ls deploy And the output shows that we have one \"base directory\" and one \"override directory\": base overlay-apps Take a look at what's in the base directory: ls deploy/base And the output: kustomization.yaml olapp-cos.yaml Each directory used for kustomization contains one kustomization.yaml cat deploy/base/kustomization.yaml Content of kustomization.yaml: This is a simple kustomization directory that just lists the resources (yaml files) to be deployed. resources: - olapp-cos.yaml The olapp-cos.yaml file contains the Open Liberty custom resource definition to deploy the application and will be covered in detail later. Take a look at the files in the overlay-apps directory. ls deploy/overlay-apps There is the kustomization.yaml file, two secrets, and a configmap: configmap.yaml kustomization.yaml secret-db-creds.yaml secret-liberty-creds.yaml Take a look at the kustomization.yaml in the overlay-apps directory: cat deploy/overlay-apps/kustomization.yaml And the output: namespace: apps resources: - configmap.yaml - secret-db-creds.yaml - secret-liberty-creds.yaml bases: - ./../base Note that: The namespace is defined. This means that all resources originating from this overlay-apps directory will be applied to the apps namespace. Resources from the base directory will also be included. The configurations in this directory contain the overrides specific to this environment. For a real environment, DO NOT store the secret yamls into source control. It is a security expsoure. See extra credit section on how to secure your secrets. Note: You may define additional overlay directories for different environments, each with a different namespace. For example, overlay-staging, overlay-prod. To preview the resources that will be applied for a specific override directory, use the kustomize option of the Openshift command line. oc kustomize deploy/overlay-apps The output is the same as displayed in Deploy (Hands-on) section. Secrets Specifying credentials and tokens in plain text is not secure. Secrets are used to store sensitive information. The stored data can be referenced by other resources. OpenShift handles secrets with special care. For example, they will not be logged or shown anywhere. There are two secrets used for this application secret-db-creds.yaml for the DB2 database credentials secret-liberty-creds.yaml or Liberty metrics credentials, which is needed to access the /metrics endpoint Now, View the contents of the secrets in overlay-apps directory View the content of secret-db-creds.yaml file: The file secret-db-creds.yaml contains the credentials for the database. It is injected into the container as an environment variable via the secretRef specification for the Open Liberty Operator. cat deploy/overlay-apps/secret-db-creds.yaml Example output: kind: Secret apiVersion: v1 metadata: name: db-creds data: DB_PASSWORD: ZGIyaW5zdDE= DB_USER: ZGIyaW5zdDE= type: Opaque View the content of secret-liberty-creds.yaml file: The file secret-liberty-creds.yaml contains the secret to access liberty server. cat deploy/overlay-apps/secret-liberty-creds.yaml Example output: kind: Secret apiVersion: v1 metadata: name: liberty-creds stringData: username: admin password: admin type: Opaque Note that the first Secret provides the credentials in base64 encoded format using the data field. The second one provides them in plain text using the stringData field. OpenShift will automatically convert the credentials to base64 format and place the information under the data field. We can see this by viewing the YAML of the liberty-creds secret from the OpenShift console: View the YAML of the liberty-creds secret from the OpenShift console: a. Return to the OpenShift console through a Firefox browser window. b. Select the apps project from the project drop down menu. c. Click on the Secrets tab under Workloads from the left menu and search for the liberty-creds secret. d. Navigate to the YAML tab. Note that the data field contains the credentials in encoded form. Configmap Configmaps allows you to store name/value pairs that can be injected into your container when it starts. For our example, the values of the configmap.yaml are injected as environment variables in the configMapRef specification on the Open Liberty Operator as described in the next section. View the configmap in the overlay-apps directory cat deploy/overlay-apps/configmap.yaml Example output: apiVersion: v1 kind: ConfigMap metadata: name: cos-config data: SEC_TLS_TRUSTDEFAULTCERTS: \"true\" DB_HOST : \"cos-db-liberty.db.svc\" Open Liberty Operator We could have created Deployment, Service, and Route resources to deploy the Liberty image. However, for this lab we use the Open Liberty Operator instead. The Open Liberty Operator provides all functionalities of Runtime Component Operator used when deploying traditional WebSphere images in a previous lab. In addition, it also offers Open Liberty specific capabilities, such as day-2 operations (gather trace & dumps) and single sign-on (SSO). The olapp-cos.yaml file is the OpenLiberty Operator custom resource used to deploy the cos appicatoin in this lab. Let's explore the OpenLiberty Operator resource definitions. The file deploy/base/olapp-cos.yaml defines the OpenLibertyApplication custom resource. apiVersion: openliberty.io/v1beta1 kind: OpenLibertyApplication metadata: name: cos namespace: apps spec: applicationImage: 'image-registry.openshift-image-registry.svc:5000/apps/cos' pullPolicy: Always readinessProbe: httpGet: path: /health/ready port: 9443 scheme: HTTPS livenessProbe: httpGet: path: /health/live port: 9443 scheme: HTTPS service: annotations: service.beta.openshift.io/serving-cert-secret-name: cos-tls certificateSecretRef: cos-tls port: 9443 expose: true route: termination: reencrypt insecureEdgeTerminationPolicy: Redirect env: envFrom: - configMapRef: name: cos-config - secretRef: name: db-creds monitoring: endpoints: - basicAuth: password: key: password name: liberty-creds username: key: username name: liberty-creds interval: 5s scheme: HTTPS tlsConfig: insecureSkipVerify: true labels: app-monitoring: 'true' The OpenLibertyApplication is a custom resource supported by the Open Liberty Operator, which is designed to help you with Liberty deployment. It allows you to provide Liberty specific configurations (day-2 operations, single sign-on). The application image you pushed earlier to internal image registry is specified by the applicationImage parameter. MicroProfile Health endpoints /health/ready and /health/live are used for readiness and liveness probes. The configMapRef surfaces all entries of the ConfigMap cos-config as environment variables. The secretRef surfaces the entries in the Secret db-creds as environment variables. These are the database user and password. Enabled application monitoring so that Prometheus can scrape the information provided by MicroProfile Metric's /metrics endpoint in Liberty. The /metrics endpoint is protected, hence the credentials are provided using the Secret liberty-creds you created earlier. Cleanup (Hands-on) (Skip this step if you're going to run the next lab Application Management on the same assigned cluster) The controller for the Open Liberty Operator creates the necessary Deployment, Service, and Route objects for the Customer Order Services application. To list these resources, run the commands: Reminder: Run oc project apps to confirm you are at the apps project before running the following commands. oc get deployment oc get service oc get route Example output: # oc get deployment NAME READY UP-TO-DATE AVAILABLE AGE cos 1/1 1 1 2d18h # oc get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cos ClusterIP 172.21.202.9 <none> 9443/TCP 2d18h # oc get route NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD cos cos-apps.apps.demo.ibmdte.net cos 9443-tcp reencrypt/Redirect None To remove these resources, run the commands a. Ensure you are in directory openshift-workshop-was/labs/Openshift/RuntimeModernization cd openshift-workshop-was/labs/Openshift/RuntimeModernization Note: The pre-installed resources such as Open Liberty Operator and DB2, are not removed. b. delete the resouces oc delete -k deploy/overlay-apps Output: configmap \"cos-config\" deleted secret \"db-creds\" deleted secret \"liberty-creds\" deleted openlibertyapplication.openliberty.io \"cos\" deleted c. Double check the corresponding Deployment, Service, and Route objects are deleted: oc get deployment oc get service oc get route Output from each get command above: No resources found in apps namespace. Summary Congratulations! You've completed Runtime Modernization lab! This application has been modified from the initial WebSphere ND v8.5.5 version to run on modern & cloud-native runtime Open Liberty and deployed by IBM Cloud Pak for Applications to RedHat OpenShift. Next Follow the link to the next lab Application Management : - Application Management","title":"Runtime Modernization"},{"location":"appmod-labs/RuntimeModernization/#runtime-modernization","text":"","title":"Runtime Modernization"},{"location":"appmod-labs/RuntimeModernization/#table-of-contents","text":"Introduction Analysis (Hands-on) Build (Hands-on) Deploy (Hands-on) Access the Application (Hands-on) Review Deployment Cleanup (can be skipped if the next lab Application Management is included in the same session) Extra Credit Summary Next","title":"Table of Contents"},{"location":"appmod-labs/RuntimeModernization/#introduction","text":"Runtime modernization moves an application to a 'built for the cloud' runtime with the least amount of effort. Open Liberty is a fast, dynamic, and easy-to-use Java application server. Ideal for the cloud, Liberty is open sourced, with fast start-up times (<2 seconds), no server restarts to pick up changes, and a simple XML configuration. This path gets the application on to a cloud-ready runtime container which is easy to use and portable. In addition to the necessary library changes, some aspects of the application were modernized. However, it has not been 'modernized' to a newer architecture such as micro-services . This lab demonstrates runtime modernization . It uses the Customer Order Services application, which originates from WebSphere ND V8.5.5. Click here to get to know the application, including its architecture and components. The application will go through the analysis , build and deploy phases. It is modernized to run on the Liberty runtime, and deployed via the IBM Cloud Pak for Applications to RedHat OpenShift Container Platform (OCP).","title":"Introduction"},{"location":"appmod-labs/RuntimeModernization/#login-to-the-vm","text":"If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password: Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window.","title":"Login to the VM"},{"location":"appmod-labs/RuntimeModernization/#analysis-hands-on","text":"In this lab, we will demonstrate how the Transformation Advisor can be used specifically in the runtime modernization process, using Liberty in containers on OpenShift. The steps needed to analyze the existing Customer Order Services application are: Open a Firefox browser window from within the VM. Click on the openshift console bookmark in the top left and log in with the htpasswd option. Log in to the OpenShift account using the following credentials: Username: ibmadmin Password: engageibm From the Red Hat OpenShift Container Platform console, go to the Networking tab and click on Routes . Ensure that you are in the ta project by using the project drop down and click on the Location URL next to ta-ui-route . This will open the Transformation Advisor user interface. Click Create new under Workspaces to create a new workspace. Name it RuntimeModernization and click Next . You'll be asked to create a new collection to store the data collected from the Customer Order Services application. Name the new collection CustomerOrderServices . Click Create . To provide application assessment data and receive recommendations, you would typically download and execute the Data Collector against an existing WebSphere environment. The output from the data collector is a zip file containing the application and configuration metadata gathered from the WebSphere Server, and is the input to the IBM Transformation Advsor tool. However, for this lab, the data collection archive has already been created for you and the resulting data is stored here . Click the Upload button, as we already have the data collectin archive to upload. Upload the results of the data collection (the datacollector.zip file) to IBM Cloud Transformation Advisor. When the upload is complete, you will see a list of applications analyzed from the source environment. At the top of the page, you can see the source environment and the target environment settings. Under the Migration target field, click the down arrow and select Compatible runtimes . This shows an entry for each application for each compatible destination runtime you can migrate it to. Click the CustomerOrderServicesApp.ear application with the Open Liberty migration target to open the Application details page . This lab covers runtime modernization, so the application will be re-platformed to run on Open Liberty, and will be placed in a container and deployed to OCP. Look over the migration analysis which shows a summary of the complexity of migrating this application to this target. In summary, migration of this application to Open Liberty is of Moderate complexity as some code changes may be required.","title":"Analysis (Hands-on)"},{"location":"appmod-labs/RuntimeModernization/#note-there-may-be-a-severe-issue-related-to-third-party-apis-but-this-doesnt-apply-as-they-occur-in-test-code","text":"Click on View migration plan in the top right corner of the page. This page will help you assemble an archive containing the following resources: your application's source or binary files (you upload these here or specify Maven coordinates to download them) a Dockerfile to build the Liberty container image with the application installed (generated by Transformation Advisor and automatically included) a jvm.options file that is used for configuring the Liberty runetime (generated by Transformation Advisor and automatically included) a server.xml file to configure the Liberty server for the application (generated by Transformation Advisor and automatically included) a pom.xml file that is used to build the applcation binaries (Jar, WAR, EAR) (generated by Transformation Advisor and automatically included) the deployment artifacts needed to create the container image and deploy the application to OCP (generated by Transformation Advisor and automatically included) Note: These artifacts have already been provided for you as part of the lab files, so you don't need to download the migration plan. However, you can do so if you wish to look around at the files. These files can also be sent to a Git repository by Transformation Advisor.","title":"Note: There may be a severe issue related to third-party APIs, but this doesn't apply as they occur in test code."},{"location":"appmod-labs/RuntimeModernization/#build-hands-on","text":"In this section, you'll learn how to build a Docker image for Customer Order Services application running on Liberty. Building this image could take around ~3 minutes so let's kick that process off and then come back to learn what you did. Open a new terminal window from the VM desktop. Login to OpenShift CLI with the oc login command from the web terminal. When prompted for the username and password, enter the following login credentials: Username: ibmadmin Password: engageibm If you have not yet cloned the GitHub repo with the lab artifacts, run the following command on your terminal: git clone https://github.com/IBM/openshift-workshop-was.git Change directory to where this lab is located: cd openshift-workshop-was cd labs/Openshift/RuntimeModernization ls 5 Run the following command to start building the image. Make sure to copy the entire command, including the \".\" at the end (indicated as the location of current directory). While the image is building (which takes ~3 minutes), continue with rest of the lab: docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos .","title":"Build (Hands-on)"},{"location":"appmod-labs/RuntimeModernization/#modernize-with-microprofile-for-reading-only","text":"Eclipse MicroProfile is a modular set of technologies designed so that you can write cloud-native microservices. However, even though our application has not been refactored into microservices, we can still take advantage of some of the technologies from MicroProfile.","title":"Modernize with MicroProfile (for reading only)"},{"location":"appmod-labs/RuntimeModernization/#determine-applications-availability-for-reading-only","text":"In the last lab, we used /CustomerOrderServicesWeb/index.html for readiness and liveness probes, which is not the best indication that an application is ready to handle traffic or is healthy to process requests correctly within a reasonable amount of time. What if the database is down? What if the application's security layer is not yet ready/unable to handle authentication? The Pod would still be considered ready and healthy and traffic would still be sent to it. All of those requests would fail or queue up, leading to bigger problems. MicroProfile Health provides a common REST endpoint format to determine whether a microservice (or in our case a monolith application) is healthy or not. The service itself might be running but considered unhealthy if the things it requires for normal operation are unavailable. All of the checks are performed periodically, and the result is served as a simple UP or DOWN at /health/ready and /health/live which can be used for readiness and liveness probes. We implemented the following health checks: ReadinessCheck : The application reports it is ready as long as the readiness check endpoint can be reached. Connections to other services and any other required conditions for the application to be considered ready are checked here. return HealthCheckResponse.named(\"Readiness\").up().build(); LivenessCheck : The requests should be processed within a reasonable amount of time. Monitor thread block times to identify potential deadlocks which can cause the application to hang. ThreadMXBean tBean = ManagementFactory.getThreadMXBean(); long ids[] = tBean.findMonitorDeadlockedThreads(); if (ids !=null) { ThreadInfo threadInfos[] = tBean.getThreadInfo(ids); for (ThreadInfo ti : threadInfos) { double seconds = ti.getBlockedTime() / 1000.0; if (seconds > 60) { return HealthCheckResponse.named(\"Liveness\").down().build(); } } } return HealthCheckResponse.named(\"Liveness\").up().build();","title":"Determine application's availability (for reading only)"},{"location":"appmod-labs/RuntimeModernization/#adding-metrics-to-application-for-reading-only","text":"MicroProfile Metrics is used to gather metrics about the time it takes to add an item to cart, retrieve customer information, and count the number of times these operations are performed. @GET @Produces(MediaType.APPLICATION_JSON) @Counted @Timed(name = \"getCustomer_timed\") public Response getCustomer() {","title":"Adding metrics to application (for reading only)"},{"location":"appmod-labs/RuntimeModernization/#liberty-server-configuration-for-reading-only","text":"The Liberty runtime configuration files are based on a template provided by IBM Cloud Transformation Advisor. For this lab, instead of using a single server.xml, the configurations have been split into multiple configuration files and placed into the config/configDropins/overrides directory. You may place configuration files into configDropins/overrides directory to override pre-existing configurations. You may define separate template configurations that reflect the resources in your environment, and copy them into the configDropsins/overrides directory only for those applications that need them.","title":"Liberty server configuration (for reading only)"},{"location":"appmod-labs/RuntimeModernization/#build-image-hands-on","text":"Go back to your terminal to check the build you started earlier. You should see the following message if the image was built successfully. Please wait if it's still building: Successfully tagged default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos:latest Validate that the image is in the repository via the command line: docker images You should see the following images on the output. Notice that the base image, openliberty/open-liberty , is also listed. It was pulled as the first step of building application image. Example output: REPOSITORY TAG IMAGE ID CREATED SIZE default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos latest 4758119add3f 2 minutes ago 883MB <none> <none> 5bcb83fad548 5 minutes ago 792MB openliberty/open-liberty full-java8-openj9-ubi e6b5411076fe 5 days ago 794MB maven latest 1337d55397f7 4 weeks ago 631MB Before we push the image to OpenShift's internal image registry, create a separate project named apps . Choose one of two ways to create the project, command line or OpenShift console : Via the command line: oc new-project apps Example output: Now using project \"apps\" on server \"https://api.demo.ibmdte.net:64 . . . Via the OpenShift console: Open a Firefox browser window and click on the openshift console bookmark. Under the Home tab on the left menu, click Projects . Click on the Create Project button. Enter apps for the Name field and click on Create . Return to your terminal window. Switch the current project in the command line to apps oc project apps Enable monitoring by adding the necessary label to the apps namespace. Choose one of two options to label the namespace: Use Command Line or OpenShift Console Via the command line: oc label namespace apps app-monitoring=true Example output: namespace/apps labeled Via the OpenShift console: Under the Administration tab on the left menu, click on Namespaces . Click on the menu-options for apps namespace Click on Edit Labels . Copy and paste app-monitoring=true into the text box . Click Save . Login to the image registry via the command line: Note: From below command, a session token is obtained from the value of another command oc whoami -t and used as the password to login. docker login -u openshift -p $(oc whoami -t) default-route-openshift-image-registry.apps.demo.ibmdte.net Example output: WARNING! Using --password via the CLI is insecure. Use --password-stdin. WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded Push the image to OpenShift's internal image registry via the command line, which could take up to a minute: docker push default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos Example output: The push refers to repository [default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos] 9247390b40be: Pushed 9a21ca46f8e3: Pushed b3cee8ba43fe: Pushed 1dd2f7265f58: Pushed 33b2a4ee94ff: Pushed 2b2a8abdd0c4: Pushed 91ffc437f551: Pushed 4f04e7098d96: Pushed 248016390e0a: Pushed 0fa7eb58a57c: Pushed b5489882eed9: Pushed 2fb5caadbbb0: Pushed d06182ac791b: Pushed b39b0291530b: Pushed a04c77af4b60: Pushed 479c44e860ff: Pushed fc905c23b8a3: Pushed 161ec220381b: Pushed b7b591e3443f: Pushed ccf04fbd6e19: Pushed latest: digest: sha256:56d926b7ef64ed163ff026b7b5608ae97df4630235c1d0443a32a4fc8eb35a6c size: 4513 Verify that the image is in image registry via the command line: oc get images | grep apps/cos The application image you just pushed should be listed. Example output: sha256:56d926b7ef64ed163ff026b7b5608ae97df4630235c1d0443a32a4fc8eb35a6c image-registry.openshift-image-registry.svc:5000/apps/cos@sha256:56d926b7ef64ed163ff026b7b5608ae97df4630235c1d0443a32a4fc8eb35a6c Verify that the image stream is created via the command line: oc get imagestreams Example output: NAME IMAGE REPOSITORY TAGS UPDATED cos default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos latest 2 minutes ago You may also check the image stream via the OpenShift console: Return to the OpenShift console through your Firefox browser window. Under the Builds tab in the left menu, click on Image Streams . Select project apps from the Project drop-down list. Click on cos from the list. Scroll down to the bottom to see the image that you pushed.","title":"Build image (Hands-on)"},{"location":"appmod-labs/RuntimeModernization/#deploy-hands-on","text":"Customer Order Services application uses DB2 as its database. To deploy it to Liberty, a separate instance of the database is already pre-configured in the OpenShift cluster you are using. The database is exposed within the cluster using a Service , and the application references this database using the address of the Service . Preview what will be deployed, using the -k , or kustomize option of Openshift CLI Note: We will explain how the deployment works in a later section. oc kustomize deploy/overlay-apps Example output of yaml that will be used to deploy the application resources. apiVersion: v1 data: DB_HOST: cos-db-liberty.db.svc kind: ConfigMap metadata: name: cos-config namespace: apps --- apiVersion: v1 data: DB_PASSWORD: ZGIyaW5zdDE= DB_USER: ZGIyaW5zdDE= kind: Secret metadata: name: db-creds namespace: apps type: Opaque --- apiVersion: v1 kind: Secret metadata: name: liberty-creds namespace: apps stringData: password: admin username: admin type: Opaque --- apiVersion: openliberty.io/v1beta1 kind: OpenLibertyApplication metadata: name: cos namespace: apps spec: applicationImage: image-registry.openshift-image-registry.svc:5000/apps/cos envFrom: - configMapRef: name: cos-config - secretRef: name: db-creds expose: true livenessProbe: httpGet: path: /health/live port: 9443 scheme: HTTPS monitoring: endpoints: - basicAuth: password: key: password name: liberty-creds username: key: username name: liberty-creds interval: 5s scheme: HTTPS tlsConfig: insecureSkipVerify: true labels: app-monitoring: \"true\" pullPolicy: Always readinessProbe: httpGet: path: /health/ready port: 9443 scheme: HTTPS route: insecureEdgeTerminationPolicy: Redirect termination: reencrypt service: annotations: service.beta.openshift.io/serving-cert-secret-name: cos-tls certificateSecretRef: cos-tls port: 9443 Run the following command to deploy the yaml files: oc apply -k deploy/overlay-apps Output of deploy command: configmap/cos-config created secret/db-creds created secret/liberty-creds created openlibertyapplication.openliberty.io/cos created Verify that the route is created for your application: oc get route cos Example output: NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD cos cos-apps.apps.demo.ibmdte.net cos 9443-tcp reencrypt/Redirect None Verify that your pod from the project apps is ready: First, confirm you're at the current project apps : oc project If it's not at project apps , then switch: oc project apps Run the following command to view the pod status: oc get pod Example output of pod status: NAME READY STATUS RESTARTS AGE cos-596b4f849f-2fg4h 1/1 Running 0 18m TROUBESHOOTING: If the pod doesn't display the expected Running status (for example, after 5 minutes), then delete the pod to restart it. oc delete pod <pod name> Note: pod name is the string under NAME column from the output of `oc get pod`","title":"Deploy (Hands-on)"},{"location":"appmod-labs/RuntimeModernization/#access-the-application-hands-on","text":"Run the following command to get the URL of your application: echo http://$(oc get route cos --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-apps.apps.demo.ibmdte.net/CustomerOrderServicesWeb Return to your Firefox browser window and go to the URL outputted by the previous command. You'll be shown a login dialog. Login with user skywalker and password force . (The user is pre-created/registered in the basicRegistry configured in Liberty.) After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total . Close the browser.","title":"Access the application (Hands-on)"},{"location":"appmod-labs/RuntimeModernization/#review-the-application-workload-flow-with-open-liberty-operator-hands-on","text":"Return to the OpenShift console through a Firefox window to view the resources on deployment. View the resources in the project openshift-operators : a. Select the openshift-operators project from the project drop down menu at the top of the page. b. View the operator's deployment details: Click on the Deployments tab under Workloads from the left menu and select open-liberty-operator Navigate to the YAML tab to view the content of yaml c. View the operator's pod details: Click on the Pods tab under Workloads from the left menu, and select the pod starting with open-liberty-operator Navigate to the Logs tab to view the open-liberty-operator container log Navigate to the Terminal tab to view the files in the container View the resources in the project apps : a. Select the apps project from the project drop down menu at the top of the page. b. View Open Liberty Application instance details: Click on the Installed Operators tab under Operators from the left menu and select Open Liberty Operator . Note: The operator is installed at cluster level and is visible to all existing projects, but Open Liberty Application instance is created under the project apps . Navigate to the Open Liberty Application tab and select cos to view the details of the Open Liberty Application instance Navigate to the YAML tab to view the content of yaml Navigate to the Resources tab to view the resources of Open Liberty Application instance c. View application deployment details: Click on the Deployments tab under Workloads from the left menu and select cos Navigate to the YAML tab to view the content of yaml. Note: the deployment is created through the controller of the OpenLibertyApplication custom resource. d. View application pod details: Click on the Pods tab under Workloads from the left menu and select the pod starting with cos- Navigate to the Logs tab to view the liberty access log Note: by default, the Open Liberty Application instance is configured with liberty access log: e. View application service details: Click on the Services tab under Networking from the left menu and select cos Navigate to the YAML tab to view the content of yaml. **Note** the service is created through the controller of OpenLibertyApplication custom resource. f. View application route details: Click on the Routes tab under Networking from the left menu and select cos Navigate to the YAML tab to view the content of yaml. **Note** the route is created through the controller of OpenLibertyApplication custom resource. g. View application secret details: First, return to the YAML of the Open Liberty Application instance to view the configured secrets: Click on the Secrets tab under Workloads from the left menu and select the respective secrets and view the details View the resources in the project db : a. Select the db project from the project drop down menu. b. View db deployment details: Click on the Deployments tab under Workloads from the left menu and select cos-db-liberty Navigate to the YAML tab to view the content of yaml c. View db pod details: Click on the Pods tab under Workloads from the left menu and select the pod starting with cos-db-liberty Navigate to the Logs tab to view the database logs Navigate to the Terminal tab to view the files in the database container d. View db service details: Click on the Services tab under Networking from the left menu and select cos-db-liberty","title":"Review the application workload flow with Open Liberty Operator (Hands-on)"},{"location":"appmod-labs/RuntimeModernization/#review-deployment","text":"Let's review the configuration files used for our deployment. Our configuration files are structured for the -k , or kustomize option of Openshift CLI. Kustomize is a separate tool that has been integrated into Openshift CLI that allows you to customize yaml without using variables. You can define a base directory and override directories to customize the base directory Make sure you are in directory /openshift-workshop-was/labs/Openshift/RuntimeModernization Use the following command to change directory with cd openshift-workshop-was/labs/Openshift/RuntimeModernization List the deploy files: ls deploy And the output shows that we have one \"base directory\" and one \"override directory\": base overlay-apps Take a look at what's in the base directory: ls deploy/base And the output: kustomization.yaml olapp-cos.yaml Each directory used for kustomization contains one kustomization.yaml cat deploy/base/kustomization.yaml Content of kustomization.yaml: This is a simple kustomization directory that just lists the resources (yaml files) to be deployed. resources: - olapp-cos.yaml The olapp-cos.yaml file contains the Open Liberty custom resource definition to deploy the application and will be covered in detail later. Take a look at the files in the overlay-apps directory. ls deploy/overlay-apps There is the kustomization.yaml file, two secrets, and a configmap: configmap.yaml kustomization.yaml secret-db-creds.yaml secret-liberty-creds.yaml Take a look at the kustomization.yaml in the overlay-apps directory: cat deploy/overlay-apps/kustomization.yaml And the output: namespace: apps resources: - configmap.yaml - secret-db-creds.yaml - secret-liberty-creds.yaml bases: - ./../base Note that: The namespace is defined. This means that all resources originating from this overlay-apps directory will be applied to the apps namespace. Resources from the base directory will also be included. The configurations in this directory contain the overrides specific to this environment. For a real environment, DO NOT store the secret yamls into source control. It is a security expsoure. See extra credit section on how to secure your secrets. Note: You may define additional overlay directories for different environments, each with a different namespace. For example, overlay-staging, overlay-prod. To preview the resources that will be applied for a specific override directory, use the kustomize option of the Openshift command line. oc kustomize deploy/overlay-apps The output is the same as displayed in Deploy (Hands-on) section.","title":"Review Deployment"},{"location":"appmod-labs/RuntimeModernization/#secrets","text":"Specifying credentials and tokens in plain text is not secure. Secrets are used to store sensitive information. The stored data can be referenced by other resources. OpenShift handles secrets with special care. For example, they will not be logged or shown anywhere. There are two secrets used for this application secret-db-creds.yaml for the DB2 database credentials secret-liberty-creds.yaml or Liberty metrics credentials, which is needed to access the /metrics endpoint Now, View the contents of the secrets in overlay-apps directory View the content of secret-db-creds.yaml file: The file secret-db-creds.yaml contains the credentials for the database. It is injected into the container as an environment variable via the secretRef specification for the Open Liberty Operator. cat deploy/overlay-apps/secret-db-creds.yaml Example output: kind: Secret apiVersion: v1 metadata: name: db-creds data: DB_PASSWORD: ZGIyaW5zdDE= DB_USER: ZGIyaW5zdDE= type: Opaque View the content of secret-liberty-creds.yaml file: The file secret-liberty-creds.yaml contains the secret to access liberty server. cat deploy/overlay-apps/secret-liberty-creds.yaml Example output: kind: Secret apiVersion: v1 metadata: name: liberty-creds stringData: username: admin password: admin type: Opaque Note that the first Secret provides the credentials in base64 encoded format using the data field. The second one provides them in plain text using the stringData field. OpenShift will automatically convert the credentials to base64 format and place the information under the data field. We can see this by viewing the YAML of the liberty-creds secret from the OpenShift console: View the YAML of the liberty-creds secret from the OpenShift console: a. Return to the OpenShift console through a Firefox browser window. b. Select the apps project from the project drop down menu. c. Click on the Secrets tab under Workloads from the left menu and search for the liberty-creds secret. d. Navigate to the YAML tab. Note that the data field contains the credentials in encoded form.","title":"Secrets"},{"location":"appmod-labs/RuntimeModernization/#configmap","text":"Configmaps allows you to store name/value pairs that can be injected into your container when it starts. For our example, the values of the configmap.yaml are injected as environment variables in the configMapRef specification on the Open Liberty Operator as described in the next section. View the configmap in the overlay-apps directory cat deploy/overlay-apps/configmap.yaml Example output: apiVersion: v1 kind: ConfigMap metadata: name: cos-config data: SEC_TLS_TRUSTDEFAULTCERTS: \"true\" DB_HOST : \"cos-db-liberty.db.svc\"","title":"Configmap"},{"location":"appmod-labs/RuntimeModernization/#open-liberty-operator","text":"We could have created Deployment, Service, and Route resources to deploy the Liberty image. However, for this lab we use the Open Liberty Operator instead. The Open Liberty Operator provides all functionalities of Runtime Component Operator used when deploying traditional WebSphere images in a previous lab. In addition, it also offers Open Liberty specific capabilities, such as day-2 operations (gather trace & dumps) and single sign-on (SSO). The olapp-cos.yaml file is the OpenLiberty Operator custom resource used to deploy the cos appicatoin in this lab. Let's explore the OpenLiberty Operator resource definitions. The file deploy/base/olapp-cos.yaml defines the OpenLibertyApplication custom resource. apiVersion: openliberty.io/v1beta1 kind: OpenLibertyApplication metadata: name: cos namespace: apps spec: applicationImage: 'image-registry.openshift-image-registry.svc:5000/apps/cos' pullPolicy: Always readinessProbe: httpGet: path: /health/ready port: 9443 scheme: HTTPS livenessProbe: httpGet: path: /health/live port: 9443 scheme: HTTPS service: annotations: service.beta.openshift.io/serving-cert-secret-name: cos-tls certificateSecretRef: cos-tls port: 9443 expose: true route: termination: reencrypt insecureEdgeTerminationPolicy: Redirect env: envFrom: - configMapRef: name: cos-config - secretRef: name: db-creds monitoring: endpoints: - basicAuth: password: key: password name: liberty-creds username: key: username name: liberty-creds interval: 5s scheme: HTTPS tlsConfig: insecureSkipVerify: true labels: app-monitoring: 'true' The OpenLibertyApplication is a custom resource supported by the Open Liberty Operator, which is designed to help you with Liberty deployment. It allows you to provide Liberty specific configurations (day-2 operations, single sign-on). The application image you pushed earlier to internal image registry is specified by the applicationImage parameter. MicroProfile Health endpoints /health/ready and /health/live are used for readiness and liveness probes. The configMapRef surfaces all entries of the ConfigMap cos-config as environment variables. The secretRef surfaces the entries in the Secret db-creds as environment variables. These are the database user and password. Enabled application monitoring so that Prometheus can scrape the information provided by MicroProfile Metric's /metrics endpoint in Liberty. The /metrics endpoint is protected, hence the credentials are provided using the Secret liberty-creds you created earlier.","title":"Open Liberty Operator"},{"location":"appmod-labs/RuntimeModernization/#cleanup-hands-on-skip-this-step-if-youre-going-to-run-the-next-lab-application-management-on-the-same-assigned-cluster","text":"The controller for the Open Liberty Operator creates the necessary Deployment, Service, and Route objects for the Customer Order Services application. To list these resources, run the commands: Reminder: Run oc project apps to confirm you are at the apps project before running the following commands. oc get deployment oc get service oc get route Example output: # oc get deployment NAME READY UP-TO-DATE AVAILABLE AGE cos 1/1 1 1 2d18h # oc get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cos ClusterIP 172.21.202.9 <none> 9443/TCP 2d18h # oc get route NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD cos cos-apps.apps.demo.ibmdte.net cos 9443-tcp reencrypt/Redirect None To remove these resources, run the commands a. Ensure you are in directory openshift-workshop-was/labs/Openshift/RuntimeModernization cd openshift-workshop-was/labs/Openshift/RuntimeModernization Note: The pre-installed resources such as Open Liberty Operator and DB2, are not removed. b. delete the resouces oc delete -k deploy/overlay-apps Output: configmap \"cos-config\" deleted secret \"db-creds\" deleted secret \"liberty-creds\" deleted openlibertyapplication.openliberty.io \"cos\" deleted c. Double check the corresponding Deployment, Service, and Route objects are deleted: oc get deployment oc get service oc get route Output from each get command above: No resources found in apps namespace.","title":"Cleanup (Hands-on) (Skip this step if you're going to run the next lab Application Management on the same assigned cluster)"},{"location":"appmod-labs/RuntimeModernization/#summary","text":"Congratulations! You've completed Runtime Modernization lab! This application has been modified from the initial WebSphere ND v8.5.5 version to run on modern & cloud-native runtime Open Liberty and deployed by IBM Cloud Pak for Applications to RedHat OpenShift.","title":"Summary"},{"location":"appmod-labs/RuntimeModernization/#next","text":"Follow the link to the next lab Application Management : - Application Management","title":"Next"},{"location":"appmod-labs/RuntimeModernization/README - Copy/","text":"Runtime Modernization Table of Contents Introduction Analysis (Hands-on) Build (Hands-on) Deploy (Hands-on) Access the Application (Hands-on) Review Deployment Cleanup (can be skipped if the next lab Application Management is included in the same session) Extra Credit Summary Next Introduction Runtime modernization moves an application to a 'built for the cloud' runtime with the least amount of effort. Open Liberty is a fast, dynamic, and easy-to-use Java application server. Ideal for the cloud, Liberty is open sourced, with fast start-up times (<2 seconds), no server restarts to pick up changes, and a simple XML configuration. This path gets the application on to a cloud-ready runtime container which is easy to use and portable. In addition to the necessary library changes, some aspects of the application were modernized. However, it has not been 'modernized' to a newer architecture such as micro-services . This lab demonstrates runtime modernization . It uses the Customer Order Services application, which originates from WebSphere ND V8.5.5. Click here to get to know the application, including its architecture and components. The application will go through the analysis , build and deploy phases. It is modernized to run on the Liberty runtime, and deployed via the IBM Cloud Pak for Applications to RedHat OpenShift Container Platform (OCP). Login to the VM If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password: \\ Password: engageibm Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window. Analysis (Hands-on) In this lab, we will demonstrate how the Transformation Advisor can be used specifically in the runtime modernization process. The steps needed to analyze the existing Customer Order Services application are: Open a Firefox browser window from within the VM. Click on the openshift console bookmark in the top left and log in with the htpasswd option. Log in to the OpenShift account using the following credentials: Username: ibmadmin Password: engageibm From the Red Hat OpenShift Container Platform console, go to the Networking tab and click on Routes . Ensure that you are in the ta project by using the project drop down and click on the Location URL next to ta-ui-route . This will open the Transformation Advisor user interface. Click Create new under Workspaces to create a new workspace. Name it RuntimeModernization and click Next . You'll be asked to create a new collection to store the data collected from the Customer Order Services application. Name it CustomerOrderServices . Click Create . The No recommendations available page is displayed. To provide data and receive recommendations, you can either download and execute the Data Collector against an existing WebSphere environment, or upload an existing data collection archive. The archive has already been created for you and the resulting data is stored here . Upload the results of the data collection (the datacollector.zip file) to IBM Cloud Transformation Advisor. When the upload is complete, you will see a list of applications analyzed from the source environment. At the top of the page, you can see the source environment and the target environment settings. Under the Migration target field, click the down arrow and select Compatible runtimes . This shows an entry for each application for each compatible destination runtime you can migrate it to. Click the CustomerOrderServicesApp.ear application with the Open Liberty migration target to open the Application details page . This lab covers runtime modernization, so the application will be re-platformed to run on Open Liberty, and will be placed in a container and deployed to OCP. Look over the migration analysis which shows a summary of the complexity of migrating this application to this target. In summary, migration of this application to Open Liberty is of Moderate complexity as some code changes may be required. (Note: there may be a severe issue related to third-party APIs, but this doesn't apply as they occur in test code.) Click on View migration plan in the top right corner of the page. This page will help you assemble an archive containing: - your application's source or binary files (you upload these here or specify Maven coordinates to download them) - the wsadmin scripts needed to configure your application and its resources (generated by Transformation Advisor and automatically included) - the deployment artifacts needed to create the container image and deploy the application to OCP (generated by Transformation Advisor and automatically included) NOTE: These artifacts have already been provided for you as part of the lab files, so you don't need to download the migration plan. However, you can do so if you wish to look around at the files. These files can also be sent to a Git repository by Transformation Advisor. Build (Hands-on) In this section, you'll learn how to build a Docker image for Customer Order Services application running on Liberty. Building this image could take around ~3 minutes so let's kick that process off and then come back to learn what you did. Open a new terminal window from the VM desktop. Login to OpenShift CLI with the oc login command from the web terminal. When prompted for the username and password, enter the following login credentials: Username: ibmadmin Password: engageibm If you have not yet cloned the GitHub repo with the lab artifacts, run the following command on your terminal: git clone https://github.com/IBM/openshift-workshop-was.git Change directory to where this lab is located: cd openshift-workshop-was cd labs/Openshift/RuntimeModernization ls Run the following command to start building the image. Make sure to copy the entire command, including the \".\" at the end (indicated as the location of current directory). While the image is building (which takes ~3 minutes), continue with rest of the lab: docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos . Modernize with MicroProfile (for reading only) Eclipse MicroProfile is a modular set of technologies designed so that you can write cloud-native microservices. However, even though our application has not been refactored into microservices, we can still take advantage of some of the technologies from MicroProfile. Determine application's availability (for reading only) In the last lab, we used /CustomerOrderServicesWeb/index.html for readiness and liveness probes, which is not the best indication that an application is ready to handle traffic or is healthy to process requests correctly within a reasonable amount of time. What if the database is down? What if the application's security layer is not yet ready/unable to handle authentication? The Pod would still be considered ready and healthy and traffic would still be sent to it. All of those requests would fail or queue up, leading to bigger problems. MicroProfile Health provides a common REST endpoint format to determine whether a microservice (or in our case a monolith application) is healthy or not. The service itself might be running but considered unhealthy if the things it requires for normal operation are unavailable. All of the checks are performed periodically, and the result is served as a simple UP or DOWN at /health/ready and /health/live which can be used for readiness and liveness probes. We implemented the following health checks: - ReadinessCheck : The application reports it is ready as long as the readiness check endpoint can be reached. Connections to other services and any other required conditions for the application to be considered ready are checked here. return HealthCheckResponse.named(\"Readiness\").up().build(); LivenessCheck : The requests should be processed within a reasonable amount of time. Monitor thread block times to identify potential deadlocks which can cause the application to hang. ThreadMXBean tBean = ManagementFactory.getThreadMXBean(); long ids[] = tBean.findMonitorDeadlockedThreads(); if (ids !=null) { ThreadInfo threadInfos[] = tBean.getThreadInfo(ids); for (ThreadInfo ti : threadInfos) { double seconds = ti.getBlockedTime() / 1000.0; if (seconds > 60) { return HealthCheckResponse.named(\"Liveness\").down().build(); } } } return HealthCheckResponse.named(\"Liveness\").up().build(); Adding metrics to application (for reading only) MicroProfile Metrics is used to gather metrics about the time it takes to add an item to cart, retrieve customer information, and count the number of times these operations are performed. @GET @Produces(MediaType.APPLICATION_JSON) @Counted @Timed(name = \"getCustomer_timed\") public Response getCustomer() { Liberty server configuration (for reading only) The Liberty runtime configuration files are based on a template provided by IBM Cloud Transformation Advisor. For this lab, instead of using a single server.xml, the configurations have been split into multiple configuration files and placed into the config/configDropins/overrides directory. You may place configuration files into configDropins/overrides directory to override pre-existing configurations. You may define separate template configurations that reflect the resources in your environment, and copy them into the configDropsins/overrides directory only for those applications that need them. Build image (Hands-on) Go back to your terminal to check the build you started earlier. You should see the following message if the image was built successfully. Please wait if it's still building: Successfully tagged default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos:latest Validate that the image is in the repository via the command line: docker images You should see the following images on the output. Notice that the base image, openliberty/open-liberty , is also listed. It was pulled as the first step of building application image. Example output: REPOSITORY TAG IMAGE ID CREATED SIZE default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos latest 4758119add3f 2 minutes ago 883MB <none> <none> 5bcb83fad548 5 minutes ago 792MB openliberty/open-liberty full-java8-openj9-ubi e6b5411076fe 5 days ago 794MB maven latest 1337d55397f7 4 weeks ago 631MB Before we push the image to OpenShift's internal image registry, create a separate project named apps . Choose one of two ways to create the project: Via the command line: oc new-project apps Example output: Now using project \"apps\" on server \"https://api.demo.ibmdte.net:64 . . . Via the OpenShift console: Open a Firefox browser window and click on the openshift console bookmark. Under the Home tab on the left menu, click Projects . Click on the Create Project button. Enter apps for the Name field and click on Create . Return to your terminal window. Switch the current project in the command line to apps oc project apps Enable monitoring by adding the necessary label to the apps namespace. Choose one of two options to label the namespace: Via the command line: oc label namespace apps app-monitoring=true Example output: namespace/apps labeled Via the OpenShift console: Under the Administration tab on the left menu, click on Namespaces . Click on the menu-options for apps namespace Click on Edit Labels . Copy and paste app-monitoring=true into the text box . Click Save . Login to the image registry via the command line: Note: From below command, a session token is obtained from the value of another command oc whoami -t and used as the password to login. docker login -u openshift -p $(oc whoami -t) default-route-openshift-image-registry.apps.demo.ibmdte.net Example output: WARNING! Using --password via the CLI is insecure. Use --password-stdin. WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded Push the image to OpenShift's internal image registry via the command line, which could take up to a minute: docker push default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos Example output: The push refers to repository [default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos] 9247390b40be: Pushed 9a21ca46f8e3: Pushed b3cee8ba43fe: Pushed 1dd2f7265f58: Pushed 33b2a4ee94ff: Pushed 2b2a8abdd0c4: Pushed 91ffc437f551: Pushed 4f04e7098d96: Pushed 248016390e0a: Pushed 0fa7eb58a57c: Pushed b5489882eed9: Pushed 2fb5caadbbb0: Pushed d06182ac791b: Pushed b39b0291530b: Pushed a04c77af4b60: Pushed 479c44e860ff: Pushed fc905c23b8a3: Pushed 161ec220381b: Pushed b7b591e3443f: Pushed ccf04fbd6e19: Pushed latest: digest: sha256:56d926b7ef64ed163ff026b7b5608ae97df4630235c1d0443a32a4fc8eb35a6c size: 4513 Verify that the image is in image registry via the command line: oc get images | grep apps/cos The application image you just pushed should be listed. Example output: sha256:56d926b7ef64ed163ff026b7b5608ae97df4630235c1d0443a32a4fc8eb35a6c image-registry.openshift-image-registry.svc:5000/apps/cos@sha256:56d926b7ef64ed163ff026b7b5608ae97df4630235c1d0443a32a4fc8eb35a6c Verify that the image stream is created via the command line: oc get imagestreams Example output: NAME IMAGE REPOSITORY TAGS UPDATED cos default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos latest 2 minutes ago You may also check the image stream via the console: Return to the OpenShift console through your Firefox browser window. Under the Builds tab in the left menu, click on Image Streams . Select project apps from the Project drop-down list. Click on cos from the list. Scroll down to the bottom to see the image that you pushed. Deploy (Hands-on) Customer Order Services application uses DB2 as its database. To deploy it to Liberty, a separate instance of the database is already pre-configured in the OpenShift cluster you are using. The database is exposed within the cluster using a Service , and the application references this database using the address of the Service . Deploy the application using the -k , or kustomize option of Openshift CLI now and we will explain how the deployment works in a later section. Preview what will be deployed: oc kustomize deploy/overlay-apps Example output of yaml: apiVersion: v1 data: DB_HOST: cos-db-liberty.db.svc kind: ConfigMap metadata: name: cos-config namespace: apps --- apiVersion: v1 data: DB_PASSWORD: ZGIyaW5zdDE= DB_USER: ZGIyaW5zdDE= kind: Secret metadata: name: db-creds namespace: apps type: Opaque --- apiVersion: v1 kind: Secret metadata: name: liberty-creds namespace: apps stringData: password: admin username: admin type: Opaque --- apiVersion: openliberty.io/v1beta1 kind: OpenLibertyApplication metadata: name: cos namespace: apps spec: applicationImage: image-registry.openshift-image-registry.svc:5000/apps/cos envFrom: - configMapRef: name: cos-config - secretRef: name: db-creds expose: true livenessProbe: httpGet: path: /health/live port: 9443 scheme: HTTPS monitoring: endpoints: - basicAuth: password: key: password name: liberty-creds username: key: username name: liberty-creds interval: 5s scheme: HTTPS tlsConfig: insecureSkipVerify: true labels: app-monitoring: \"true\" pullPolicy: Always readinessProbe: httpGet: path: /health/ready port: 9443 scheme: HTTPS route: insecureEdgeTerminationPolicy: Redirect termination: reencrypt service: annotations: service.beta.openshift.io/serving-cert-secret-name: cos-tls certificateSecretRef: cos-tls port: 9443 Run the following command to deploy the yaml files: oc apply -k deploy/overlay-apps Output of deploy command: configmap/cos-config created secret/db-creds created secret/liberty-creds created openlibertyapplication.openliberty.io/cos created Verify that the route is created for your application: oc get route cos Example output: NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD cos cos-apps.apps.demo.ibmdte.net cos 9443-tcp reencrypt/Redirect None Verify that your pod from the project apps is ready: First, confirm you're at the current project apps : oc project If it's not at project apps , then switch: oc project apps and then run the following command to view the pod status: oc get pod Example output of pod status: NAME READY STATUS RESTARTS AGE cos-596b4f849f-2fg4h 1/1 Running 0 18m - If the pod doesn't display the expected Running status (for example, after 5 minutes), then delete the pod to restart it. From the command line, run oc delete pod <pod name> (pod name is the string under NAME column from the output of oc get pod ) Access the application (Hands-on) Run the following command to get the URL of your application: echo http://$(oc get route cos --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-apps.apps.demo.ibmdte.net/CustomerOrderServicesWeb Return to your Firefox browser window and go to the URL outputted by the previous command. You'll be shown a login dialog. Login with user skywalker and password force . (The user is pre-created/registered in the basicRegistry configured in Liberty.) After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total . Close the browser. Review the application workload flow with Open Liberty Operator (Hands-on) Return to the OpenShift console through a Firefox window to view the resources on deployment. View the resources in the project openshift-operators : Select the openshift-operators project from the project drop down menu at the top of the page. View the operator's deployment details: Click on the Deployments tab under Workloads from the left menu and select open-liberty-operator Navigate to the YAML tab to view the content of yaml View the operator's pod details: Click on the Pods tab under Workloads from the left menu, and select the pod starting with open-liberty-operator Navigate to the Logs tab to view the open-liberty-operator container log Navigate to the Terminal tab to view the files in the container View the resources in the project apps : Select the apps project from the project drop down menu at the top of the page. View Open Liberty Application instance details: Click on the Installed Operators tab under Operators from the left menu and select Open Liberty Operator . Note: The operator is installed at cluster level and is visible to all existing projects, but Open Liberty Application instance is created under the project apps . Navigate to the Open Liberty Application tab and select cos to view the details of the Open Liberty Application instance Navigate to the YAML tab to view the content of yaml Navigate to the Resources tab to view the resources of Open Liberty Application instance View application deployment details: Click on the Deployments tab under Workloads from the left menu and select cos Navigate to the YAML tab to view the content of yaml. Note the deployment is created through the controller of the OpenLibertyApplication custom resource. View application pod details: Click on the Pods tab under Workloads from the left menu and select the pod starting with cos- Navigate to the Logs tab to view the liberty access log Note: by default, the Open Liberty Application instance is configured with liberty access log: View application service details: Click on the Services tab under Networking from the left menu and select cos Navigate to the YAML tab to view the content of yaml. > Note the service is created through the controller of OpenLibertyApplication custom resource. View application route details: Click on the Routes tab under Networking from the left menu and select cos Navigate to the YAML tab to view the content of yaml. >Note the route is created through the controller of OpenLibertyApplication custom resource. View application secret details: First, return to the YAML of the Open Liberty Application instance to view the configured secrets: Click on the Secrets tab under Workloads from the left menu and select the respective secrets and view the details View the resources in the project db : Select the db project from the project drop down menu. View db deployment details: Click on the Deployments tab under Workloads from the left menu and select cos-db-liberty Navigate to the YAML tab to view the content of yaml View db pod details: Click on the Pods tab under Workloads from the left menu and select the pod starting with cos-db-liberty Navigate to the Logs tab to view the database logs Navigate to the Terminal tab to view the files in the database container View db service details: Click on the Services tab under Networking from the left menu and select cos-db-liberty Review Deployment Let's review the configuration files used for our deployment. Our configuration files are structured for the -k , or kustomize option of Openshift CLI. Kustomize is a separate tool that has been integrated into Openshift CLI that allows you to customize yaml without using variables. You can define a base directory and override directories to customize the base directory Make sure you are in directory /openshift-workshop-was/labs/Openshift/RuntimeModernization or change directory with cd openshift-workshop-was/labs/Openshift/RuntimeModernization List the deploy files: ls deploy And the output shows that we have one base directory and one override directory: base overlay-apps Take a look at what's in the base directory: ls deploy/base And the output: kustomization.yaml olapp-cos.yaml Each directory used for kustomization contains one kustomization.yaml cat deploy/base/kustomization.yaml Content of yaml: - This is a simple kustomization directory that just lists the yaml files to be deployed. resources: - olapp-cos.yaml The file olapp-cos.yaml contains the custom resource definition to deploy the application and will be covered in detail later. Take a look at the files in the overlay-apps directory. ls deploy/overlay-apps And the output: configmap.yaml kustomization.yaml secret-db-creds.yaml secret-liberty-creds.yaml Take a look at the kustomization.yaml in the overlay-apps directory: cat deploy/overlay-apps/kustomization.yaml And the output: namespace: apps resources: - configmap.yaml - secret-db-creds.yaml - secret-liberty-creds.yaml bases: - ./../base Note that: - The namespace is defined. This means that all resources originating from this directory will be applied to the apps namespace. - Resources from the base directory will also be included. - You may define additional overlay directories for different environments, each with a different namespace. For example, overlay-test, overlay-prod. - The configurations in this directory contain the overrides specific to this environment. - For a real environment, DO NOT store the secret yamls into source control. It is a security expsoure. See extra credit section on how to secure your secrets. To preview the resources that will be applied for a specific override directory, use the kustomize option of the Openshift command line. For example, oc kustomize deploy/overlay-apps The output is the same as displayed in Deploy (Hands-on) section. Secrets Specifying credentials and tokens in plain text is not secure. Secrets are used to store sensitive information. The stored data can be referenced by other resources. OpenShift handles secrets with special care. For example, they will not be logged or shown anywhere. There are two secrets - one for database credentials and one for Liberty metrics credentials, which is needed to access the /metrics endpoint. The file secret-db-creds.yaml contains the credentials for the database. It is injected into the container as an environment variable via the secretRef specification for the Open Liberty Operator. View the content of secret-db-creds.yaml file: cat deploy/overlay-apps/secret-db-creds.yaml Example output: kind: Secret apiVersion: v1 metadata: name: db-creds data: DB_PASSWORD: ZGIyaW5zdDE= DB_USER: ZGIyaW5zdDE= type: Opaque The file secret-liberty-creds.yaml contains the secret to access liberty server. View the content of secret-liberty-creds.yaml file: cat deploy/overlay-apps/secret-liberty-creds.yaml Example output: kind: Secret apiVersion: v1 metadata: name: liberty-creds stringData: username: admin password: admin type: Opaque Note that the first Secret provides the credentials in base64 encoded format using the data field. The second one provides them in plain text using the stringData field. OpenShift will automatically convert the credentials to base64 format and place the information under the data field. We can see this by viewing the YAML of the liberty-creds secret: Return to the OpenShift console through a Firefox browser window. Select the apps project from the project drop down menu. Click on the Secrets tab under Workloads from the left menu and search for the liberty-creds secret. Navigate to the YAML tab. Note that the data field contains the credentials in encoded form. Configmap Configmaps allows you to store name/value pairs that can be injected into your container when it starts. For our example, the values of the configmap.yaml are injected as environment variables in the configMapRef specification on the Open Liberty Operator in the next section. cat deploy/overlay-apps/configmap.yaml Example output: apiVersion: v1 kind: ConfigMap metadata: name: cos-config data: SEC_TLS_TRUSTDEFAULTCERTS: \"true\" DB_HOST : \"cos-db-liberty.db.svc\" Open Liberty Operator We could have created Deployment, Service, and Route resources to deploy the Liberty image. However, for this lab we will use the Open Liberty Operator instead. The Open Liberty Operator provides all functionalities of Runtime Component Operator used when deploying traditional WebSphere images in a previous lab. In addition, it also offers Open Liberty specific capabilities, such as day-2 operations (gather trace & dumps) and single sign-on (SSO). The file deploy/base/olapp-cos.yaml looks like: apiVersion: openliberty.io/v1beta1 kind: OpenLibertyApplication metadata: name: cos namespace: apps spec: applicationImage: 'image-registry.openshift-image-registry.svc:5000/apps/cos' pullPolicy: Always readinessProbe: httpGet: path: /health/ready port: 9443 scheme: HTTPS livenessProbe: httpGet: path: /health/live port: 9443 scheme: HTTPS service: annotations: service.beta.openshift.io/serving-cert-secret-name: cos-tls certificateSecretRef: cos-tls port: 9443 expose: true route: termination: reencrypt insecureEdgeTerminationPolicy: Redirect env: envFrom: - configMapRef: name: cos-config - secretRef: name: db-creds monitoring: endpoints: - basicAuth: password: key: password name: liberty-creds username: key: username name: liberty-creds interval: 5s scheme: HTTPS tlsConfig: insecureSkipVerify: true labels: app-monitoring: 'true' The OpenLibertyApplication is a custom resource supported by the Open Liberty Operator, which is designed to help you with Liberty deployment. It allows you to provide Liberty specific configurations (day-2 operations, single sign-on). The application image you pushed earlier to internal image registry is specified by the applicationImage parameter. MicroProfile Health endpoints /health/ready and /health/live are used for readiness and liveness probes. The configMapRef surfaces all entries of the ConfigMap cos-config as environment variables. The secretRef surfaces the entries in the Secret db-creds as environment variables. These are the database user and password. Enabled application monitoring so that Prometheus can scrape the information provided by MicroProfile Metric's /metrics endpoint in Liberty. The /metrics endpoint is protected, hence the credentials are provided using the Secret liberty-creds you created earlier. Cleanup (Hands-on) (Skip this step if you're going to run the next lab Application Management on the same assigned cluster) The controller for the Open Liberty Operator creates the necessary Deployment, Service, and Route objects for the Customer Order Services application. To list these resources, run the commands: Reminder: Run oc project to confirm you're at the apps project before running the following commands. oc get deployment oc get service oc get route Example output: # oc get deployment NAME READY UP-TO-DATE AVAILABLE AGE cos 1/1 1 1 2d18h # oc get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cos ClusterIP 172.21.202.9 <none> 9443/TCP 2d18h # oc get route NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD cos cos-apps.apps.demo.ibmdte.net cos 9443-tcp reencrypt/Redirect None To remove these resources, run the command (Ensure you are in directory openshift-workshop-was/labs/Openshift/RuntimeModernization ) Note: The pre-installed resources such as Open Liberty Operator and DB2, are not removed. oc delete -k deploy/overlay-apps Output: configmap \"cos-config\" deleted secret \"db-creds\" deleted secret \"liberty-creds\" deleted openlibertyapplication.openliberty.io \"cos\" deleted Double check the corresponding Deployment, Service, and Route objects are deleted: oc get deployment oc get service oc get route Output from each get command above: No resources found in apps namespace. Extra Credit Read about how to protect your secrets: https://www.openshift.com/blog/gitops-secret-management Summary Congratulations! You've completed Runtime Modernization lab! This application has been modified from the initial WebSphere ND v8.5.5 version to run on modern & cloud-native runtime Open Liberty and deployed by IBM Cloud Pak for Applications to RedHat OpenShift. Next Follow the link to the next lab Application Management : - Application Management","title":"Runtime Modernization"},{"location":"appmod-labs/RuntimeModernization/README - Copy/#runtime-modernization","text":"","title":"Runtime Modernization"},{"location":"appmod-labs/RuntimeModernization/README - Copy/#table-of-contents","text":"Introduction Analysis (Hands-on) Build (Hands-on) Deploy (Hands-on) Access the Application (Hands-on) Review Deployment Cleanup (can be skipped if the next lab Application Management is included in the same session) Extra Credit Summary Next","title":"Table of Contents"},{"location":"appmod-labs/RuntimeModernization/README - Copy/#introduction","text":"Runtime modernization moves an application to a 'built for the cloud' runtime with the least amount of effort. Open Liberty is a fast, dynamic, and easy-to-use Java application server. Ideal for the cloud, Liberty is open sourced, with fast start-up times (<2 seconds), no server restarts to pick up changes, and a simple XML configuration. This path gets the application on to a cloud-ready runtime container which is easy to use and portable. In addition to the necessary library changes, some aspects of the application were modernized. However, it has not been 'modernized' to a newer architecture such as micro-services . This lab demonstrates runtime modernization . It uses the Customer Order Services application, which originates from WebSphere ND V8.5.5. Click here to get to know the application, including its architecture and components. The application will go through the analysis , build and deploy phases. It is modernized to run on the Liberty runtime, and deployed via the IBM Cloud Pak for Applications to RedHat OpenShift Container Platform (OCP).","title":"Introduction"},{"location":"appmod-labs/RuntimeModernization/README - Copy/#login-to-the-vm","text":"If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password: \\ Password: engageibm Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window.","title":"Login to the VM"},{"location":"appmod-labs/RuntimeModernization/README - Copy/#analysis-hands-on","text":"In this lab, we will demonstrate how the Transformation Advisor can be used specifically in the runtime modernization process. The steps needed to analyze the existing Customer Order Services application are: Open a Firefox browser window from within the VM. Click on the openshift console bookmark in the top left and log in with the htpasswd option. Log in to the OpenShift account using the following credentials: Username: ibmadmin Password: engageibm From the Red Hat OpenShift Container Platform console, go to the Networking tab and click on Routes . Ensure that you are in the ta project by using the project drop down and click on the Location URL next to ta-ui-route . This will open the Transformation Advisor user interface. Click Create new under Workspaces to create a new workspace. Name it RuntimeModernization and click Next . You'll be asked to create a new collection to store the data collected from the Customer Order Services application. Name it CustomerOrderServices . Click Create . The No recommendations available page is displayed. To provide data and receive recommendations, you can either download and execute the Data Collector against an existing WebSphere environment, or upload an existing data collection archive. The archive has already been created for you and the resulting data is stored here . Upload the results of the data collection (the datacollector.zip file) to IBM Cloud Transformation Advisor. When the upload is complete, you will see a list of applications analyzed from the source environment. At the top of the page, you can see the source environment and the target environment settings. Under the Migration target field, click the down arrow and select Compatible runtimes . This shows an entry for each application for each compatible destination runtime you can migrate it to. Click the CustomerOrderServicesApp.ear application with the Open Liberty migration target to open the Application details page . This lab covers runtime modernization, so the application will be re-platformed to run on Open Liberty, and will be placed in a container and deployed to OCP. Look over the migration analysis which shows a summary of the complexity of migrating this application to this target. In summary, migration of this application to Open Liberty is of Moderate complexity as some code changes may be required. (Note: there may be a severe issue related to third-party APIs, but this doesn't apply as they occur in test code.) Click on View migration plan in the top right corner of the page. This page will help you assemble an archive containing: - your application's source or binary files (you upload these here or specify Maven coordinates to download them) - the wsadmin scripts needed to configure your application and its resources (generated by Transformation Advisor and automatically included) - the deployment artifacts needed to create the container image and deploy the application to OCP (generated by Transformation Advisor and automatically included) NOTE: These artifacts have already been provided for you as part of the lab files, so you don't need to download the migration plan. However, you can do so if you wish to look around at the files. These files can also be sent to a Git repository by Transformation Advisor.","title":"Analysis (Hands-on)"},{"location":"appmod-labs/RuntimeModernization/README - Copy/#build-hands-on","text":"In this section, you'll learn how to build a Docker image for Customer Order Services application running on Liberty. Building this image could take around ~3 minutes so let's kick that process off and then come back to learn what you did. Open a new terminal window from the VM desktop. Login to OpenShift CLI with the oc login command from the web terminal. When prompted for the username and password, enter the following login credentials: Username: ibmadmin Password: engageibm If you have not yet cloned the GitHub repo with the lab artifacts, run the following command on your terminal: git clone https://github.com/IBM/openshift-workshop-was.git Change directory to where this lab is located: cd openshift-workshop-was cd labs/Openshift/RuntimeModernization ls Run the following command to start building the image. Make sure to copy the entire command, including the \".\" at the end (indicated as the location of current directory). While the image is building (which takes ~3 minutes), continue with rest of the lab: docker build --tag default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos .","title":"Build (Hands-on)"},{"location":"appmod-labs/RuntimeModernization/README - Copy/#modernize-with-microprofile-for-reading-only","text":"Eclipse MicroProfile is a modular set of technologies designed so that you can write cloud-native microservices. However, even though our application has not been refactored into microservices, we can still take advantage of some of the technologies from MicroProfile.","title":"Modernize with MicroProfile (for reading only)"},{"location":"appmod-labs/RuntimeModernization/README - Copy/#determine-applications-availability-for-reading-only","text":"In the last lab, we used /CustomerOrderServicesWeb/index.html for readiness and liveness probes, which is not the best indication that an application is ready to handle traffic or is healthy to process requests correctly within a reasonable amount of time. What if the database is down? What if the application's security layer is not yet ready/unable to handle authentication? The Pod would still be considered ready and healthy and traffic would still be sent to it. All of those requests would fail or queue up, leading to bigger problems. MicroProfile Health provides a common REST endpoint format to determine whether a microservice (or in our case a monolith application) is healthy or not. The service itself might be running but considered unhealthy if the things it requires for normal operation are unavailable. All of the checks are performed periodically, and the result is served as a simple UP or DOWN at /health/ready and /health/live which can be used for readiness and liveness probes. We implemented the following health checks: - ReadinessCheck : The application reports it is ready as long as the readiness check endpoint can be reached. Connections to other services and any other required conditions for the application to be considered ready are checked here. return HealthCheckResponse.named(\"Readiness\").up().build(); LivenessCheck : The requests should be processed within a reasonable amount of time. Monitor thread block times to identify potential deadlocks which can cause the application to hang. ThreadMXBean tBean = ManagementFactory.getThreadMXBean(); long ids[] = tBean.findMonitorDeadlockedThreads(); if (ids !=null) { ThreadInfo threadInfos[] = tBean.getThreadInfo(ids); for (ThreadInfo ti : threadInfos) { double seconds = ti.getBlockedTime() / 1000.0; if (seconds > 60) { return HealthCheckResponse.named(\"Liveness\").down().build(); } } } return HealthCheckResponse.named(\"Liveness\").up().build();","title":"Determine application's availability (for reading only)"},{"location":"appmod-labs/RuntimeModernization/README - Copy/#adding-metrics-to-application-for-reading-only","text":"MicroProfile Metrics is used to gather metrics about the time it takes to add an item to cart, retrieve customer information, and count the number of times these operations are performed. @GET @Produces(MediaType.APPLICATION_JSON) @Counted @Timed(name = \"getCustomer_timed\") public Response getCustomer() {","title":"Adding metrics to application (for reading only)"},{"location":"appmod-labs/RuntimeModernization/README - Copy/#liberty-server-configuration-for-reading-only","text":"The Liberty runtime configuration files are based on a template provided by IBM Cloud Transformation Advisor. For this lab, instead of using a single server.xml, the configurations have been split into multiple configuration files and placed into the config/configDropins/overrides directory. You may place configuration files into configDropins/overrides directory to override pre-existing configurations. You may define separate template configurations that reflect the resources in your environment, and copy them into the configDropsins/overrides directory only for those applications that need them.","title":"Liberty server configuration (for reading only)"},{"location":"appmod-labs/RuntimeModernization/README - Copy/#build-image-hands-on","text":"Go back to your terminal to check the build you started earlier. You should see the following message if the image was built successfully. Please wait if it's still building: Successfully tagged default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos:latest Validate that the image is in the repository via the command line: docker images You should see the following images on the output. Notice that the base image, openliberty/open-liberty , is also listed. It was pulled as the first step of building application image. Example output: REPOSITORY TAG IMAGE ID CREATED SIZE default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos latest 4758119add3f 2 minutes ago 883MB <none> <none> 5bcb83fad548 5 minutes ago 792MB openliberty/open-liberty full-java8-openj9-ubi e6b5411076fe 5 days ago 794MB maven latest 1337d55397f7 4 weeks ago 631MB Before we push the image to OpenShift's internal image registry, create a separate project named apps . Choose one of two ways to create the project: Via the command line: oc new-project apps Example output: Now using project \"apps\" on server \"https://api.demo.ibmdte.net:64 . . . Via the OpenShift console: Open a Firefox browser window and click on the openshift console bookmark. Under the Home tab on the left menu, click Projects . Click on the Create Project button. Enter apps for the Name field and click on Create . Return to your terminal window. Switch the current project in the command line to apps oc project apps Enable monitoring by adding the necessary label to the apps namespace. Choose one of two options to label the namespace: Via the command line: oc label namespace apps app-monitoring=true Example output: namespace/apps labeled Via the OpenShift console: Under the Administration tab on the left menu, click on Namespaces . Click on the menu-options for apps namespace Click on Edit Labels . Copy and paste app-monitoring=true into the text box . Click Save . Login to the image registry via the command line: Note: From below command, a session token is obtained from the value of another command oc whoami -t and used as the password to login. docker login -u openshift -p $(oc whoami -t) default-route-openshift-image-registry.apps.demo.ibmdte.net Example output: WARNING! Using --password via the CLI is insecure. Use --password-stdin. WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded Push the image to OpenShift's internal image registry via the command line, which could take up to a minute: docker push default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos Example output: The push refers to repository [default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos] 9247390b40be: Pushed 9a21ca46f8e3: Pushed b3cee8ba43fe: Pushed 1dd2f7265f58: Pushed 33b2a4ee94ff: Pushed 2b2a8abdd0c4: Pushed 91ffc437f551: Pushed 4f04e7098d96: Pushed 248016390e0a: Pushed 0fa7eb58a57c: Pushed b5489882eed9: Pushed 2fb5caadbbb0: Pushed d06182ac791b: Pushed b39b0291530b: Pushed a04c77af4b60: Pushed 479c44e860ff: Pushed fc905c23b8a3: Pushed 161ec220381b: Pushed b7b591e3443f: Pushed ccf04fbd6e19: Pushed latest: digest: sha256:56d926b7ef64ed163ff026b7b5608ae97df4630235c1d0443a32a4fc8eb35a6c size: 4513 Verify that the image is in image registry via the command line: oc get images | grep apps/cos The application image you just pushed should be listed. Example output: sha256:56d926b7ef64ed163ff026b7b5608ae97df4630235c1d0443a32a4fc8eb35a6c image-registry.openshift-image-registry.svc:5000/apps/cos@sha256:56d926b7ef64ed163ff026b7b5608ae97df4630235c1d0443a32a4fc8eb35a6c Verify that the image stream is created via the command line: oc get imagestreams Example output: NAME IMAGE REPOSITORY TAGS UPDATED cos default-route-openshift-image-registry.apps.demo.ibmdte.net/apps/cos latest 2 minutes ago You may also check the image stream via the console: Return to the OpenShift console through your Firefox browser window. Under the Builds tab in the left menu, click on Image Streams . Select project apps from the Project drop-down list. Click on cos from the list. Scroll down to the bottom to see the image that you pushed.","title":"Build image (Hands-on)"},{"location":"appmod-labs/RuntimeModernization/README - Copy/#deploy-hands-on","text":"Customer Order Services application uses DB2 as its database. To deploy it to Liberty, a separate instance of the database is already pre-configured in the OpenShift cluster you are using. The database is exposed within the cluster using a Service , and the application references this database using the address of the Service . Deploy the application using the -k , or kustomize option of Openshift CLI now and we will explain how the deployment works in a later section. Preview what will be deployed: oc kustomize deploy/overlay-apps Example output of yaml: apiVersion: v1 data: DB_HOST: cos-db-liberty.db.svc kind: ConfigMap metadata: name: cos-config namespace: apps --- apiVersion: v1 data: DB_PASSWORD: ZGIyaW5zdDE= DB_USER: ZGIyaW5zdDE= kind: Secret metadata: name: db-creds namespace: apps type: Opaque --- apiVersion: v1 kind: Secret metadata: name: liberty-creds namespace: apps stringData: password: admin username: admin type: Opaque --- apiVersion: openliberty.io/v1beta1 kind: OpenLibertyApplication metadata: name: cos namespace: apps spec: applicationImage: image-registry.openshift-image-registry.svc:5000/apps/cos envFrom: - configMapRef: name: cos-config - secretRef: name: db-creds expose: true livenessProbe: httpGet: path: /health/live port: 9443 scheme: HTTPS monitoring: endpoints: - basicAuth: password: key: password name: liberty-creds username: key: username name: liberty-creds interval: 5s scheme: HTTPS tlsConfig: insecureSkipVerify: true labels: app-monitoring: \"true\" pullPolicy: Always readinessProbe: httpGet: path: /health/ready port: 9443 scheme: HTTPS route: insecureEdgeTerminationPolicy: Redirect termination: reencrypt service: annotations: service.beta.openshift.io/serving-cert-secret-name: cos-tls certificateSecretRef: cos-tls port: 9443 Run the following command to deploy the yaml files: oc apply -k deploy/overlay-apps Output of deploy command: configmap/cos-config created secret/db-creds created secret/liberty-creds created openlibertyapplication.openliberty.io/cos created Verify that the route is created for your application: oc get route cos Example output: NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD cos cos-apps.apps.demo.ibmdte.net cos 9443-tcp reencrypt/Redirect None Verify that your pod from the project apps is ready: First, confirm you're at the current project apps : oc project If it's not at project apps , then switch: oc project apps and then run the following command to view the pod status: oc get pod Example output of pod status: NAME READY STATUS RESTARTS AGE cos-596b4f849f-2fg4h 1/1 Running 0 18m - If the pod doesn't display the expected Running status (for example, after 5 minutes), then delete the pod to restart it. From the command line, run oc delete pod <pod name> (pod name is the string under NAME column from the output of oc get pod )","title":"Deploy (Hands-on)"},{"location":"appmod-labs/RuntimeModernization/README - Copy/#access-the-application-hands-on","text":"Run the following command to get the URL of your application: echo http://$(oc get route cos --template='{{ .spec.host }}')/CustomerOrderServicesWeb Example output: http://cos-apps.apps.demo.ibmdte.net/CustomerOrderServicesWeb Return to your Firefox browser window and go to the URL outputted by the previous command. You'll be shown a login dialog. Login with user skywalker and password force . (The user is pre-created/registered in the basicRegistry configured in Liberty.) After login, the application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on an item (a movie) and on the next pop-up panel, drag and drop the item into the shopping cart. Add a few items to the cart. As the items are added, they\u2019ll be shown under Current Shopping Cart (on the upper right) with Order Total . Close the browser.","title":"Access the application (Hands-on)"},{"location":"appmod-labs/RuntimeModernization/README - Copy/#review-the-application-workload-flow-with-open-liberty-operator-hands-on","text":"Return to the OpenShift console through a Firefox window to view the resources on deployment. View the resources in the project openshift-operators : Select the openshift-operators project from the project drop down menu at the top of the page. View the operator's deployment details: Click on the Deployments tab under Workloads from the left menu and select open-liberty-operator Navigate to the YAML tab to view the content of yaml View the operator's pod details: Click on the Pods tab under Workloads from the left menu, and select the pod starting with open-liberty-operator Navigate to the Logs tab to view the open-liberty-operator container log Navigate to the Terminal tab to view the files in the container View the resources in the project apps : Select the apps project from the project drop down menu at the top of the page. View Open Liberty Application instance details: Click on the Installed Operators tab under Operators from the left menu and select Open Liberty Operator . Note: The operator is installed at cluster level and is visible to all existing projects, but Open Liberty Application instance is created under the project apps . Navigate to the Open Liberty Application tab and select cos to view the details of the Open Liberty Application instance Navigate to the YAML tab to view the content of yaml Navigate to the Resources tab to view the resources of Open Liberty Application instance View application deployment details: Click on the Deployments tab under Workloads from the left menu and select cos Navigate to the YAML tab to view the content of yaml. Note the deployment is created through the controller of the OpenLibertyApplication custom resource. View application pod details: Click on the Pods tab under Workloads from the left menu and select the pod starting with cos- Navigate to the Logs tab to view the liberty access log Note: by default, the Open Liberty Application instance is configured with liberty access log: View application service details: Click on the Services tab under Networking from the left menu and select cos Navigate to the YAML tab to view the content of yaml. > Note the service is created through the controller of OpenLibertyApplication custom resource. View application route details: Click on the Routes tab under Networking from the left menu and select cos Navigate to the YAML tab to view the content of yaml. >Note the route is created through the controller of OpenLibertyApplication custom resource. View application secret details: First, return to the YAML of the Open Liberty Application instance to view the configured secrets: Click on the Secrets tab under Workloads from the left menu and select the respective secrets and view the details View the resources in the project db : Select the db project from the project drop down menu. View db deployment details: Click on the Deployments tab under Workloads from the left menu and select cos-db-liberty Navigate to the YAML tab to view the content of yaml View db pod details: Click on the Pods tab under Workloads from the left menu and select the pod starting with cos-db-liberty Navigate to the Logs tab to view the database logs Navigate to the Terminal tab to view the files in the database container View db service details: Click on the Services tab under Networking from the left menu and select cos-db-liberty","title":"Review the application workload flow with Open Liberty Operator (Hands-on)"},{"location":"appmod-labs/RuntimeModernization/README - Copy/#review-deployment","text":"Let's review the configuration files used for our deployment. Our configuration files are structured for the -k , or kustomize option of Openshift CLI. Kustomize is a separate tool that has been integrated into Openshift CLI that allows you to customize yaml without using variables. You can define a base directory and override directories to customize the base directory Make sure you are in directory /openshift-workshop-was/labs/Openshift/RuntimeModernization or change directory with cd openshift-workshop-was/labs/Openshift/RuntimeModernization List the deploy files: ls deploy And the output shows that we have one base directory and one override directory: base overlay-apps Take a look at what's in the base directory: ls deploy/base And the output: kustomization.yaml olapp-cos.yaml Each directory used for kustomization contains one kustomization.yaml cat deploy/base/kustomization.yaml Content of yaml: - This is a simple kustomization directory that just lists the yaml files to be deployed. resources: - olapp-cos.yaml The file olapp-cos.yaml contains the custom resource definition to deploy the application and will be covered in detail later. Take a look at the files in the overlay-apps directory. ls deploy/overlay-apps And the output: configmap.yaml kustomization.yaml secret-db-creds.yaml secret-liberty-creds.yaml Take a look at the kustomization.yaml in the overlay-apps directory: cat deploy/overlay-apps/kustomization.yaml And the output: namespace: apps resources: - configmap.yaml - secret-db-creds.yaml - secret-liberty-creds.yaml bases: - ./../base Note that: - The namespace is defined. This means that all resources originating from this directory will be applied to the apps namespace. - Resources from the base directory will also be included. - You may define additional overlay directories for different environments, each with a different namespace. For example, overlay-test, overlay-prod. - The configurations in this directory contain the overrides specific to this environment. - For a real environment, DO NOT store the secret yamls into source control. It is a security expsoure. See extra credit section on how to secure your secrets. To preview the resources that will be applied for a specific override directory, use the kustomize option of the Openshift command line. For example, oc kustomize deploy/overlay-apps The output is the same as displayed in Deploy (Hands-on) section.","title":"Review Deployment"},{"location":"appmod-labs/RuntimeModernization/README - Copy/#secrets","text":"Specifying credentials and tokens in plain text is not secure. Secrets are used to store sensitive information. The stored data can be referenced by other resources. OpenShift handles secrets with special care. For example, they will not be logged or shown anywhere. There are two secrets - one for database credentials and one for Liberty metrics credentials, which is needed to access the /metrics endpoint. The file secret-db-creds.yaml contains the credentials for the database. It is injected into the container as an environment variable via the secretRef specification for the Open Liberty Operator. View the content of secret-db-creds.yaml file: cat deploy/overlay-apps/secret-db-creds.yaml Example output: kind: Secret apiVersion: v1 metadata: name: db-creds data: DB_PASSWORD: ZGIyaW5zdDE= DB_USER: ZGIyaW5zdDE= type: Opaque The file secret-liberty-creds.yaml contains the secret to access liberty server. View the content of secret-liberty-creds.yaml file: cat deploy/overlay-apps/secret-liberty-creds.yaml Example output: kind: Secret apiVersion: v1 metadata: name: liberty-creds stringData: username: admin password: admin type: Opaque Note that the first Secret provides the credentials in base64 encoded format using the data field. The second one provides them in plain text using the stringData field. OpenShift will automatically convert the credentials to base64 format and place the information under the data field. We can see this by viewing the YAML of the liberty-creds secret: Return to the OpenShift console through a Firefox browser window. Select the apps project from the project drop down menu. Click on the Secrets tab under Workloads from the left menu and search for the liberty-creds secret. Navigate to the YAML tab. Note that the data field contains the credentials in encoded form.","title":"Secrets"},{"location":"appmod-labs/RuntimeModernization/README - Copy/#configmap","text":"Configmaps allows you to store name/value pairs that can be injected into your container when it starts. For our example, the values of the configmap.yaml are injected as environment variables in the configMapRef specification on the Open Liberty Operator in the next section. cat deploy/overlay-apps/configmap.yaml Example output: apiVersion: v1 kind: ConfigMap metadata: name: cos-config data: SEC_TLS_TRUSTDEFAULTCERTS: \"true\" DB_HOST : \"cos-db-liberty.db.svc\"","title":"Configmap"},{"location":"appmod-labs/RuntimeModernization/README - Copy/#open-liberty-operator","text":"We could have created Deployment, Service, and Route resources to deploy the Liberty image. However, for this lab we will use the Open Liberty Operator instead. The Open Liberty Operator provides all functionalities of Runtime Component Operator used when deploying traditional WebSphere images in a previous lab. In addition, it also offers Open Liberty specific capabilities, such as day-2 operations (gather trace & dumps) and single sign-on (SSO). The file deploy/base/olapp-cos.yaml looks like: apiVersion: openliberty.io/v1beta1 kind: OpenLibertyApplication metadata: name: cos namespace: apps spec: applicationImage: 'image-registry.openshift-image-registry.svc:5000/apps/cos' pullPolicy: Always readinessProbe: httpGet: path: /health/ready port: 9443 scheme: HTTPS livenessProbe: httpGet: path: /health/live port: 9443 scheme: HTTPS service: annotations: service.beta.openshift.io/serving-cert-secret-name: cos-tls certificateSecretRef: cos-tls port: 9443 expose: true route: termination: reencrypt insecureEdgeTerminationPolicy: Redirect env: envFrom: - configMapRef: name: cos-config - secretRef: name: db-creds monitoring: endpoints: - basicAuth: password: key: password name: liberty-creds username: key: username name: liberty-creds interval: 5s scheme: HTTPS tlsConfig: insecureSkipVerify: true labels: app-monitoring: 'true' The OpenLibertyApplication is a custom resource supported by the Open Liberty Operator, which is designed to help you with Liberty deployment. It allows you to provide Liberty specific configurations (day-2 operations, single sign-on). The application image you pushed earlier to internal image registry is specified by the applicationImage parameter. MicroProfile Health endpoints /health/ready and /health/live are used for readiness and liveness probes. The configMapRef surfaces all entries of the ConfigMap cos-config as environment variables. The secretRef surfaces the entries in the Secret db-creds as environment variables. These are the database user and password. Enabled application monitoring so that Prometheus can scrape the information provided by MicroProfile Metric's /metrics endpoint in Liberty. The /metrics endpoint is protected, hence the credentials are provided using the Secret liberty-creds you created earlier.","title":"Open Liberty Operator"},{"location":"appmod-labs/RuntimeModernization/README - Copy/#cleanup-hands-on-skip-this-step-if-youre-going-to-run-the-next-lab-application-management-on-the-same-assigned-cluster","text":"The controller for the Open Liberty Operator creates the necessary Deployment, Service, and Route objects for the Customer Order Services application. To list these resources, run the commands: Reminder: Run oc project to confirm you're at the apps project before running the following commands. oc get deployment oc get service oc get route Example output: # oc get deployment NAME READY UP-TO-DATE AVAILABLE AGE cos 1/1 1 1 2d18h # oc get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cos ClusterIP 172.21.202.9 <none> 9443/TCP 2d18h # oc get route NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD cos cos-apps.apps.demo.ibmdte.net cos 9443-tcp reencrypt/Redirect None To remove these resources, run the command (Ensure you are in directory openshift-workshop-was/labs/Openshift/RuntimeModernization ) Note: The pre-installed resources such as Open Liberty Operator and DB2, are not removed. oc delete -k deploy/overlay-apps Output: configmap \"cos-config\" deleted secret \"db-creds\" deleted secret \"liberty-creds\" deleted openlibertyapplication.openliberty.io \"cos\" deleted Double check the corresponding Deployment, Service, and Route objects are deleted: oc get deployment oc get service oc get route Output from each get command above: No resources found in apps namespace.","title":"Cleanup (Hands-on) (Skip this step if you're going to run the next lab Application Management on the same assigned cluster)"},{"location":"appmod-labs/RuntimeModernization/README - Copy/#extra-credit","text":"Read about how to protect your secrets: https://www.openshift.com/blog/gitops-secret-management","title":"Extra Credit"},{"location":"appmod-labs/RuntimeModernization/README - Copy/#summary","text":"Congratulations! You've completed Runtime Modernization lab! This application has been modified from the initial WebSphere ND v8.5.5 version to run on modern & cloud-native runtime Open Liberty and deployed by IBM Cloud Pak for Applications to RedHat OpenShift.","title":"Summary"},{"location":"appmod-labs/RuntimeModernization/README - Copy/#next","text":"Follow the link to the next lab Application Management : - Application Management","title":"Next"},{"location":"appmod-labs/RuntimeModernization/extras/application/","text":"Application Overview The Customer Order Services application is a simple store-front shopping application, built during the early days of the Web 2.0 movement. Users interact directly with a browser-based interface and manage their cart to submit orders. This application is built using the traditional 3-Tier Architecture model, with an HTTP server, an application server, and a supporting database. There are several components of the overall application architecture: - Starting with the database, the application leverages two SQL-based databases running on IBM DB2. The application exposes its data model through an Enterprise JavaBean layer, named CustomerOrderServices . This component leverages the Java Persistence API to expose the backend data model to calling services with minimal coding effort. This build of the application uses JavaEE6 features for EJBs and JPA. The next tier of the application, named CustomerOrderServicesWeb , exposes the necessary business APIs via REST-based web services. This component leverages the JAX-RS libraries for creating Java-based REST services with minimal coding effort. This build of the application is using JAX-RS 1.1 version of the respective capability. The application's user interface is exposed through the CustomerOrderServicesWeb component as well, in the form of a Dojo Toolkit-based JavaScript application. Delivering the user interface and business APIs in the same component is one major inhibitor our migration strategy will help to alleviate in the long-term. Finally, there is an additional integration testing component, named CustomerOrderServicesTest that is built to quickly validate an application's build and deployment to a given application server. This test component contains both JPA and JAX-RS -based tests.","title":"Application Overview"},{"location":"appmod-labs/RuntimeModernization/extras/application/#application-overview","text":"The Customer Order Services application is a simple store-front shopping application, built during the early days of the Web 2.0 movement. Users interact directly with a browser-based interface and manage their cart to submit orders. This application is built using the traditional 3-Tier Architecture model, with an HTTP server, an application server, and a supporting database. There are several components of the overall application architecture: - Starting with the database, the application leverages two SQL-based databases running on IBM DB2. The application exposes its data model through an Enterprise JavaBean layer, named CustomerOrderServices . This component leverages the Java Persistence API to expose the backend data model to calling services with minimal coding effort. This build of the application uses JavaEE6 features for EJBs and JPA. The next tier of the application, named CustomerOrderServicesWeb , exposes the necessary business APIs via REST-based web services. This component leverages the JAX-RS libraries for creating Java-based REST services with minimal coding effort. This build of the application is using JAX-RS 1.1 version of the respective capability. The application's user interface is exposed through the CustomerOrderServicesWeb component as well, in the form of a Dojo Toolkit-based JavaScript application. Delivering the user interface and business APIs in the same component is one major inhibitor our migration strategy will help to alleviate in the long-term. Finally, there is an additional integration testing component, named CustomerOrderServicesTest that is built to quickly validate an application's build and deployment to a given application server. This test component contains both JPA and JAX-RS -based tests.","title":"Application Overview"},{"location":"appmod-labs/RuntimeModernization/extras/liberty-analyze/","text":"Liberty - Analyze This section covers how to use IBM Cloud Transformation Advisor to analyze an existing traditional WebSphere application. For this scenario the Liberty runtime is chosen as the target runtime and the intention is to migrate this application with minimal code changes. Liberty is a fast, dynamic, and easy-to-use Java application server. Ideal or the cloud, Liberty is a combination of IBM technology and open source software, with fast startup times (<2 seconds), no server restarts to pick up changes, and a simple XML configuration. Summary This section has the following steps: Introduction to IBM Cloud Transformation Advisor Install IBM Cloud Transformation Advisor Download and run the Data Collector Upload and analyze the results Determine the migration/modernization path and next steps Introduction to IBM Cloud Transformation Advisor IBM Cloud Transformation Advisor helps you access, analyze and modernize middleware based apps into IBM Cloud(s). It categorizes Java EE apps and MQ queue managers as simple, medium and complex based on migration complexity and provides guidance for modernization. IBM Cloud Transformation Advisor can accelerate the process to move your on-premises apps to cloud, minimize migration errors and risks, and reduce time to market in five steps. You can use IBM Transformation Advisor for these activities: - Identify the Java EE programming models in an app - Determine the complexity of apps by reviewing a high-level inventory of the content and structure of each app - Highlight the Java EE programming model and WebSphere API differences between the profile types - Learn any Java EE specification implementation differences that might affect the app The tool also provides suggestions for the right-fit IBM WebSphere Application Server edition and offers advice, practices, and potential solutions to move apps to Liberty or to newer versions of WebSphere Application Server. Install IBM Cloud Transformation Advisor IBM Cloud Transformation Advisor is split in to two components (the analysis engine and the data collector ). Access Transformation Advisor within IBM Cloud Pak for Applications. You can also run it locally on a machine with Docker. See Installing IBM Cloud Transformation Advisor Beta Edition locally . Download the Data Collector Once IBM Cloud Transformation Advisor is installed, it is necessary to create a new Workspace and Collection and then download the Data Collector that will be used to examine the existing environment and applications. Open IBM Cloud Transformation Advisor in a browser and click the button to create a new Workspace Enter a Workspace name such as CloudPak_for_Applications and click Next Enter a Collection name such as WAS855_AppSrv01 and click Let's go When the No recommendations available page is displayed, click the Data Collector button When the Data Collector page is displayed, select the Source Operating System for your environment and click the Download button to download the Data Collector. This results in a file with a name similar to transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz being downloaded. Run the Data Collector Upload the Data Collector zip file that was downloaded from IBM Cloud Transformation Advisor in the previous step to the machine that the WebSphere ND Deployment Manager or the Standalone WebSphere Application Server is installed. The directory used arbitrary. Navigate to the directory you uploaded the transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz file to and issue the following commands to extract the Data Collector: mkdir datacollector cd datacollector mv transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz . tar -zxvf transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz cd transformationadvisor-* It is necessary to modify the scan performed by the Data Collector to include the org.pwte package as the Data Collector doesn't scan org.* packages by default. Open the conf/customCmd.properties file and modify it as shown below: evaluation=--evaluate --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp --includePackages=org.pwte migration_liberty=--analyze --sourceAppServer=was855 --targetAppServer=liberty --targetCloud=dockerIBMCloud --includePackages=org.pwte --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp migration_was=--analyze --sourceAppServer=was855 --targetAppServer=was90 --targetCloud=vmIBMCloud --includePackages=org.pwte --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp #inventory=--inventory --excludeFiles=\".*/directory/LargeXMLFileName.xml\" #featureList=--featureList --excludeFiles=\".*/directory/LargeXMLFileName.xml\" #java_opt=-Xmx2g The following command assumes that WebSphere Application Server v855 is installed to /opt/IBM/WebSphere/AppServer855 with a profile named AppSrv01 and that the administration user is wasadmin with a password of wasadmin . Modify and issue the following command as necessary to execute the Data Collector against the WebSphere environment: ./bin/transformationadvisor -w /opt/IBM/WebSphere/AppServer855 -p AppSrv01 wasadmin wasadmin When prompted, accept the license agreement . The Data Collection process will now start and will analyze all of the applications installed in the WebSphere Application Server environment and will also collect the related Java EE artifacts such as Data Sources and JMS definitions. When the analysis is complete, the Data Collector will attempt to upload the collection results to IBM Cloud Transformation Advisor. If this is successful, you can skip to the Analyze the Recommendations section. If not, you will receive an error at the end of Data Collection and will find a file named AppSrv01.zip in your current directory as shown below. ~/datacollector/transformationadvisor-1.9.6# ls -la *.zip -rw-r--r-- 1 root root 625493 Jun 12 12:58 AppSrv01.zip Download this Data Collector Results zip file ready for uploading to IBM Cloud Transformation Advisor in the next section Upload the Data Collector results In this section the results from the Data Collector will be uploaded to IBM Cloud Transformation Advisor. In the IBM Cloud Transformation Advisor web browser session, click the Recomendations link in the top left corner and then click the Upload data button as shown below When the Upload data dialog is displayed, use the Drop or Add File button to select the Data Collector Results zip file that was downloaded in the previous section. Click Upload After a few moments the upload of the data collector results will be completed. Analyze the Recommendations Once the Data Collector Results have been uploaded to IBM Cloud Transformation Advisor a set of recommendations will be created and shown on the Recommendations page. In this section the recommendations will be analyzed and interpreted. The default recommendations are based on a target runtime of Liberty on Private Cloud . The Data Collector analyzed all of the applications running on the traditional WebSphere profile a displays a row in the chart for each application. In the case of the CustomerOrderServicesApp.ear application, IBM Cloud Transformation Advisor has determined that the migration to WebSphere Traditional on Private Cloud is of Moderate complexity and that there are two Severe Issues that have been detected. Click on the CustomerOrderServicesApp.ear application name to see more information. Review the analysis results and scroll down to the Technology Issues section. Note that IBM Cloud Transformation Advisor has detected that there are issues with lookups for Enterprise JavaBeans and with accessing the Apache Wink APIs. In order to review the IBM Cloud Transformation Advisor results in more detail, scroll to the bottom of the analysis page and click on the Analysis Report link When the warning dialog is displayed, click OK The Detailed Migration Analysis Report will be displayed which show the results of the migration rules that were executed by the Data Collector and returned results. Scroll down to the Severe Rules section and click on the Show rule help link for each of the results. Review the recommendations. Behavior change on lookups for Enterprise JavaBeans In Liberty, EJB components are not bound to a server root Java Naming and Directory Interface (JNDI) namespace as they are in WebSphere Application Server traditional. The fix for this is to change the three classes that use ejblocal to use the correct URL for Liberty The user of system provided Apache Wink APIs requires configuration To use system-provided third-party APIs in Liberty applications, you must configure the applications to include the APIs. In WebSphere Application Server traditional, these APIs are available without configuration. This is a configuration only change and can be achieved by using a classloader definition in the Liberty server.xml file. Final Analysis The intention of this traditional WebSphere V855 --> Liberty (Private Cloud) scenario is to migrate the Customer Order Services application to the cloud-ready new runtime with minimal code changes. IBM Cloud Transformation Advisor was used to analyze the application for compatibility with traditional WebSphere V9 (Private Cloud) and determined that only small changes to three classes would be required. While this scenario will move the application to the cloud-ready Liberty runtime in a container, it will not modernize the application architecture and code in any way.","title":"Liberty - Analyze"},{"location":"appmod-labs/RuntimeModernization/extras/liberty-analyze/#liberty-analyze","text":"This section covers how to use IBM Cloud Transformation Advisor to analyze an existing traditional WebSphere application. For this scenario the Liberty runtime is chosen as the target runtime and the intention is to migrate this application with minimal code changes. Liberty is a fast, dynamic, and easy-to-use Java application server. Ideal or the cloud, Liberty is a combination of IBM technology and open source software, with fast startup times (<2 seconds), no server restarts to pick up changes, and a simple XML configuration.","title":"Liberty - Analyze"},{"location":"appmod-labs/RuntimeModernization/extras/liberty-analyze/#summary","text":"This section has the following steps: Introduction to IBM Cloud Transformation Advisor Install IBM Cloud Transformation Advisor Download and run the Data Collector Upload and analyze the results Determine the migration/modernization path and next steps","title":"Summary"},{"location":"appmod-labs/RuntimeModernization/extras/liberty-analyze/#introduction-to-ibm-cloud-transformation-advisor","text":"IBM Cloud Transformation Advisor helps you access, analyze and modernize middleware based apps into IBM Cloud(s). It categorizes Java EE apps and MQ queue managers as simple, medium and complex based on migration complexity and provides guidance for modernization. IBM Cloud Transformation Advisor can accelerate the process to move your on-premises apps to cloud, minimize migration errors and risks, and reduce time to market in five steps. You can use IBM Transformation Advisor for these activities: - Identify the Java EE programming models in an app - Determine the complexity of apps by reviewing a high-level inventory of the content and structure of each app - Highlight the Java EE programming model and WebSphere API differences between the profile types - Learn any Java EE specification implementation differences that might affect the app The tool also provides suggestions for the right-fit IBM WebSphere Application Server edition and offers advice, practices, and potential solutions to move apps to Liberty or to newer versions of WebSphere Application Server.","title":"Introduction to IBM Cloud Transformation Advisor"},{"location":"appmod-labs/RuntimeModernization/extras/liberty-analyze/#install-ibm-cloud-transformation-advisor","text":"IBM Cloud Transformation Advisor is split in to two components (the analysis engine and the data collector ). Access Transformation Advisor within IBM Cloud Pak for Applications. You can also run it locally on a machine with Docker. See Installing IBM Cloud Transformation Advisor Beta Edition locally .","title":"Install IBM Cloud Transformation Advisor"},{"location":"appmod-labs/RuntimeModernization/extras/liberty-analyze/#download-the-data-collector","text":"Once IBM Cloud Transformation Advisor is installed, it is necessary to create a new Workspace and Collection and then download the Data Collector that will be used to examine the existing environment and applications. Open IBM Cloud Transformation Advisor in a browser and click the button to create a new Workspace Enter a Workspace name such as CloudPak_for_Applications and click Next Enter a Collection name such as WAS855_AppSrv01 and click Let's go When the No recommendations available page is displayed, click the Data Collector button When the Data Collector page is displayed, select the Source Operating System for your environment and click the Download button to download the Data Collector. This results in a file with a name similar to transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz being downloaded.","title":"Download the Data Collector"},{"location":"appmod-labs/RuntimeModernization/extras/liberty-analyze/#run-the-data-collector","text":"Upload the Data Collector zip file that was downloaded from IBM Cloud Transformation Advisor in the previous step to the machine that the WebSphere ND Deployment Manager or the Standalone WebSphere Application Server is installed. The directory used arbitrary. Navigate to the directory you uploaded the transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz file to and issue the following commands to extract the Data Collector: mkdir datacollector cd datacollector mv transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz . tar -zxvf transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz cd transformationadvisor-* It is necessary to modify the scan performed by the Data Collector to include the org.pwte package as the Data Collector doesn't scan org.* packages by default. Open the conf/customCmd.properties file and modify it as shown below: evaluation=--evaluate --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp --includePackages=org.pwte migration_liberty=--analyze --sourceAppServer=was855 --targetAppServer=liberty --targetCloud=dockerIBMCloud --includePackages=org.pwte --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp migration_was=--analyze --sourceAppServer=was855 --targetAppServer=was90 --targetCloud=vmIBMCloud --includePackages=org.pwte --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp #inventory=--inventory --excludeFiles=\".*/directory/LargeXMLFileName.xml\" #featureList=--featureList --excludeFiles=\".*/directory/LargeXMLFileName.xml\" #java_opt=-Xmx2g The following command assumes that WebSphere Application Server v855 is installed to /opt/IBM/WebSphere/AppServer855 with a profile named AppSrv01 and that the administration user is wasadmin with a password of wasadmin . Modify and issue the following command as necessary to execute the Data Collector against the WebSphere environment: ./bin/transformationadvisor -w /opt/IBM/WebSphere/AppServer855 -p AppSrv01 wasadmin wasadmin When prompted, accept the license agreement . The Data Collection process will now start and will analyze all of the applications installed in the WebSphere Application Server environment and will also collect the related Java EE artifacts such as Data Sources and JMS definitions. When the analysis is complete, the Data Collector will attempt to upload the collection results to IBM Cloud Transformation Advisor. If this is successful, you can skip to the Analyze the Recommendations section. If not, you will receive an error at the end of Data Collection and will find a file named AppSrv01.zip in your current directory as shown below. ~/datacollector/transformationadvisor-1.9.6# ls -la *.zip -rw-r--r-- 1 root root 625493 Jun 12 12:58 AppSrv01.zip Download this Data Collector Results zip file ready for uploading to IBM Cloud Transformation Advisor in the next section","title":"Run the Data Collector"},{"location":"appmod-labs/RuntimeModernization/extras/liberty-analyze/#upload-the-data-collector-results","text":"In this section the results from the Data Collector will be uploaded to IBM Cloud Transformation Advisor. In the IBM Cloud Transformation Advisor web browser session, click the Recomendations link in the top left corner and then click the Upload data button as shown below When the Upload data dialog is displayed, use the Drop or Add File button to select the Data Collector Results zip file that was downloaded in the previous section. Click Upload After a few moments the upload of the data collector results will be completed.","title":"Upload the Data Collector results"},{"location":"appmod-labs/RuntimeModernization/extras/liberty-analyze/#analyze-the-recommendations","text":"Once the Data Collector Results have been uploaded to IBM Cloud Transformation Advisor a set of recommendations will be created and shown on the Recommendations page. In this section the recommendations will be analyzed and interpreted. The default recommendations are based on a target runtime of Liberty on Private Cloud . The Data Collector analyzed all of the applications running on the traditional WebSphere profile a displays a row in the chart for each application. In the case of the CustomerOrderServicesApp.ear application, IBM Cloud Transformation Advisor has determined that the migration to WebSphere Traditional on Private Cloud is of Moderate complexity and that there are two Severe Issues that have been detected. Click on the CustomerOrderServicesApp.ear application name to see more information. Review the analysis results and scroll down to the Technology Issues section. Note that IBM Cloud Transformation Advisor has detected that there are issues with lookups for Enterprise JavaBeans and with accessing the Apache Wink APIs. In order to review the IBM Cloud Transformation Advisor results in more detail, scroll to the bottom of the analysis page and click on the Analysis Report link When the warning dialog is displayed, click OK The Detailed Migration Analysis Report will be displayed which show the results of the migration rules that were executed by the Data Collector and returned results. Scroll down to the Severe Rules section and click on the Show rule help link for each of the results. Review the recommendations. Behavior change on lookups for Enterprise JavaBeans In Liberty, EJB components are not bound to a server root Java Naming and Directory Interface (JNDI) namespace as they are in WebSphere Application Server traditional. The fix for this is to change the three classes that use ejblocal to use the correct URL for Liberty The user of system provided Apache Wink APIs requires configuration To use system-provided third-party APIs in Liberty applications, you must configure the applications to include the APIs. In WebSphere Application Server traditional, these APIs are available without configuration. This is a configuration only change and can be achieved by using a classloader definition in the Liberty server.xml file.","title":"Analyze the Recommendations"},{"location":"appmod-labs/RuntimeModernization/extras/liberty-analyze/#final-analysis","text":"The intention of this traditional WebSphere V855 --> Liberty (Private Cloud) scenario is to migrate the Customer Order Services application to the cloud-ready new runtime with minimal code changes. IBM Cloud Transformation Advisor was used to analyze the application for compatibility with traditional WebSphere V9 (Private Cloud) and determined that only small changes to three classes would be required. While this scenario will move the application to the cloud-ready Liberty runtime in a container, it will not modernize the application architecture and code in any way.","title":"Final Analysis"},{"location":"basic-labs/","text":"Basic Containerization and App Mod Tools Labs Docker Introduction OCP Introduction IBM Transformation Advisor Introduction","title":"Basic Containerization & App Mod Tools Labs"},{"location":"basic-labs/#basic-containerization-and-app-mod-tools-labs","text":"Docker Introduction OCP Introduction IBM Transformation Advisor Introduction","title":"Basic Containerization and App Mod Tools Labs"},{"location":"basic-labs/HelloContainer/","text":"Introduction to Containerization Table of Contents Introduction to Containerization Table of Contents Background Prerequisites What is a Container Login to the VM Check your environment Run a pre-built image Build and Run Your Own Image Managing Image Versions Extra Credit Next Background If you are expecting a lab about docker , you are at the right place. This lab will introduce you to the basic concepts of containerization, including: What are containers and container images How to start, stop, and remove containers. How to create container images How to version container images Prerequisites You have podman or docker installed. Only docker is installed for this lab. You have access to the internet. You have cloned this lab from github. If not, follows these steps: git clone https://github.com/IBM/openshift-workshop-was.git cd openshift-workshop-was/labs/Openshift/HelloContainer What is a Container Compared to virtual machines, containers supports virtualization at the process level. Think of them as virtual processes. The isolation abstraction provided by the operating system makes the process think that it's running in its own virtual machine. As processes, containers may be created, started, and stopped much more quickly than virtual machines. Everything you need to run your application, from the operating system and up, is stored in a special file called a container image. Container images are self contained and portable. You may run one or more instances anywhere. And you don't have to worry about missing prerequisites, because all prerequisites are stored in the image. Container images are created via tools such as docker or podman . Existing images are hosted in container registries. For example, docker hub, or registry.access.redhat.com, or your own internal registry. If you need more background on containers: https://www.docker.com/resources/what-container Login to the VM If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password. Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window. Check your environment Open a terminal window from within your VM. List version of docker: docker --version Example output: Docker version 20.10.7, build f0df350 For more background on docker command line: https://docs.docker.com/engine/reference/commandline/cli/ Run a pre-built image Container images must be available locally before they can be run. To list available local images: docker images Note: no container inages are listed at this point REPOSITORY TAG IMAGE ID CREATED SIZE Images are hosted in container registries. The default container registry for docker is docker hub, located at https://hub.docker.com. Let's pull a test image from docker hub: docker pull openshift/hello-openshift And the output: Using default tag: latest latest: Pulling from openshift/hello-openshift 4f4fb700ef54: Pull complete 8b32988996c5: Pull complete Digest: sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e Status: Downloaded newer image for openshift/hello-openshift:latest docker.io/openshift/hello-openshift:latest List available local images again: docker images The hello-openshift image is now listed REPOSITORY TAG IMAGE ID CREATED SIZE openshift/hello-openshift latest 7af3297a3fb4 2 years ago 6.09MB Inspect the image metadata: docker inspect openshift/hello-openshift Note : It exposes two ports: 8080 and 8888 It runs as user 1001 The entry point executable is /hello-openshift [ { \"Id\": \"sha256:7af3297a3fb4487b740ed6798163f618e6eddea1ee5fa0ba340329fcae31c8f6\", \"RepoTags\": [ \"openshift/hello-openshift:latest\" ], \"RepoDigests\": [ \"openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e\" ], ... \"Config\": { \"User\": \"1001\", ... \"ExposedPorts\": { \"8080/tcp\": {}, \"8888/tcp\": {} }, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], ... \"Entrypoint\": [ \"/hello-openshift\" ] ... } ] Run the image in an container: docker run --name hello1 -d -p 8080:8080 -p 8888:8888 openshift/hello-openshift Note that: The --name option gives the container a name. The -d option runs the command in the background as a daemon The -p option maps the port on the host to the port in the container. Through virtual networking, the port within the container is always the same for all running instances. But to support multiple concurrent running instances, the actual port on the host must be different for each instance. When you start the container, you may assign a new port on the host dynamically. The output of the command is the container ID for the running container. If the container starts successfully, the executable specified by the Entrypoint in the metadata is run. For our sample, it is /hello-openshift . Access the application in the container. a. Open the Firefox Web Browser from inside of the VM. b. Go to the URL http://localhost:8080 c. Also try port 8888 Run another instance of the same image. Note that this new instance is assigned new port numbers 8081 and 8889 on the host. This is so that they don't conflict with the ports 8080 and 8888 already allocated to the first instance. docker run --name hello2 -d -p 8081:8080 -p 8889:8888 openshift/hello-openshift Question: How does this compare to the time it takes to start a new virtual machine? Access the application in the new container the same way. a. Return to the Firefox Web Browser but instead go to the URL http://localhost:8081 b. Also try port 8889 Verify there are two containers running in the same host: docker ps : CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5a62f8527b44 openshift/hello-openshift \"/hello-openshift\" About a minute ago Up About a minute 0.0.0.0:8081->8080/tcp, 0.0.0.0:8889->8888/tcp hello2 c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 4 minutes ago Up 4 minutes 0.0.0.0:8080->8080/tcp, 0.0.0.0:8888->8888/tcp hello1 View the logs: docker logs hello1 And the output: serving on 8888 serving on 8080 View the logs on the second container: docker logs hello2 And the output: serving on 8888 serving on 8080 Note: within the container, each instance behaves as if it's running in its own virtual environment, and has opened the same ports. Outside of the container, different ports are opened. To export the file system of a running container: docker export hello1 > hello1.tar List the files on the file system: tar -tvf hello1.tar Note that this is a very small image. -rwxr-xr-x 0/0 0 2020-04-29 16:48 .dockerenv drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/ -rwxr-xr-x 0/0 0 2020-04-29 16:48 dev/console drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/pts/ drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/shm/ drwxr-xr-x 0/0 0 2020-04-29 16:48 etc/ -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/hostname -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/hosts lrwxrwxrwx 0/0 0 2020-04-29 16:48 etc/mtab -> /proc/mounts -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/resolv.conf -rwxr-xr-x 0/0 6089990 2018-04-18 10:22 hello-openshift drwxr-xr-x 0/0 0 2020-04-29 16:48 proc/ drwxr-xr-x 0/0 0 2020-04-29 16:48 sys/ Run another command in the running container. You can reach into the running container to run another command. The typical use case is to run a shell command, so you can use the shell to navigate within the container and run other commands. However, our image is tiny, and there is no built-in shell. For the purpose of this lab, we'll execute the same command again: docker exec -ti hello1 /hello-openshift`. Running this command again in the same container results in an error, because there is already another copy running in the background that is bound to the ports 8080 and 8888: serving on 8888 serving on 8080 panic: ListenAndServe: listen tcp :8888: bind: address already in use ... Stop the containers: docker stop hello1 docker stop hello2 List running containers: docker ps There should not be any running containers listed CONTAINER ID IMAGE COMMAND CREATED List all containers, including stopped containers: docker ps -a You should see the two cntainers listed, although not running; STATUS is Exited CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5a62f8527b44 openshift/hello-openshift \"/hello-openshift\" 28 minutes ago Exited (2) 28 seconds ago hello2 c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 31 minutes ago Exited (2) 32 seconds ago hello1 Restart a stopped container: docker restart hello1 List running containers: docker ps The hello-openshift container is now running again. CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 33 minutes ago Up 8 seconds 0.0.0.0:8080->8080/tcp, 0.0.0.0:8888->8888/tcp hello1 Stop the container: docker stop hello1 Remove stopped containers, and note that there are no more containers: docker rm hello1 docker rm hello2 docker ps -a Remove the image from local cache: a. View current images: docker images Example output: REPOSITORY TAG IMAGE ID CREATED SIZE openshift/hello-openshift latest 7af3297a3fb4 2 years ago 6.09MB b. Remove the image: docker rmi openshift/hello-openshift Example output: Untagged: openshift/hello-openshift:latest Untagged: openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e Deleted: sha256:7af3297a3fb4487b740ed6798163f618e6eddea1ee5fa0ba340329fcae31c8f6 Deleted: sha256:8fd6a1ece3ceceae6d714004614bae5b581c83ab962d838ef88ce760583dcb80 Deleted: sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef c. Check that the image has been removed: docker images Example output: REPOSITORY TAG IMAGE ID CREATED SIZE Build and Run Your Own Image We use a Containerfile , which contains the instructions to create the new layers of your image. For those familiar with docker, the Containerfile is equivalent to Dockerfile . Recall that an image contains the entire file system that you want to use to run your virtual process in a container. For this sample, we are building a new image for a Java EE web application ServletApp.war . It is configured to run on the WebSphere Liberty Runtime. The configuration file for the server is in the server.xml . Change directory to openshift-workshop-was/labs/Openshift/HelloContainer cd openshift-workshop-was/labs/Openshift/HelloContainer Review the provided Containerfile from the directory: cat Containerfile Content of Containerfile : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi COPY server.xml /config COPY ServletApp.war /config/dropins/app.war RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml To create a new image, you start with a pre-existing image. The first line FROM specifies the existing image to be used as the base. If this is not in the local registry, it will be pulled from a remote registry such as docker hub. The base image we are using, ibmcom/websphere-liberty , is already prepackaged for us and made available on docker hub. The second line COPY is a straight copy of the file server.xml from the local directory to /config/server.xml in the image. This adds a new layer to the image with the actual server configuration to be used. The third line, another COPY , copies ServletApp.war from the current directory into a new layer in the image you are creating, at the location /config/dropins/app.war . The last line RUN runs the installUtility command within the image to install additional features required to run the server as specified in server.xml . You can use the RUN command to run any command that is available within the image to customize the image itself. Run the build. Ensure you include . at the end of the command (the dot indicates using the file from the current directory): docker build -t app -f Containerfile . The -t option tags the name of the image as app . The -f option specifies the name of the Containerfile . The build command runs the commands in Containerfile to build a new image called app . Example output: Sending build context to Docker daemon 25.6kB Step 1/4 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi kernel-java8-ibmjava-ubi: Pulling from ibmcom/websphere-liberty ee2244abc66f: Pull complete befb03b11956: Pull complete 137dc88f6a93: Pull complete 5bdd69a33184: Pull complete d4e2554981d7: Pull complete 32c91bc0f2e1: Pull complete db7e931336a9: Pull complete 3b32f9956ae2: Pull complete 304584ffa0a2: Pull complete 9f6da4c82b7e: Pull complete b6fa5b2e2325: Pull complete Digest: sha256:d76f79695afe2f653fc7b272f9a629105446e6b78ff0d733d494c93ff05728e7 Status: Downloaded newer image for ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> 4d9265befb26 Step 2/4 : COPY server.xml /config ---> 4a02d03d3725 Step 3/4 : COPY ServletApp.war /config/dropins/app.war ---> b2def2a0feac Step 4/4 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Running in 5f5b05aec1ae Checking for missing features required by the server ... The server requires the following additional features: appsecurity-2.0 servlet-3.0. Installing features from the repository ... Establishing a connection to the configured repositories ... This process might take several minutes to complete. Successfully connected to all configured repositories. Preparing assets for installation. This process might take several minutes to complete. Additional Liberty features must be installed for this server. To install the additional features, review and accept the feature license agreement: The --acceptLicense argument was found. This indicates that you have accepted the terms of the license agreement. Step 1 of 12: Downloading ssl-1.0 ... Step 2 of 12: Installing ssl-1.0 ... Step 3 of 12: Downloading appSecurity-2.0 ... Step 4 of 12: Installing appSecurity-2.0 ... Step 5 of 12: Downloading servlet-3.0 ... Step 6 of 12: Installing servlet-3.0 ... Step 7 of 12: Downloading jndi-1.0 ... Step 8 of 12: Installing jndi-1.0 ... Step 9 of 12: Downloading distributedMap-1.0 ... Step 10 of 12: Installing distributedMap-1.0 ... Step 11 of 12: Validating installed fixes ... Step 12 of 12: Cleaning up temporary files ... All assets were successfully installed. Start product validation... Product validation completed successfully. Removing intermediate container 5f5b05aec1ae ---> e1c6bfabda76 Successfully built e1c6bfabda76 Successfully tagged app:latest List the images to see that the new image app is built: docker images Note that the base image, ibmcom/websphere-liberty has also been pulled into the local registry. REPOSITORY TAG IMAGE ID CREATED SIZE app latest baa6bb9ad29d 2 minutes ago 544 MB ibmcom/websphere-liberty kernel-java8-ibmjava-ubi 7ea3d0a2b3fe 4 hours ago 544 MB Start the container. Note: You are running with both http and https ports: docker run -d -p 9080:9080 -p 9443:9443 --name=app-instance app Access the application running in the container: a. Open the Firefox Web Browswer and go to URL: http://localhost:9080/app b. Check that it renders a page showing Simple Servlet ran successfully . c. Also point your browser to 9443: https://localhost:9443/app List the running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 595cdc49c710 app \"/opt/ibm/helpers/ru\u2026\" 8 minutes ago Up 8 minutes 0.0.0.0:9080->9080/tcp, 0.0.0.0:9443->9443/tcp app-instance Access the logs to your container: docker logs -f app-instance Use Ctrl-C To exit. Remote shell into your running container to poke around: docker exec -it app-instance /bin/sh In the shell session, a. Run whoami and then run id , note you're not running as root. whoami id b. Note that this is a stripped down environment where many commands are not available. For example, try the following: which ps to see running processes. cd /logs to find the log files. cd /liberty/wlp to find the location of the liberty install cd /liberty/wlp/usr/servers/defaultServer to find the server configuration. cd /opt/ibm/wlp/output/defaultServer to find the workarea files required by the server runtime. Exit from the container: exit Cleanup: docker stop app-instance docker rm app-instance Managing Image Versions There is no built-in versioning for container images. However, you may use a tagging convention to version your images. The convention is to use major.minor.patch , such as 1.3.5 . The default tag if you don't specify one is latest . Let's assume that the first version we will build for our environment is 1.3.5. (The earlier versions are built in a different environment.) Run the commands to tag the latest app image for our first version: docker tag app app:1 docker tag app app:1.3 docker tag app app:1.3.5 List the images again: docker images And the output: REPOSITORY TAG IMAGE ID CREATED SIZE app 1 d98cbdf82a0d 21 hours ago 542MB app 1.3 d98cbdf82a0d 21 hours ago 542MB app 1.3.5 d98cbdf82a0d 21 hours ago 542MB app latest d98cbdf82a0d 21 hours ago 542MB Note that all the different tags are currently associated with the same image, as they have the same image ID. After tagging, the command docker run app:<version> ... or docker pull app:<version> ... resolves the available versions as follows: app:1 resolves to the latest 1.x.x version, which in this case is 1.3.5 . app:1.3 resolves to the latest 1.3.x version, which in this case is the 1.3.5 app:1.3.5 resolves to the exact version 1.3.5 . After you build a new patch image containing defect fixes, you want to manage the tags for the new image so that a new docker run app:<version> ... or docker pull app:<version> ... command resolves the images as follows: app:1.3.5 : resolves to the existing 1.3.5 image. app:1.3.6 : resolves to the new image app:1.3 : resolves to the new image. app:1 : resolves to the new image Let's simulate a defect fix by building a new image using Containerfile1 instead of Containerfile : docker build -t app -f Containerfile1 . Example output: Sending build context to Docker daemon 32.26kB Step 1/5 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> bb79b9e26fd3 Step 2/5 : COPY server.xml /config ---> Using cache ---> f10659bc62b2 Step 3/5 : COPY ServletApp.war /config/dropins/app.war ---> Using cache ---> 24d85579e404 Step 4/5 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Using cache ---> 5e924d776a9c Step 5/5 : RUN echo test1 > /config/test1 ---> Running in 08e6135b00f5 Removing intermediate container 08e6135b00f5 ---> 69d20332a5e0 Successfully built 69d20332a5e0 Successfully tagged app:latest Tag it as follows : docker tag app app:1 docker tag app app:1.3 docker tag app app:1.3.6 Verify that these are the same images: app:1 , app:1.3 , app:1.3.6 . A new minor version involves compatible changes beyond just bug fixes. After you build a new minor version image, you want to manage the tags such that: app:1.3.5 : resolves to the existing 1.3.5 image. app:1.3.6 : resolves to the existing 1.3.6 image app:1.4.0 : resolves to the new image app:1.3 : resolves to the existing 1.3.6 image. app:1.4 : resolves to the new image. app:1 : resolves to the new image Build a new image using Containerfile2 : docker build -t app -f Containerfile2 . The Output: Sending build context to Docker daemon 32.26kB Step 1/6 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> bb79b9e26fd3 Step 2/6 : COPY server.xml /config ---> Using cache ---> f10659bc62b2 Step 3/6 : COPY ServletApp.war /config/dropins/app.war ---> Using cache ---> 24d85579e404 Step 4/6 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Using cache ---> 5e924d776a9c Step 5/6 : RUN echo test1 > /config/test1 ---> Using cache ---> 69d20332a5e0 Step 6/6 : RUN echo test2 > /config/test2 ---> Running in 96ea24b9ba66 Removing intermediate container 96ea24b9ba66 ---> 31b27169b3bc Successfully built 31b27169b3bc Tag it as follows: docker tag app app:1 docker tag app app:1.4 docker tag app app:1.4.0 Verify the following: 1 , 1.4 , and 1.4.0 are the same image 1.3 and 1.3.6 are the same image Congratulations! You have completed the Introduction to Containerization lab. Next Please follow the link to do the next lab Introduction to Container Orchestration using Openshift : Introduction to Container Orchestration using Openshift","title":"Introduction to Containerization"},{"location":"basic-labs/HelloContainer/#introduction-to-containerization","text":"","title":"Introduction to Containerization"},{"location":"basic-labs/HelloContainer/#table-of-contents","text":"Introduction to Containerization Table of Contents Background Prerequisites What is a Container Login to the VM Check your environment Run a pre-built image Build and Run Your Own Image Managing Image Versions Extra Credit Next","title":"Table of Contents"},{"location":"basic-labs/HelloContainer/#background","text":"If you are expecting a lab about docker , you are at the right place. This lab will introduce you to the basic concepts of containerization, including: What are containers and container images How to start, stop, and remove containers. How to create container images How to version container images","title":"Background"},{"location":"basic-labs/HelloContainer/#prerequisites","text":"You have podman or docker installed. Only docker is installed for this lab. You have access to the internet. You have cloned this lab from github. If not, follows these steps: git clone https://github.com/IBM/openshift-workshop-was.git cd openshift-workshop-was/labs/Openshift/HelloContainer","title":"Prerequisites"},{"location":"basic-labs/HelloContainer/#what-is-a-container","text":"Compared to virtual machines, containers supports virtualization at the process level. Think of them as virtual processes. The isolation abstraction provided by the operating system makes the process think that it's running in its own virtual machine. As processes, containers may be created, started, and stopped much more quickly than virtual machines. Everything you need to run your application, from the operating system and up, is stored in a special file called a container image. Container images are self contained and portable. You may run one or more instances anywhere. And you don't have to worry about missing prerequisites, because all prerequisites are stored in the image. Container images are created via tools such as docker or podman . Existing images are hosted in container registries. For example, docker hub, or registry.access.redhat.com, or your own internal registry. If you need more background on containers: https://www.docker.com/resources/what-container","title":"What is a Container"},{"location":"basic-labs/HelloContainer/#login-to-the-vm","text":"If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password. Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window.","title":"Login to the VM"},{"location":"basic-labs/HelloContainer/#check-your-environment","text":"Open a terminal window from within your VM. List version of docker: docker --version Example output: Docker version 20.10.7, build f0df350 For more background on docker command line: https://docs.docker.com/engine/reference/commandline/cli/","title":"Check your environment"},{"location":"basic-labs/HelloContainer/#run-a-pre-built-image","text":"Container images must be available locally before they can be run. To list available local images: docker images Note: no container inages are listed at this point REPOSITORY TAG IMAGE ID CREATED SIZE Images are hosted in container registries. The default container registry for docker is docker hub, located at https://hub.docker.com. Let's pull a test image from docker hub: docker pull openshift/hello-openshift And the output: Using default tag: latest latest: Pulling from openshift/hello-openshift 4f4fb700ef54: Pull complete 8b32988996c5: Pull complete Digest: sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e Status: Downloaded newer image for openshift/hello-openshift:latest docker.io/openshift/hello-openshift:latest List available local images again: docker images The hello-openshift image is now listed REPOSITORY TAG IMAGE ID CREATED SIZE openshift/hello-openshift latest 7af3297a3fb4 2 years ago 6.09MB Inspect the image metadata: docker inspect openshift/hello-openshift Note : It exposes two ports: 8080 and 8888 It runs as user 1001 The entry point executable is /hello-openshift [ { \"Id\": \"sha256:7af3297a3fb4487b740ed6798163f618e6eddea1ee5fa0ba340329fcae31c8f6\", \"RepoTags\": [ \"openshift/hello-openshift:latest\" ], \"RepoDigests\": [ \"openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e\" ], ... \"Config\": { \"User\": \"1001\", ... \"ExposedPorts\": { \"8080/tcp\": {}, \"8888/tcp\": {} }, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], ... \"Entrypoint\": [ \"/hello-openshift\" ] ... } ] Run the image in an container: docker run --name hello1 -d -p 8080:8080 -p 8888:8888 openshift/hello-openshift Note that: The --name option gives the container a name. The -d option runs the command in the background as a daemon The -p option maps the port on the host to the port in the container. Through virtual networking, the port within the container is always the same for all running instances. But to support multiple concurrent running instances, the actual port on the host must be different for each instance. When you start the container, you may assign a new port on the host dynamically. The output of the command is the container ID for the running container. If the container starts successfully, the executable specified by the Entrypoint in the metadata is run. For our sample, it is /hello-openshift . Access the application in the container. a. Open the Firefox Web Browser from inside of the VM. b. Go to the URL http://localhost:8080 c. Also try port 8888 Run another instance of the same image. Note that this new instance is assigned new port numbers 8081 and 8889 on the host. This is so that they don't conflict with the ports 8080 and 8888 already allocated to the first instance. docker run --name hello2 -d -p 8081:8080 -p 8889:8888 openshift/hello-openshift Question: How does this compare to the time it takes to start a new virtual machine? Access the application in the new container the same way. a. Return to the Firefox Web Browser but instead go to the URL http://localhost:8081 b. Also try port 8889 Verify there are two containers running in the same host: docker ps : CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5a62f8527b44 openshift/hello-openshift \"/hello-openshift\" About a minute ago Up About a minute 0.0.0.0:8081->8080/tcp, 0.0.0.0:8889->8888/tcp hello2 c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 4 minutes ago Up 4 minutes 0.0.0.0:8080->8080/tcp, 0.0.0.0:8888->8888/tcp hello1 View the logs: docker logs hello1 And the output: serving on 8888 serving on 8080 View the logs on the second container: docker logs hello2 And the output: serving on 8888 serving on 8080 Note: within the container, each instance behaves as if it's running in its own virtual environment, and has opened the same ports. Outside of the container, different ports are opened. To export the file system of a running container: docker export hello1 > hello1.tar List the files on the file system: tar -tvf hello1.tar Note that this is a very small image. -rwxr-xr-x 0/0 0 2020-04-29 16:48 .dockerenv drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/ -rwxr-xr-x 0/0 0 2020-04-29 16:48 dev/console drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/pts/ drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/shm/ drwxr-xr-x 0/0 0 2020-04-29 16:48 etc/ -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/hostname -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/hosts lrwxrwxrwx 0/0 0 2020-04-29 16:48 etc/mtab -> /proc/mounts -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/resolv.conf -rwxr-xr-x 0/0 6089990 2018-04-18 10:22 hello-openshift drwxr-xr-x 0/0 0 2020-04-29 16:48 proc/ drwxr-xr-x 0/0 0 2020-04-29 16:48 sys/ Run another command in the running container. You can reach into the running container to run another command. The typical use case is to run a shell command, so you can use the shell to navigate within the container and run other commands. However, our image is tiny, and there is no built-in shell. For the purpose of this lab, we'll execute the same command again: docker exec -ti hello1 /hello-openshift`. Running this command again in the same container results in an error, because there is already another copy running in the background that is bound to the ports 8080 and 8888: serving on 8888 serving on 8080 panic: ListenAndServe: listen tcp :8888: bind: address already in use ... Stop the containers: docker stop hello1 docker stop hello2 List running containers: docker ps There should not be any running containers listed CONTAINER ID IMAGE COMMAND CREATED List all containers, including stopped containers: docker ps -a You should see the two cntainers listed, although not running; STATUS is Exited CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5a62f8527b44 openshift/hello-openshift \"/hello-openshift\" 28 minutes ago Exited (2) 28 seconds ago hello2 c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 31 minutes ago Exited (2) 32 seconds ago hello1 Restart a stopped container: docker restart hello1 List running containers: docker ps The hello-openshift container is now running again. CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 33 minutes ago Up 8 seconds 0.0.0.0:8080->8080/tcp, 0.0.0.0:8888->8888/tcp hello1 Stop the container: docker stop hello1 Remove stopped containers, and note that there are no more containers: docker rm hello1 docker rm hello2 docker ps -a Remove the image from local cache: a. View current images: docker images Example output: REPOSITORY TAG IMAGE ID CREATED SIZE openshift/hello-openshift latest 7af3297a3fb4 2 years ago 6.09MB b. Remove the image: docker rmi openshift/hello-openshift Example output: Untagged: openshift/hello-openshift:latest Untagged: openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e Deleted: sha256:7af3297a3fb4487b740ed6798163f618e6eddea1ee5fa0ba340329fcae31c8f6 Deleted: sha256:8fd6a1ece3ceceae6d714004614bae5b581c83ab962d838ef88ce760583dcb80 Deleted: sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef c. Check that the image has been removed: docker images Example output: REPOSITORY TAG IMAGE ID CREATED SIZE","title":"Run a pre-built image"},{"location":"basic-labs/HelloContainer/#build-and-run-your-own-image","text":"We use a Containerfile , which contains the instructions to create the new layers of your image. For those familiar with docker, the Containerfile is equivalent to Dockerfile . Recall that an image contains the entire file system that you want to use to run your virtual process in a container. For this sample, we are building a new image for a Java EE web application ServletApp.war . It is configured to run on the WebSphere Liberty Runtime. The configuration file for the server is in the server.xml . Change directory to openshift-workshop-was/labs/Openshift/HelloContainer cd openshift-workshop-was/labs/Openshift/HelloContainer Review the provided Containerfile from the directory: cat Containerfile Content of Containerfile : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi COPY server.xml /config COPY ServletApp.war /config/dropins/app.war RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml To create a new image, you start with a pre-existing image. The first line FROM specifies the existing image to be used as the base. If this is not in the local registry, it will be pulled from a remote registry such as docker hub. The base image we are using, ibmcom/websphere-liberty , is already prepackaged for us and made available on docker hub. The second line COPY is a straight copy of the file server.xml from the local directory to /config/server.xml in the image. This adds a new layer to the image with the actual server configuration to be used. The third line, another COPY , copies ServletApp.war from the current directory into a new layer in the image you are creating, at the location /config/dropins/app.war . The last line RUN runs the installUtility command within the image to install additional features required to run the server as specified in server.xml . You can use the RUN command to run any command that is available within the image to customize the image itself. Run the build. Ensure you include . at the end of the command (the dot indicates using the file from the current directory): docker build -t app -f Containerfile . The -t option tags the name of the image as app . The -f option specifies the name of the Containerfile . The build command runs the commands in Containerfile to build a new image called app . Example output: Sending build context to Docker daemon 25.6kB Step 1/4 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi kernel-java8-ibmjava-ubi: Pulling from ibmcom/websphere-liberty ee2244abc66f: Pull complete befb03b11956: Pull complete 137dc88f6a93: Pull complete 5bdd69a33184: Pull complete d4e2554981d7: Pull complete 32c91bc0f2e1: Pull complete db7e931336a9: Pull complete 3b32f9956ae2: Pull complete 304584ffa0a2: Pull complete 9f6da4c82b7e: Pull complete b6fa5b2e2325: Pull complete Digest: sha256:d76f79695afe2f653fc7b272f9a629105446e6b78ff0d733d494c93ff05728e7 Status: Downloaded newer image for ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> 4d9265befb26 Step 2/4 : COPY server.xml /config ---> 4a02d03d3725 Step 3/4 : COPY ServletApp.war /config/dropins/app.war ---> b2def2a0feac Step 4/4 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Running in 5f5b05aec1ae Checking for missing features required by the server ... The server requires the following additional features: appsecurity-2.0 servlet-3.0. Installing features from the repository ... Establishing a connection to the configured repositories ... This process might take several minutes to complete. Successfully connected to all configured repositories. Preparing assets for installation. This process might take several minutes to complete. Additional Liberty features must be installed for this server. To install the additional features, review and accept the feature license agreement: The --acceptLicense argument was found. This indicates that you have accepted the terms of the license agreement. Step 1 of 12: Downloading ssl-1.0 ... Step 2 of 12: Installing ssl-1.0 ... Step 3 of 12: Downloading appSecurity-2.0 ... Step 4 of 12: Installing appSecurity-2.0 ... Step 5 of 12: Downloading servlet-3.0 ... Step 6 of 12: Installing servlet-3.0 ... Step 7 of 12: Downloading jndi-1.0 ... Step 8 of 12: Installing jndi-1.0 ... Step 9 of 12: Downloading distributedMap-1.0 ... Step 10 of 12: Installing distributedMap-1.0 ... Step 11 of 12: Validating installed fixes ... Step 12 of 12: Cleaning up temporary files ... All assets were successfully installed. Start product validation... Product validation completed successfully. Removing intermediate container 5f5b05aec1ae ---> e1c6bfabda76 Successfully built e1c6bfabda76 Successfully tagged app:latest List the images to see that the new image app is built: docker images Note that the base image, ibmcom/websphere-liberty has also been pulled into the local registry. REPOSITORY TAG IMAGE ID CREATED SIZE app latest baa6bb9ad29d 2 minutes ago 544 MB ibmcom/websphere-liberty kernel-java8-ibmjava-ubi 7ea3d0a2b3fe 4 hours ago 544 MB Start the container. Note: You are running with both http and https ports: docker run -d -p 9080:9080 -p 9443:9443 --name=app-instance app Access the application running in the container: a. Open the Firefox Web Browswer and go to URL: http://localhost:9080/app b. Check that it renders a page showing Simple Servlet ran successfully . c. Also point your browser to 9443: https://localhost:9443/app List the running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 595cdc49c710 app \"/opt/ibm/helpers/ru\u2026\" 8 minutes ago Up 8 minutes 0.0.0.0:9080->9080/tcp, 0.0.0.0:9443->9443/tcp app-instance Access the logs to your container: docker logs -f app-instance Use Ctrl-C To exit. Remote shell into your running container to poke around: docker exec -it app-instance /bin/sh In the shell session, a. Run whoami and then run id , note you're not running as root. whoami id b. Note that this is a stripped down environment where many commands are not available. For example, try the following: which ps to see running processes. cd /logs to find the log files. cd /liberty/wlp to find the location of the liberty install cd /liberty/wlp/usr/servers/defaultServer to find the server configuration. cd /opt/ibm/wlp/output/defaultServer to find the workarea files required by the server runtime. Exit from the container: exit Cleanup: docker stop app-instance docker rm app-instance","title":"Build and Run Your Own Image"},{"location":"basic-labs/HelloContainer/#managing-image-versions","text":"There is no built-in versioning for container images. However, you may use a tagging convention to version your images. The convention is to use major.minor.patch , such as 1.3.5 . The default tag if you don't specify one is latest . Let's assume that the first version we will build for our environment is 1.3.5. (The earlier versions are built in a different environment.) Run the commands to tag the latest app image for our first version: docker tag app app:1 docker tag app app:1.3 docker tag app app:1.3.5 List the images again: docker images And the output: REPOSITORY TAG IMAGE ID CREATED SIZE app 1 d98cbdf82a0d 21 hours ago 542MB app 1.3 d98cbdf82a0d 21 hours ago 542MB app 1.3.5 d98cbdf82a0d 21 hours ago 542MB app latest d98cbdf82a0d 21 hours ago 542MB Note that all the different tags are currently associated with the same image, as they have the same image ID. After tagging, the command docker run app:<version> ... or docker pull app:<version> ... resolves the available versions as follows: app:1 resolves to the latest 1.x.x version, which in this case is 1.3.5 . app:1.3 resolves to the latest 1.3.x version, which in this case is the 1.3.5 app:1.3.5 resolves to the exact version 1.3.5 . After you build a new patch image containing defect fixes, you want to manage the tags for the new image so that a new docker run app:<version> ... or docker pull app:<version> ... command resolves the images as follows: app:1.3.5 : resolves to the existing 1.3.5 image. app:1.3.6 : resolves to the new image app:1.3 : resolves to the new image. app:1 : resolves to the new image Let's simulate a defect fix by building a new image using Containerfile1 instead of Containerfile : docker build -t app -f Containerfile1 . Example output: Sending build context to Docker daemon 32.26kB Step 1/5 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> bb79b9e26fd3 Step 2/5 : COPY server.xml /config ---> Using cache ---> f10659bc62b2 Step 3/5 : COPY ServletApp.war /config/dropins/app.war ---> Using cache ---> 24d85579e404 Step 4/5 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Using cache ---> 5e924d776a9c Step 5/5 : RUN echo test1 > /config/test1 ---> Running in 08e6135b00f5 Removing intermediate container 08e6135b00f5 ---> 69d20332a5e0 Successfully built 69d20332a5e0 Successfully tagged app:latest Tag it as follows : docker tag app app:1 docker tag app app:1.3 docker tag app app:1.3.6 Verify that these are the same images: app:1 , app:1.3 , app:1.3.6 . A new minor version involves compatible changes beyond just bug fixes. After you build a new minor version image, you want to manage the tags such that: app:1.3.5 : resolves to the existing 1.3.5 image. app:1.3.6 : resolves to the existing 1.3.6 image app:1.4.0 : resolves to the new image app:1.3 : resolves to the existing 1.3.6 image. app:1.4 : resolves to the new image. app:1 : resolves to the new image Build a new image using Containerfile2 : docker build -t app -f Containerfile2 . The Output: Sending build context to Docker daemon 32.26kB Step 1/6 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> bb79b9e26fd3 Step 2/6 : COPY server.xml /config ---> Using cache ---> f10659bc62b2 Step 3/6 : COPY ServletApp.war /config/dropins/app.war ---> Using cache ---> 24d85579e404 Step 4/6 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Using cache ---> 5e924d776a9c Step 5/6 : RUN echo test1 > /config/test1 ---> Using cache ---> 69d20332a5e0 Step 6/6 : RUN echo test2 > /config/test2 ---> Running in 96ea24b9ba66 Removing intermediate container 96ea24b9ba66 ---> 31b27169b3bc Successfully built 31b27169b3bc Tag it as follows: docker tag app app:1 docker tag app app:1.4 docker tag app app:1.4.0 Verify the following: 1 , 1.4 , and 1.4.0 are the same image 1.3 and 1.3.6 are the same image Congratulations! You have completed the Introduction to Containerization lab.","title":"Managing Image Versions"},{"location":"basic-labs/HelloContainer/#next","text":"Please follow the link to do the next lab Introduction to Container Orchestration using Openshift : Introduction to Container Orchestration using Openshift","title":"Next"},{"location":"basic-labs/HelloContainer/README - Copy/","text":"Introduction to Containerization Table of Contents Introduction to Containerization Table of Contents Background Prerequisites What is a Container Login to the VM Check your environment Run a pre-built image Build and Run Your Own Image Managing Image Versions Extra Credit Next Background If you are expecting a lab about docker , you are at the right place. This lab will introduce you to the basic concepts of containerization, including: What are containers and container images How to start, stop, and remove containers. How to create container images How to version container images Prerequisites You have podman or docker installed. Only docker is installed for this lab. You have access to the internet. You have cloned this lab from github. If not, follows these steps: git clone https://github.com/IBM/openshift-workshop-was.git cd openshift-workshop-was/labs/Openshift/HelloContainer What is a Container Compared to virtual machines, containers supports virtualization at the process level. Think of them as virtual processes. The isolation abstraction provided by the operating system makes the process think that it's running in its own virtual machine. As processes, containers may be created, started, and stopped much more quickly than virtual machines. Everything you need to run your application, from the operating system and up, is stored in a special file called a container image. Container images are self contained and portable. You may run one or more instances anywhere. And you don't have to worry about missing prerequisites, because all prerequisites are stored in the image. Container images are created via tools such as docker or podman . Existing images are hosted in container registries. For example, docker hub, or registry.access.redhat.com, or your own internal registry. If you need more background on containers: https://www.docker.com/resources/what-container Login to the VM If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password: \\ Password: engageibm Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window. Check your environment Open a terminal window from within your VM. List version of docker: docker --version Example output: Docker version 20.10.7, build f0df350 For more background on docker command line: https://docs.docker.com/engine/reference/commandline/cli/ Run a pre-built image Container images must be available locally before they can be run. To list available local images: docker images REPOSITORY TAG IMAGE ID CREATED SIZE Images are hosted in container registries. The default container registry for docker is docker hub, located at https://hub.docker.com. Let's pull a test image from docker hub: docker pull openshift/hello-openshift And the output: Using default tag: latest latest: Pulling from openshift/hello-openshift 4f4fb700ef54: Pull complete 8b32988996c5: Pull complete Digest: sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e Status: Downloaded newer image for openshift/hello-openshift:latest docker.io/openshift/hello-openshift:latest List available local images again: docker images REPOSITORY TAG IMAGE ID CREATED SIZE openshift/hello-openshift latest 7af3297a3fb4 2 years ago 6.09MB Inspect the image metadata: docker inspect openshift/hello-openshift Note that: It exposes two ports: 8080 and 8888 It runs as user 1001 The entry point executable is /hello-openshift [ { \"Id\": \"sha256:7af3297a3fb4487b740ed6798163f618e6eddea1ee5fa0ba340329fcae31c8f6\", \"RepoTags\": [ \"openshift/hello-openshift:latest\" ], \"RepoDigests\": [ \"openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e\" ], ... \"Config\": { \"User\": \"1001\", ... \"ExposedPorts\": { \"8080/tcp\": {}, \"8888/tcp\": {} }, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], ... \"Entrypoint\": [ \"/hello-openshift\" ] ... } ] Run the image in an container: docker run --name hello1 -d -p 8080:8080 -p 8888:8888 openshift/hello-openshift Note that: The --name option gives the container a name. The -d option runs the command in the background as a daemon The -p option maps the port on the host to the port in the container. Through virtual networking, the port within the container is always the same for all running instances. But to support multiple concurrent running instances, the actual port on the host must be different for each instance. When you start the container, you may assign a new port on the host dynamically. The output of the command is the container ID for the running container. If the container starts successfully, the executable specified by the Entrypoint in the metadata is run. For our sample, it is /hello-openshift . Access the application in the container. Open the Firefox Web Browser from inside of the VM. Go to the URL http://localhost:8080 Also try port 8888 Run another instance of the same image. Note that this new instance is assigned new port numbers 8081 and 8889 on the host. This is so that they don't conflict with the ports 8080 and 8888 already allocated to the first instance. docker run --name hello2 -d -p 8081:8080 -p 8889:8888 openshift/hello-openshift Question: how does this compare to the time it takes to start a new virtual machine? Access the application in the new container the same way. Return to the Firefox Web Browser but instead go to the URL http://localhost:8081 Also try port 8889 Verify there are two containers running in the same host: docker ps : CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5a62f8527b44 openshift/hello-openshift \"/hello-openshift\" About a minute ago Up About a minute 0.0.0.0:8081->8080/tcp, 0.0.0.0:8889->8888/tcp hello2 c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 4 minutes ago Up 4 minutes 0.0.0.0:8080->8080/tcp, 0.0.0.0:8888->8888/tcp hello1 View the logs: docker logs hello1 And the output: ``` serving on 8888 serving on 8080 ``` View the logs on the second container: docker logs hello2 And the output: serving on 8888 serving on 8080 Note: within the container, each instance behaves as if it's running in its own virtual environment, and has opened the same ports. Outside of the container, different ports are opened. To export the file system of a running container: docker export hello1 > hello1.tar List the files on the file system: tar -tvf hello1.tar Note that this is a very small image. -rwxr-xr-x 0/0 0 2020-04-29 16:48 .dockerenv drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/ -rwxr-xr-x 0/0 0 2020-04-29 16:48 dev/console drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/pts/ drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/shm/ drwxr-xr-x 0/0 0 2020-04-29 16:48 etc/ -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/hostname -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/hosts lrwxrwxrwx 0/0 0 2020-04-29 16:48 etc/mtab -> /proc/mounts -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/resolv.conf -rwxr-xr-x 0/0 6089990 2018-04-18 10:22 hello-openshift drwxr-xr-x 0/0 0 2020-04-29 16:48 proc/ drwxr-xr-x 0/0 0 2020-04-29 16:48 sys/ Run another command in the running container. You can reach into the running container to run another command. The typical use case is to run a shell command, so you can use the shell to navigate within the container and run other commands. However, our image is tiny, and there is no built-in shell. For the purpose of this lab, we'll execute the same command again: docker exec -ti hello1 /hello-openshift . Running this command again in the same container results in an error, because there is already another copy running in the background that is bound to the ports 8080 and 8888: serving on 8888 serving on 8080 panic: ListenAndServe: listen tcp :8888: bind: address already in use ... Stop the containers: docker stop hello1 docker stop hello2 List running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES List all containers, including stopped ones: docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5a62f8527b44 openshift/hello-openshift \"/hello-openshift\" 28 minutes ago Exited (2) 28 seconds ago hello2 c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 31 minutes ago Exited (2) 32 seconds ago hello1 Restart a stopped container: docker restart hello1 List running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 33 minutes ago Up 8 seconds 0.0.0.0:8080->8080/tcp, 0.0.0.0:8888->8888/tcp hello1 Stop the container: docker stop hello1 Remove stopped containers, and note that there are no more containers: docker rm hello1 docker rm hello2 docker ps -a Remove the image from local cache: View current images: docker images Example output: REPOSITORY TAG IMAGE ID CREATED SIZE openshift/hello-openshift latest 7af3297a3fb4 2 years ago 6.09MB Remove the image: docker rmi openshift/hello-openshift Example output: Untagged: openshift/hello-openshift:latest Untagged: openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e Deleted: sha256:7af3297a3fb4487b740ed6798163f618e6eddea1ee5fa0ba340329fcae31c8f6 Deleted: sha256:8fd6a1ece3ceceae6d714004614bae5b581c83ab962d838ef88ce760583dcb80 Deleted: sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef Check that the image has been removed: docker images Example output: REPOSITORY TAG IMAGE ID CREATED SIZE Build and Run Your Own Image We use a Containerfile , which contains the instructions to create the new layers of your image. For those familiar with docker, the Containerfile is equivalent to Dockerfile . Recall that an image contains the entire file system that you want to use to run your virtual process in a container. For this sample, we are building a new image for a Java EE web application ServletApp.war. It is configured to run on the WebSphere Liberty Runtime. The configuration file for the server is in the server.xml. Change directory to openshift-workshop-was/labs/Openshift/HelloContainer cd openshift-workshop-was/labs/Openshift/HelloContainer Review the provided Containerfile from the directory: cat Containerfile Content of Containerfile : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi COPY server.xml /config COPY ServletApp.war /config/dropins/app.war RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml To create a new image, you start with a pre-existing image. The first line FROM specifies the existing image to be used as the base. If this is not in the local registry, it will be pulled from a remote registry such as docker hub. The base image we are using, ibmcom/websphere-liberty , is already prepackaged for us and made available on docker hub. The second line COPY is a straight copy of the file server.xml from the local directory to /config/server.xml in the image. This adds a new layer to the image with the actual server configuration to be used. The third line, another COPY , copies ServletApp.war from the current directory into a new layer in the image you are creating, at the location /config/dropins/app.war . The last line RUN runs the installUtility command within the image to install additional features required to run the server as specified in server.xml . You can use the RUN command to run any command that is available within the image to customize the image itself. Run the build. Ensure you include . at the end of the command (the dot indicates using the file from the current directory): docker build -t app -f Containerfile . The -t option tags the name of the image as app . The -f option specifies the name of the Containerfile . The build command runs the commands in Containerfile to build a new image called app . Example output: Sending build context to Docker daemon 25.6kB Step 1/4 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi kernel-java8-ibmjava-ubi: Pulling from ibmcom/websphere-liberty ee2244abc66f: Pull complete befb03b11956: Pull complete 137dc88f6a93: Pull complete 5bdd69a33184: Pull complete d4e2554981d7: Pull complete 32c91bc0f2e1: Pull complete db7e931336a9: Pull complete 3b32f9956ae2: Pull complete 304584ffa0a2: Pull complete 9f6da4c82b7e: Pull complete b6fa5b2e2325: Pull complete Digest: sha256:d76f79695afe2f653fc7b272f9a629105446e6b78ff0d733d494c93ff05728e7 Status: Downloaded newer image for ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> 4d9265befb26 Step 2/4 : COPY server.xml /config ---> 4a02d03d3725 Step 3/4 : COPY ServletApp.war /config/dropins/app.war ---> b2def2a0feac Step 4/4 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Running in 5f5b05aec1ae Checking for missing features required by the server ... The server requires the following additional features: appsecurity-2.0 servlet-3.0. Installing features from the repository ... Establishing a connection to the configured repositories ... This process might take several minutes to complete. Successfully connected to all configured repositories. Preparing assets for installation. This process might take several minutes to complete. Additional Liberty features must be installed for this server. To install the additional features, review and accept the feature license agreement: The --acceptLicense argument was found. This indicates that you have accepted the terms of the license agreement. Step 1 of 12: Downloading ssl-1.0 ... Step 2 of 12: Installing ssl-1.0 ... Step 3 of 12: Downloading appSecurity-2.0 ... Step 4 of 12: Installing appSecurity-2.0 ... Step 5 of 12: Downloading servlet-3.0 ... Step 6 of 12: Installing servlet-3.0 ... Step 7 of 12: Downloading jndi-1.0 ... Step 8 of 12: Installing jndi-1.0 ... Step 9 of 12: Downloading distributedMap-1.0 ... Step 10 of 12: Installing distributedMap-1.0 ... Step 11 of 12: Validating installed fixes ... Step 12 of 12: Cleaning up temporary files ... All assets were successfully installed. Start product validation... Product validation completed successfully. Removing intermediate container 5f5b05aec1ae ---> e1c6bfabda76 Successfully built e1c6bfabda76 Successfully tagged app:latest List the images to see that the new image app is built: docker images Note that the base image, ibmcom/websphere-liberty has also been pulled into the local registry. REPOSITORY TAG IMAGE ID CREATED SIZE app latest baa6bb9ad29d 2 minutes ago 544 MB ibmcom/websphere-liberty kernel-java8-ibmjava-ubi 7ea3d0a2b3fe 4 hours ago 544 MB Start the container. Note that you are running with both http and https ports: docker run -d -p 9080:9080 -p 9443:9443 --name=app-instance app Access the application running in the container: Open the Firefox Web Browswer and go to URL http://localhost:9080/app Check that it renders a page showing Simple Servlet ran successfully . Also point your browser to 9443: https://localhost:9443/app List the running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 595cdc49c710 app \"/opt/ibm/helpers/ru\u2026\" 8 minutes ago Up 8 minutes 0.0.0.0:9080->9080/tcp, 0.0.0.0:9443->9443/tcp app-instance Access the logs to your container: docker logs -f app-instance Use Ctrl-C To exit. Remote shell into your running container to poke around: docker exec -it app-instance /bin/sh In the shell session, run whoami and then run id , note you're not running as root. Note that this is a stripped down environment where many commands are not available. For example, try which ps . cd /logs to find the log files cd /liberty/wlp to find the location of the liberty install cd /liberty/wlp/usr/servers/defaultServer to find the server configuration. cd /opt/ibm/wlp/output/defaultServer to find the workarea files required by the server runtime. Exit from the container: exit Cleanup: docker stop app-instance docker rm app-instance Managing Image Versions There is no built-in versioning for container images. However, you may use a tagging convention to version your images. The convention is to use major.minor.patch , such as 1.3.5 . The default tag if you don't specify one is latest . Let's assume that the first version we will build for our environment is 1.3.5. (The earlier versions are built in a different environment.) Run the commands to tag the latest app image for our first version: docker tag app app:1 docker tag app app:1.3 docker tag app app:1.3.5 List the images again: docker images And the output: REPOSITORY TAG IMAGE ID CREATED SIZE app 1 d98cbdf82a0d 21 hours ago 542MB app 1.3 d98cbdf82a0d 21 hours ago 542MB app 1.3.5 d98cbdf82a0d 21 hours ago 542MB app latest d98cbdf82a0d 21 hours ago 542MB Note that all the different tags are currently associated with the same image, as they have the same image ID. After tagging, the command docker run app:<version> ... or docker pull app:<version> ... resolves the available versions as follows: app:1 resolves to the latest 1.x.x version, which in this case is 1.3.5 . app:1.3 resolves to the latest 1.3.x version, which in this case is the 1.3.5 app:1.3.5 resolves to the exact version 1.3.5 . After you build a new patch image containing defect fixes, you want to manage the tags for the new image so that a new docker run app:<version> ... or docker pull app:<version> ... command resolves the images as follows: app:1.3.5 : resolves to the existing 1.3.5 image. app:1.3.6 : resolves to the new image app:1.3 : resolves to the new image. app:1 : resolves to the new image Let's simulate a defect fix by building a new image using Containerfile1 instead of Containerfile : docker build -t app -f Containerfile1 . Example output: Sending build context to Docker daemon 32.26kB Step 1/5 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> bb79b9e26fd3 Step 2/5 : COPY server.xml /config ---> Using cache ---> f10659bc62b2 Step 3/5 : COPY ServletApp.war /config/dropins/app.war ---> Using cache ---> 24d85579e404 Step 4/5 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Using cache ---> 5e924d776a9c Step 5/5 : RUN echo test1 > /config/test1 ---> Running in 08e6135b00f5 Removing intermediate container 08e6135b00f5 ---> 69d20332a5e0 Successfully built 69d20332a5e0 Successfully tagged app:latest Tag it as follows : docker tag app app:1 docker tag app app:1.3 docker tag app app:1.3.6 Verify that these are the same images: app:1 , app:1.3 , app:1.3.6 . A new minor version involves compatible changes beyond just bug fixes. After you build a new minor version image, you want to manage the tags such that: app:1.3.5 : resolves to the existing 1.3.5 image. app:1.3.6 : resolves to the existing 1.3.6 image app:1.4.0 : resolves to the new image app:1.3 : resolves to the existing 1.3.6 image. app:1.4 : resolves to the new image. app:1 : resolves to the new image Build a new image using Containerfile2 : docker build -t app -f Containerfile2 . Sending build context to Docker daemon 32.26kB Step 1/6 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> bb79b9e26fd3 Step 2/6 : COPY server.xml /config ---> Using cache ---> f10659bc62b2 Step 3/6 : COPY ServletApp.war /config/dropins/app.war ---> Using cache ---> 24d85579e404 Step 4/6 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Using cache ---> 5e924d776a9c Step 5/6 : RUN echo test1 > /config/test1 ---> Using cache ---> 69d20332a5e0 Step 6/6 : RUN echo test2 > /config/test2 ---> Running in 96ea24b9ba66 Removing intermediate container 96ea24b9ba66 ---> 31b27169b3bc Successfully built 31b27169b3bc Tag it as follows: docker tag app app:1 docker tag app app:1.4 docker tag app app:1.4.0 Verify that 1 , 1.4 , and 1.4.0 are the same image 1.3 and 1.3.6 are the same image Extra Credit Search the internet for information about multi-stage build. In a single stage build, the final image contains both build and runtime artifacts. A multi-stage build allows you to build with one base image, and copy the result of the build to another base image. This gives you even more control over the output of the build, and the size of the final image. Start another instances of the app image for vertical scaling, but with different port numbers on the host. Point your browser to hub.docker.com , click \"Explore\" and explore the millions of available images. Think about how you would tag a new image at a major version, 2.0.0 . Think about what would be required to manage containers across multiple machines to support horizontal scaling. Congratulations! You have completed the Introduction to Containerization lab. Next Please follow the link to do the next lab Introduction to Container Orchestration using Openshift : - Introduction to Container Orchestration using Openshift","title":"Introduction to Containerization"},{"location":"basic-labs/HelloContainer/README - Copy/#introduction-to-containerization","text":"","title":"Introduction to Containerization"},{"location":"basic-labs/HelloContainer/README - Copy/#table-of-contents","text":"Introduction to Containerization Table of Contents Background Prerequisites What is a Container Login to the VM Check your environment Run a pre-built image Build and Run Your Own Image Managing Image Versions Extra Credit Next","title":"Table of Contents"},{"location":"basic-labs/HelloContainer/README - Copy/#background","text":"If you are expecting a lab about docker , you are at the right place. This lab will introduce you to the basic concepts of containerization, including: What are containers and container images How to start, stop, and remove containers. How to create container images How to version container images","title":"Background"},{"location":"basic-labs/HelloContainer/README - Copy/#prerequisites","text":"You have podman or docker installed. Only docker is installed for this lab. You have access to the internet. You have cloned this lab from github. If not, follows these steps: git clone https://github.com/IBM/openshift-workshop-was.git cd openshift-workshop-was/labs/Openshift/HelloContainer","title":"Prerequisites"},{"location":"basic-labs/HelloContainer/README - Copy/#what-is-a-container","text":"Compared to virtual machines, containers supports virtualization at the process level. Think of them as virtual processes. The isolation abstraction provided by the operating system makes the process think that it's running in its own virtual machine. As processes, containers may be created, started, and stopped much more quickly than virtual machines. Everything you need to run your application, from the operating system and up, is stored in a special file called a container image. Container images are self contained and portable. You may run one or more instances anywhere. And you don't have to worry about missing prerequisites, because all prerequisites are stored in the image. Container images are created via tools such as docker or podman . Existing images are hosted in container registries. For example, docker hub, or registry.access.redhat.com, or your own internal registry. If you need more background on containers: https://www.docker.com/resources/what-container","title":"What is a Container"},{"location":"basic-labs/HelloContainer/README - Copy/#login-to-the-vm","text":"If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password: \\ Password: engageibm Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window.","title":"Login to the VM"},{"location":"basic-labs/HelloContainer/README - Copy/#check-your-environment","text":"Open a terminal window from within your VM. List version of docker: docker --version Example output: Docker version 20.10.7, build f0df350 For more background on docker command line: https://docs.docker.com/engine/reference/commandline/cli/","title":"Check your environment"},{"location":"basic-labs/HelloContainer/README - Copy/#run-a-pre-built-image","text":"Container images must be available locally before they can be run. To list available local images: docker images REPOSITORY TAG IMAGE ID CREATED SIZE Images are hosted in container registries. The default container registry for docker is docker hub, located at https://hub.docker.com. Let's pull a test image from docker hub: docker pull openshift/hello-openshift And the output: Using default tag: latest latest: Pulling from openshift/hello-openshift 4f4fb700ef54: Pull complete 8b32988996c5: Pull complete Digest: sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e Status: Downloaded newer image for openshift/hello-openshift:latest docker.io/openshift/hello-openshift:latest List available local images again: docker images REPOSITORY TAG IMAGE ID CREATED SIZE openshift/hello-openshift latest 7af3297a3fb4 2 years ago 6.09MB Inspect the image metadata: docker inspect openshift/hello-openshift Note that: It exposes two ports: 8080 and 8888 It runs as user 1001 The entry point executable is /hello-openshift [ { \"Id\": \"sha256:7af3297a3fb4487b740ed6798163f618e6eddea1ee5fa0ba340329fcae31c8f6\", \"RepoTags\": [ \"openshift/hello-openshift:latest\" ], \"RepoDigests\": [ \"openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e\" ], ... \"Config\": { \"User\": \"1001\", ... \"ExposedPorts\": { \"8080/tcp\": {}, \"8888/tcp\": {} }, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], ... \"Entrypoint\": [ \"/hello-openshift\" ] ... } ] Run the image in an container: docker run --name hello1 -d -p 8080:8080 -p 8888:8888 openshift/hello-openshift Note that: The --name option gives the container a name. The -d option runs the command in the background as a daemon The -p option maps the port on the host to the port in the container. Through virtual networking, the port within the container is always the same for all running instances. But to support multiple concurrent running instances, the actual port on the host must be different for each instance. When you start the container, you may assign a new port on the host dynamically. The output of the command is the container ID for the running container. If the container starts successfully, the executable specified by the Entrypoint in the metadata is run. For our sample, it is /hello-openshift . Access the application in the container. Open the Firefox Web Browser from inside of the VM. Go to the URL http://localhost:8080 Also try port 8888 Run another instance of the same image. Note that this new instance is assigned new port numbers 8081 and 8889 on the host. This is so that they don't conflict with the ports 8080 and 8888 already allocated to the first instance. docker run --name hello2 -d -p 8081:8080 -p 8889:8888 openshift/hello-openshift Question: how does this compare to the time it takes to start a new virtual machine? Access the application in the new container the same way. Return to the Firefox Web Browser but instead go to the URL http://localhost:8081 Also try port 8889 Verify there are two containers running in the same host: docker ps : CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5a62f8527b44 openshift/hello-openshift \"/hello-openshift\" About a minute ago Up About a minute 0.0.0.0:8081->8080/tcp, 0.0.0.0:8889->8888/tcp hello2 c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 4 minutes ago Up 4 minutes 0.0.0.0:8080->8080/tcp, 0.0.0.0:8888->8888/tcp hello1 View the logs: docker logs hello1 And the output: ``` serving on 8888 serving on 8080 ``` View the logs on the second container: docker logs hello2 And the output: serving on 8888 serving on 8080 Note: within the container, each instance behaves as if it's running in its own virtual environment, and has opened the same ports. Outside of the container, different ports are opened. To export the file system of a running container: docker export hello1 > hello1.tar List the files on the file system: tar -tvf hello1.tar Note that this is a very small image. -rwxr-xr-x 0/0 0 2020-04-29 16:48 .dockerenv drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/ -rwxr-xr-x 0/0 0 2020-04-29 16:48 dev/console drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/pts/ drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/shm/ drwxr-xr-x 0/0 0 2020-04-29 16:48 etc/ -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/hostname -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/hosts lrwxrwxrwx 0/0 0 2020-04-29 16:48 etc/mtab -> /proc/mounts -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/resolv.conf -rwxr-xr-x 0/0 6089990 2018-04-18 10:22 hello-openshift drwxr-xr-x 0/0 0 2020-04-29 16:48 proc/ drwxr-xr-x 0/0 0 2020-04-29 16:48 sys/ Run another command in the running container. You can reach into the running container to run another command. The typical use case is to run a shell command, so you can use the shell to navigate within the container and run other commands. However, our image is tiny, and there is no built-in shell. For the purpose of this lab, we'll execute the same command again: docker exec -ti hello1 /hello-openshift . Running this command again in the same container results in an error, because there is already another copy running in the background that is bound to the ports 8080 and 8888: serving on 8888 serving on 8080 panic: ListenAndServe: listen tcp :8888: bind: address already in use ... Stop the containers: docker stop hello1 docker stop hello2 List running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES List all containers, including stopped ones: docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5a62f8527b44 openshift/hello-openshift \"/hello-openshift\" 28 minutes ago Exited (2) 28 seconds ago hello2 c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 31 minutes ago Exited (2) 32 seconds ago hello1 Restart a stopped container: docker restart hello1 List running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 33 minutes ago Up 8 seconds 0.0.0.0:8080->8080/tcp, 0.0.0.0:8888->8888/tcp hello1 Stop the container: docker stop hello1 Remove stopped containers, and note that there are no more containers: docker rm hello1 docker rm hello2 docker ps -a Remove the image from local cache: View current images: docker images Example output: REPOSITORY TAG IMAGE ID CREATED SIZE openshift/hello-openshift latest 7af3297a3fb4 2 years ago 6.09MB Remove the image: docker rmi openshift/hello-openshift Example output: Untagged: openshift/hello-openshift:latest Untagged: openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e Deleted: sha256:7af3297a3fb4487b740ed6798163f618e6eddea1ee5fa0ba340329fcae31c8f6 Deleted: sha256:8fd6a1ece3ceceae6d714004614bae5b581c83ab962d838ef88ce760583dcb80 Deleted: sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef Check that the image has been removed: docker images Example output: REPOSITORY TAG IMAGE ID CREATED SIZE","title":"Run a pre-built image"},{"location":"basic-labs/HelloContainer/README - Copy/#build-and-run-your-own-image","text":"We use a Containerfile , which contains the instructions to create the new layers of your image. For those familiar with docker, the Containerfile is equivalent to Dockerfile . Recall that an image contains the entire file system that you want to use to run your virtual process in a container. For this sample, we are building a new image for a Java EE web application ServletApp.war. It is configured to run on the WebSphere Liberty Runtime. The configuration file for the server is in the server.xml. Change directory to openshift-workshop-was/labs/Openshift/HelloContainer cd openshift-workshop-was/labs/Openshift/HelloContainer Review the provided Containerfile from the directory: cat Containerfile Content of Containerfile : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi COPY server.xml /config COPY ServletApp.war /config/dropins/app.war RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml To create a new image, you start with a pre-existing image. The first line FROM specifies the existing image to be used as the base. If this is not in the local registry, it will be pulled from a remote registry such as docker hub. The base image we are using, ibmcom/websphere-liberty , is already prepackaged for us and made available on docker hub. The second line COPY is a straight copy of the file server.xml from the local directory to /config/server.xml in the image. This adds a new layer to the image with the actual server configuration to be used. The third line, another COPY , copies ServletApp.war from the current directory into a new layer in the image you are creating, at the location /config/dropins/app.war . The last line RUN runs the installUtility command within the image to install additional features required to run the server as specified in server.xml . You can use the RUN command to run any command that is available within the image to customize the image itself. Run the build. Ensure you include . at the end of the command (the dot indicates using the file from the current directory): docker build -t app -f Containerfile . The -t option tags the name of the image as app . The -f option specifies the name of the Containerfile . The build command runs the commands in Containerfile to build a new image called app . Example output: Sending build context to Docker daemon 25.6kB Step 1/4 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi kernel-java8-ibmjava-ubi: Pulling from ibmcom/websphere-liberty ee2244abc66f: Pull complete befb03b11956: Pull complete 137dc88f6a93: Pull complete 5bdd69a33184: Pull complete d4e2554981d7: Pull complete 32c91bc0f2e1: Pull complete db7e931336a9: Pull complete 3b32f9956ae2: Pull complete 304584ffa0a2: Pull complete 9f6da4c82b7e: Pull complete b6fa5b2e2325: Pull complete Digest: sha256:d76f79695afe2f653fc7b272f9a629105446e6b78ff0d733d494c93ff05728e7 Status: Downloaded newer image for ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> 4d9265befb26 Step 2/4 : COPY server.xml /config ---> 4a02d03d3725 Step 3/4 : COPY ServletApp.war /config/dropins/app.war ---> b2def2a0feac Step 4/4 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Running in 5f5b05aec1ae Checking for missing features required by the server ... The server requires the following additional features: appsecurity-2.0 servlet-3.0. Installing features from the repository ... Establishing a connection to the configured repositories ... This process might take several minutes to complete. Successfully connected to all configured repositories. Preparing assets for installation. This process might take several minutes to complete. Additional Liberty features must be installed for this server. To install the additional features, review and accept the feature license agreement: The --acceptLicense argument was found. This indicates that you have accepted the terms of the license agreement. Step 1 of 12: Downloading ssl-1.0 ... Step 2 of 12: Installing ssl-1.0 ... Step 3 of 12: Downloading appSecurity-2.0 ... Step 4 of 12: Installing appSecurity-2.0 ... Step 5 of 12: Downloading servlet-3.0 ... Step 6 of 12: Installing servlet-3.0 ... Step 7 of 12: Downloading jndi-1.0 ... Step 8 of 12: Installing jndi-1.0 ... Step 9 of 12: Downloading distributedMap-1.0 ... Step 10 of 12: Installing distributedMap-1.0 ... Step 11 of 12: Validating installed fixes ... Step 12 of 12: Cleaning up temporary files ... All assets were successfully installed. Start product validation... Product validation completed successfully. Removing intermediate container 5f5b05aec1ae ---> e1c6bfabda76 Successfully built e1c6bfabda76 Successfully tagged app:latest List the images to see that the new image app is built: docker images Note that the base image, ibmcom/websphere-liberty has also been pulled into the local registry. REPOSITORY TAG IMAGE ID CREATED SIZE app latest baa6bb9ad29d 2 minutes ago 544 MB ibmcom/websphere-liberty kernel-java8-ibmjava-ubi 7ea3d0a2b3fe 4 hours ago 544 MB Start the container. Note that you are running with both http and https ports: docker run -d -p 9080:9080 -p 9443:9443 --name=app-instance app Access the application running in the container: Open the Firefox Web Browswer and go to URL http://localhost:9080/app Check that it renders a page showing Simple Servlet ran successfully . Also point your browser to 9443: https://localhost:9443/app List the running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 595cdc49c710 app \"/opt/ibm/helpers/ru\u2026\" 8 minutes ago Up 8 minutes 0.0.0.0:9080->9080/tcp, 0.0.0.0:9443->9443/tcp app-instance Access the logs to your container: docker logs -f app-instance Use Ctrl-C To exit. Remote shell into your running container to poke around: docker exec -it app-instance /bin/sh In the shell session, run whoami and then run id , note you're not running as root. Note that this is a stripped down environment where many commands are not available. For example, try which ps . cd /logs to find the log files cd /liberty/wlp to find the location of the liberty install cd /liberty/wlp/usr/servers/defaultServer to find the server configuration. cd /opt/ibm/wlp/output/defaultServer to find the workarea files required by the server runtime. Exit from the container: exit Cleanup: docker stop app-instance docker rm app-instance","title":"Build and Run Your Own Image"},{"location":"basic-labs/HelloContainer/README - Copy/#managing-image-versions","text":"There is no built-in versioning for container images. However, you may use a tagging convention to version your images. The convention is to use major.minor.patch , such as 1.3.5 . The default tag if you don't specify one is latest . Let's assume that the first version we will build for our environment is 1.3.5. (The earlier versions are built in a different environment.) Run the commands to tag the latest app image for our first version: docker tag app app:1 docker tag app app:1.3 docker tag app app:1.3.5 List the images again: docker images And the output: REPOSITORY TAG IMAGE ID CREATED SIZE app 1 d98cbdf82a0d 21 hours ago 542MB app 1.3 d98cbdf82a0d 21 hours ago 542MB app 1.3.5 d98cbdf82a0d 21 hours ago 542MB app latest d98cbdf82a0d 21 hours ago 542MB Note that all the different tags are currently associated with the same image, as they have the same image ID. After tagging, the command docker run app:<version> ... or docker pull app:<version> ... resolves the available versions as follows: app:1 resolves to the latest 1.x.x version, which in this case is 1.3.5 . app:1.3 resolves to the latest 1.3.x version, which in this case is the 1.3.5 app:1.3.5 resolves to the exact version 1.3.5 . After you build a new patch image containing defect fixes, you want to manage the tags for the new image so that a new docker run app:<version> ... or docker pull app:<version> ... command resolves the images as follows: app:1.3.5 : resolves to the existing 1.3.5 image. app:1.3.6 : resolves to the new image app:1.3 : resolves to the new image. app:1 : resolves to the new image Let's simulate a defect fix by building a new image using Containerfile1 instead of Containerfile : docker build -t app -f Containerfile1 . Example output: Sending build context to Docker daemon 32.26kB Step 1/5 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> bb79b9e26fd3 Step 2/5 : COPY server.xml /config ---> Using cache ---> f10659bc62b2 Step 3/5 : COPY ServletApp.war /config/dropins/app.war ---> Using cache ---> 24d85579e404 Step 4/5 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Using cache ---> 5e924d776a9c Step 5/5 : RUN echo test1 > /config/test1 ---> Running in 08e6135b00f5 Removing intermediate container 08e6135b00f5 ---> 69d20332a5e0 Successfully built 69d20332a5e0 Successfully tagged app:latest Tag it as follows : docker tag app app:1 docker tag app app:1.3 docker tag app app:1.3.6 Verify that these are the same images: app:1 , app:1.3 , app:1.3.6 . A new minor version involves compatible changes beyond just bug fixes. After you build a new minor version image, you want to manage the tags such that: app:1.3.5 : resolves to the existing 1.3.5 image. app:1.3.6 : resolves to the existing 1.3.6 image app:1.4.0 : resolves to the new image app:1.3 : resolves to the existing 1.3.6 image. app:1.4 : resolves to the new image. app:1 : resolves to the new image Build a new image using Containerfile2 : docker build -t app -f Containerfile2 . Sending build context to Docker daemon 32.26kB Step 1/6 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> bb79b9e26fd3 Step 2/6 : COPY server.xml /config ---> Using cache ---> f10659bc62b2 Step 3/6 : COPY ServletApp.war /config/dropins/app.war ---> Using cache ---> 24d85579e404 Step 4/6 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Using cache ---> 5e924d776a9c Step 5/6 : RUN echo test1 > /config/test1 ---> Using cache ---> 69d20332a5e0 Step 6/6 : RUN echo test2 > /config/test2 ---> Running in 96ea24b9ba66 Removing intermediate container 96ea24b9ba66 ---> 31b27169b3bc Successfully built 31b27169b3bc Tag it as follows: docker tag app app:1 docker tag app app:1.4 docker tag app app:1.4.0 Verify that 1 , 1.4 , and 1.4.0 are the same image 1.3 and 1.3.6 are the same image","title":"Managing Image Versions"},{"location":"basic-labs/HelloContainer/README - Copy/#extra-credit","text":"Search the internet for information about multi-stage build. In a single stage build, the final image contains both build and runtime artifacts. A multi-stage build allows you to build with one base image, and copy the result of the build to another base image. This gives you even more control over the output of the build, and the size of the final image. Start another instances of the app image for vertical scaling, but with different port numbers on the host. Point your browser to hub.docker.com , click \"Explore\" and explore the millions of available images. Think about how you would tag a new image at a major version, 2.0.0 . Think about what would be required to manage containers across multiple machines to support horizontal scaling. Congratulations! You have completed the Introduction to Containerization lab.","title":"Extra Credit"},{"location":"basic-labs/HelloContainer/README - Copy/#next","text":"Please follow the link to do the next lab Introduction to Container Orchestration using Openshift : - Introduction to Container Orchestration using Openshift","title":"Next"},{"location":"basic-labs/IntroOpenshift/","text":"Introduction to Container Orchestration using Openshift In this lab, we will introduce you to the basics of container Orchestration using Openshift. We will Perform basic navigation using the web console Deploy the hello-openshift image through the web console. Deploy the hello-openshift image through the command line. Prerequisite For background on basic Openshift concepts, read Openshift Concepts for WebSphere Administrators You have the access to OpenShift Web Console with IBM Cloud account ID login. You have cloned the lab into your working directory through the web terminal session. Login to the VM If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password: Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window. Deploy the hello-openshift image through the web console Login to the web console Open the Firefox Web Browser from the VM. Select the openshift console bookmark at the top left of the browser window to access the OpenShift Container Platform web console. This will take you to a login screen. Click on the htpasswd login option. Log in to the account using the following credentials: Username: ibmadmin Password: engageibm Overview Click on the Overview tab under Home in the left menu to view a summary of events: Scroll down to view the utilization of cluster resources: Scroll further down to view the cluster inventory. Click through each item in the inventory to find out more: Note that: Nodes represent physical or virtual hardware that your Openshift cluster is running. Pods are used to host and run one or more containers. Each node may run multiple pods. Containers in the same pod share the same network and storage. Storage classes represent the different types of storage configured and made available for your Openshift cluster. Persistent Volume Claims (PVCs) represent the usage of storage by the pods. After a pod is removed, data not persistent to persistent storage are gone. Projects Openshift projects allow you to group related resources together and to assign them separate management policies. It is common for artifacts related to different applications to be assigned to different projects . Resources that belong to the same project are stored in the same Kubernetes namespace . Click on the Projects tab under Home in the left menu, followed by Create Project : In the dialog, enter myproject as project name, then click Create : After creation, click on each of the tabs of myproject you just created. Note that: The YAML tab shows you the YAML representation of your project. Every resource in Openshift is represented as a REST data structure. We'll be working with YAML files a lot more when we interact with Openshift via the command line. The Role Bindings tab shows you the security configurations that apply to your project. For now, just take notice that there are many different roles already defined when a project is created. Each of these roles is used for a different purpose, and already mapped to different users and groups , or service accounts . First Application The typical artifacts you will need to run an application in Openshift are: A container image containing your application, hosted in a container registry One or more pods that specifies where to fetch an image and how it should be hosted. A deployment to control the number of instances pods. You don't normally configure a pod directly. Instead, you configure a deployment to manage a set of pods . A service that exposes the application within the internal network, and enables the application to be load balanced within the Openshift cluster. A route or ingress to make the application accessible outside of the Openshift cluster firewall. First deployment Under the Workloads tab, click Deployments . Then click Create Deployment : Note that the console shows you the YAML file for the deployment. Change the number of replicas from default 3 to 2 , then click Create : Here is the specification of the deployment in its entirety: apiVersion: apps/v1 kind: Deployment metadata: name: example namespace: myproject spec: selector: matchLabels: app: hello-openshift replicas: 2 template: metadata: labels: app: hello-openshift spec: containers: - name: hello-openshift image: openshift/hello-openshift ports: - containerPort: 8080 Let's review this resource: Every resource in Openshift has a group, version, and kind. For the Deployment resource: The group is apps The version is v1 The kind is Deployment The metadata specifies data that is needed for the runtime: The name of this instance is example The namespace where the resource is running is myproject Though not shown here, any labels associated with the resource. We will see the use of labels later. The spec section defines the details specific to this kind of resource: The selector defines details of the pods that this deployment will manage. The matchLabels attribute with value app: hello-openshift means this deployment instance will search for and manage all pods whose labels contain app: hello-openshift . The replicas: 2 field specifies the number of instances to run. The template section describes information about how to run the container image and create the pods : The labels section specifies what labels to add to the pods being to be created. Note that it matches the labels defined in the selector . The containers section specifies where to fetch the container image and which ports to expose. For our example, the image to run is openshift/hello-openshift . Wait for both pods to be running: Click on the YAML tab, and note the additions to the original input YAML file. Here is a sample YAML after the deployment is created : kind: Deployment apiVersion: apps/v1 metadata: name: example namespace: myproject selfLink: /apis/apps/v1/namespaces/myproject/deployments/example uid: 7c6a339b-385c-41bf-b4bf-3b6a120ef137 resourceVersion: '297294' generation: 1 creationTimestamp: '2020-01-30T15:45:15Z' annotations: deployment.kubernetes.io/revision: '1' spec: replicas: 2 selector: matchLabels: app: hello-openshift template: metadata: creationTimestamp: null labels: app: hello-openshift spec: containers: - name: hello-openshift image: openshift/hello-openshift ports: - containerPort: 8080 protocol: TCP resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File imagePullPolicy: Always restartPolicy: Always terminationGracePeriodSeconds: 30 dnsPolicy: ClusterFirst securityContext: {} schedulerName: default-scheduler strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 25% maxSurge: 25% revisionHistoryLimit: 10 progressDeadlineSeconds: 600 status: observedGeneration: 1 replicas: 2 updatedReplicas: 2 readyReplicas: 2 availableReplicas: 2 conditions: - type: Available status: 'True' lastUpdateTime: '2020-01-30T15:45:20Z' lastTransitionTime: '2020-01-30T15:45:20Z' reason: MinimumReplicasAvailable message: Deployment has minimum availability. - type: Progressing status: 'True' lastUpdateTime: '2020-01-30T15:45:20Z' lastTransitionTime: '2020-01-30T15:45:15Z' reason: NewReplicaSetAvailable message: ReplicaSet \"example-75778c488\" has successfully progressed. In the YAML, note that: There are quite a bit more metadata . Metadata may be added by any number of controllers as needed to help with their function. The spec has more attributes filled in as well. These are default values that were not specified in our original YAML file. But sometimes it is also possible that some values are overridden by background admission controllers. The status sub-resource is how Openshift communicates that status of the resource. The status is updated regularly as the underlying state of the resource changes. Click on Pods . Note that the pods resources are managed by the controller for your deployment . You do not create the pod resources yourself. That is the reason that Pods tab is under the deployment resource you just created. Click on one of the pods: Explore the various tabs for your pod Overview: displays the overall resource usage for your pod. Note that for CPU usage, the unit is m, or milli-core, which is 1/1000th of one core. YAML: examine the YAML that describes your pod. This YAML is created by the deployment controller based on the specification you supplied in your deployment. Note that labels associated with your pod are what you had specified in the deployment. Environment: lists the environment variables defined for your pod. For our hello-openshift pod, there is none. Logs: shows the console log for your container. Note that it is the same log as the log from the Introduction to Docker lab, as the same image is being used. Terminal: Opens a remote shell into your container. As with the Introduction to Docker lab, no shell is available within the container for this image. This makes it more secure, but also more difficult to debug. First Service A service enables the pods we just created to be load balanced within the Openshift cluster. Scroll down to the Networking tab on the left navigation, click Services , then click Create Service : Update the YAML parameters as follows: (Before update) a. Under spec.selector: change MyApp to hello-openshift . This is how the service will find the pods to load balance. Therefore, it matches the labels ( spec.selector.matchLabels ) that we used when creating the deployment for the hello-openshift application. b. Under spec.ports: change 80 to 8080 and change 9376 to 8080 (the same ports we used previously). c. Click Create (After update) After the service is created, click on the YAML tab: The YAML file looks like: Kind: Service apiVersion: v1 metadata: name: example namespace: myproject selfLink: /api/v1/namespaces/myproject/services/example uid: 6ca3c2b7-bbfa-432e-8757-c60dbf04b26b resourceVersion: '307351' creationTimestamp: '2020-01-30T16:28:03Z' spec: ports: - protocol: TCP port: 8080 targetPort: 8080 selector: app: hello-openshift clusterIP: 172.21.239.191 type: ClusterIP sessionAffinity: None status: loadBalancer: {} Note that for this service, there is a cluster wide IP address created, and that it is being load balanced. Also session affinity is not set for this service. First Route A route exposes your internal endpoints outside your cluster's built-in firewall. Click on the Route tab under Networking in the left navigation, then click Create Route : Supply input to the following parameters: Name: example Service: example Target Port: 8080 --> 8080 (TCP) Click Create Note that we are ignoring TLS configuration just for the purpose of this lab. Security will be addressed in a different lab. Access the route at the link provided under Location: If you have configured everything correctly, the browser will show Hello Openshift! . Congratulations , you just deployed your first application to Openshift. Changing Replica Instances Click on the Projects tab under Home from the left navigation, then click on myproject : Scroll down to see the resources that were created. Recall that we have created one deployment with 2 pods in the specification. We also created one service, and one route. Under the Inventory section, click on the 2 pods link: Delete one of the pods by clicking on the menu on the right, then selecting Delete pod . When prompted, click Delete . This is not the right way to reduce number of instances. You will notice that as soon as one of the pods is being terminated, another one is being created. The reason is that the controller for the deployment resource knows that your specification is for 2 instances , and it honors that specification by creating another one. This also gives you automatic failure recovery should one of the pods crashes on its own. To change the number of instances, you will need to change the specification of your deployment. Click on the Deployments tab under Workloads in the left navigation, then click on example deployment: Click on the down arrow to reduce the replica size down to 1: After the operation is completed, click on the YAML tab: Note that the console had changed the REST specification on your behalf so that the replica count is now 1: Deploy the hello-openshift image through the command line You can use both oc , the openshift command line tool, or kubectl , the Kubernetes command line tool, to interact with Openshift. Resources in Openshift are configured via REST data structure. For the command line tools, the REST data structure may be stored either in a YAML file, or in a JSON file. The command line tools may be used to: List available resources Create resources Update existing resources Delete resources Command Line Terminal The oc command is already installed on your VM's terminal. Open a terminal window from the VM and clone the lab to your local directory via: git clone https://github.com/IBM/openshift-workshop-was.git Change directory to: openshift-workshop-was/labs/Openshift/IntroOpenshift cd openshift-workshop-was/labs/Openshift/IntroOpenshift Login Return to the Openshift console, click on the arrow next to your login name and select Copy Login Command . In the new window that pops up, click on Display Token : Copy the oc login command and paste it into your web terminal. oc login --token=<TOKEN> --server=<SERVER Address> After login, the project last accessed is displayed, and it may or may not be the default project shown below: Logged into \"<SERVER address\" as \"<USER>\" using the token provided. You have access to 56 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\". Listing resources Use oc api-resources to list all available resource kinds. Note that resources in Openshift have a group , version , and kind . Some resources are global (not in a namespace), while others are scoped to a namespace . Many resources also have short names to save typing when using the command line tool. For example, you may use cm instead of ConfigMap as a command line parameter when the parameter is for a KIND . Example output: NAME SHORTNAMES APIGROUP NAMESPACED KIND bindings true Binding componentstatuses cs false ComponentStatu s configmaps cm true ConfigMap endpoints ep true Endpoints events ev true Event limitranges limits true LimitRange namespaces ns false Namespace nodes no false Node Listing instances of a resource kind List all projects: oc get projects NAME DISPLAY NAME STATUS default Active ibm-cert-store Active ibm-system Active kube-node-lease Active kube-public Active kube-system Active myproject Active ... List all pods in all namespaces: oc get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE ibm-system ibm-cloud-provider-ip-52-116-182-130-67c44c6d5d-bh9x5 1/1 Running 0 24h ibm-system ibm-cloud-provider-ip-52-116-182-130-67c44c6d5d-cs5ln 1/1 Running 0 24h kube-system calico-kube-controllers-549fdb8d79-khkvr 1/1 Running 0 25h ... List all pods within a namespace: oc get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-549fdb8d79-khkvr 1/1 Running 0 25h calico-node-jc6ln 1/1 Running 0 24h calico-node-t7zwg 1/1 Running 0 24h ... Projects List all projects: oc get projects NAME DISPLAY NAME STATUS default Active ibm-cert-store Active ibm-system Active kube-node-lease Active Get current project: oc project (Note: current project may not be myproject shown below): Using project \"myproject\" on server \"https://c100-e.us-south.containers.cloud.ibm.com:32541\". Create a new project and make it the current project: oc new-project project1 The output from creating a new project: Now using project \"project1\" on server \"https://c100-e.us-south.containers.cloud.ibm.com:32541\". Switch to the default project: oc project default Switch back to project1 : oc project project1 View the REST specification of the project: oc get project project1 -o yaml The output of the resource specification in yaml apiVersion: project.openshift.io/v1 kind: Project metadata: annotations: openshift.io/description: \"\" openshift.io/display-name: \"\" openshift.io/requester: IAM#mcheng@us.ibm.com openshift.io/sa.scc.mcs: s0:c24,c9 openshift.io/sa.scc.supplemental-groups: 1000570000/10000 openshift.io/sa.scc.uid-range: 1000570000/10000 creationTimestamp: \"2020-01-30T20:28:13Z\" name: project1 resourceVersion: \"364002\" selfLink: /apis/project.openshift.io/v1/projects/project1 uid: a817b908-a7fe-4f82-9bfa-e18fa4c12584 spec: finalizers: - kubernetes status: phase: Active First Application First Deployment In your web terminal session, under the directory where you clone the labs repository (.../openshift-workshop-was/labs/Openshift/IntroOpenshift) , you will find Deployment.yaml ; For example: root@lab-tools-6d4cbb56b6-cn2k5:/openshift-workshop-was/labs/Openshift/IntroOpenshift# ls -lt drwxr-xr-x. 2 root root 4096 Apr 9 01:13 images -rw-r--r--. 1 root root 32461 Apr 9 01:13 README.md -rw-r--r--. 1 root root 171 Apr 9 01:13 Route.yaml -rw-r--r--. 1 root root 207 Apr 9 01:13 Service.yaml -rw-r--r--. 1 root root 384 Apr 9 01:13 Deployment.yaml Review the contents of Deployment.yaml and note that it is identical to the to the deployment from the last section except for the namespace. cat Deployment.yaml This allows us to deploy the same image in a different project. Using the same image customized for different environments is an important concept that will be covered further in future labs. apiVersion: apps/v1 kind: Deployment metadata: name: example namespace: project1 spec: selector: matchLabels: app: hello-openshift replicas: 2 template: metadata: labels: app: hello-openshift spec: containers: - name: hello-openshift image: openshift/hello-openshift ports: - containerPort: 8080 Apply the deployment via the command line: oc apply -f Deployment.yaml After applying the yaml, you will see a message that the deployment resource was created deployment.apps/example created Check the status of deployment: oc get deployment example -o yaml If the status does not show available replica count of 2 , wait a few seconds, then rerun the command. apiVersion: extensions/v1beta1 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: \"1\" kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"metadata\":{\"annotations\":{},\"name\":\"example\",\"namespace\":\"project1\"}, \"spec\":{\"replicas\":2,\"selector\":{\"matchLabels\":{\"app\":\"hello-openshift\"}},\"template\":{\"metadata\":{\"labels\":{\"app\":\"hello -openshift\"}},\"spec\":{\"containers\":[{\"image\":\"openshift/hello-openshift\",\"name\":\"hello-openshift\",\"ports\":[{\"containerPo rt\":8080}]}]}}}} creationTimestamp: \"2020-01-30T20:37:28Z\" generation: 1 name: example namespace: project1 resourceVersion: \"366251\" selfLink: /apis/extensions/v1beta1/namespaces/project1/deployments/example uid: f011a93d-2231-4187-b265-f350ee830971 spec: progressDeadlineSeconds: 600 replicas: 2 revisionHistoryLimit: 10 selector: matchLabels: app: hello-openshift strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: hello-openshift spec: containers: - image: openshift/hello-openshift imagePullPolicy: Always name: hello-openshift ports: - containerPort: 8080 protocol: TCP resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 status: availableReplicas: 2 conditions: - lastTransitionTime: \"2020-01-30T20:37:31Z\" lastUpdateTime: \"2020-01-30T20:37:31Z\" message: Deployment has minimum availability. reason: MinimumReplicasAvailable status: \"True\" type: Available - lastTransitionTime: \"2020-01-30T20:37:28Z\" lastUpdateTime: \"2020-01-30T20:37:31Z\" message: ReplicaSet \"example-75778c488\" has successfully progressed. reason: NewReplicaSetAvailable status: \"True\" type: Progressing observedGeneration: 1 readyReplicas: 2 replicas: 2 updatedReplicas: 2 List the running pods created by the controller for the deployment: oc get pods The pods should be in the Running state NAME READY STATUS RESTARTS AGE example-75778c488-7k7q2 1/1 Running 0 3m37s example-75778c488-c9jhd 1/1 Running 0 3m37s List the details for one of the pods: oc get pods <pod name> -o yaml Note: <pod name> is listed under NAME in the previous command's output. apiVersion: v1 kind: Pod metadata: annotations: cni.projectcalico.org/podIP: 172.30.26.229/32 openshift.io/scc: restricted creationTimestamp: \"2020-01-30T20:37:28Z\" generateName: example-75778c488- labels: app: hello-openshift pod-template-hash: 75778c488 name: example-75778c488-7k7q2 namespace: project1 ownerReferences: - apiVersion: apps/v1 blockOwnerDeletion: true controller: true kind: ReplicaSet name: example-75778c488 uid: 2b03ef6a-8a1a-4f7e-9502-7249a4dabb98 resourceVersion: \"366248\" selfLink: /api/v1/namespaces/project1/pods/example-75778c488-7k7q2 uid: 504c999e-d47d-48f7-afcb-992a5a13fc67 spec: containers: - image: openshift/hello-openshift imagePullPolicy: Always name: hello-openshift ports: - containerPort: 8080 protocol: TCP resources: {} securityContext: capabilities: drop: - KILL - MKNOD - SETGID - SETUID runAsUser: 1000570000 terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: default-token-ngv4p readOnly: true dnsPolicy: ClusterFirst enableServiceLinks: true imagePullSecrets: - name: default-dockercfg-shjmz nodeName: 10.177.93.14 priority: 0 restartPolicy: Always schedulerName: default-scheduler securityContext: fsGroup: 1000570000 seLinuxOptions: level: s0:c24,c9 serviceAccount: default serviceAccountName: default terminationGracePeriodSeconds: 30 tolerations: - effect: NoExecute key: node.kubernetes.io/not-ready operator: Exists tolerationSeconds: 300 - effect: NoExecute key: node.kubernetes.io/unreachable operator: Exists tolerationSeconds: 300 volumes: - name: default-token-ngv4p secret: defaultMode: 420 secretName: default-token-ngv4p status: conditions: - lastProbeTime: null lastTransitionTime: \"2020-01-30T20:37:28Z\" status: \"True\" type: Initialized - lastProbeTime: null lastTransitionTime: \"2020-01-30T20:37:31Z\" status: \"True\" type: Ready - lastProbeTime: null lastTransitionTime: \"2020-01-30T20:37:31Z\" status: \"True\" type: ContainersReady - lastProbeTime: null lastTransitionTime: \"2020-01-30T20:37:28Z\" status: \"True\" type: PodScheduled containerStatuses: - containerID: cri-o://42828d4a8333d4fa9d3882805680d7693616610ff78a3e07f4794d91b86862b5 image: docker.io/openshift/hello-openshift:latest imageID: docker.io/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e lastState: {} name: hello-openshift ready: true restartCount: 0 started: true state: running: startedAt: \"2020-01-30T20:37:30Z\" hostIP: 10.177.93.14 phase: Running podIP: 172.30.26.229 podIPs: - ip: 172.30.26.229 qosClass: BestEffort startTime: \"2020-01-30T20:37:28Z\" Show the logs of one of the pods: oc logs <pod name> serving on 8888 serving on 8080 Take a look at Service.yaml and note that it's for the project1 namespace: cat Service.yaml Example output: apiVersion: v1 kind: Service metadata: name: example namespace: project1 spec: ports: - protocol: TCP port: 8080 targetPort: 8080 selector: app: hello-openshift type: ClusterIP Create the service so that it's accessible and load balanced for pods with label app: hello-openshift within the project1 namespace: oc apply -f Service.yaml The service is created service/example created Examine Route.yaml: cat Route.yaml Output: apiVersion: route.openshift.io/v1 kind: Route metadata: name: example namespace: project1 spec: port: targetPort: 8080 to: kind: Service name: example Apply the route to make the service reachable from outside the cluster: oc apply -f Route.yaml The route is created route.route.openshift.io/example created Generate the URL for the route, and point your browser to it: echo http://$(oc get route example --template='{{ .spec.host }}') Output: http://example-project1.apps.demo.ibmdte.net Open your Firefox browser again and visit the URL outputted by the previous command. You should see a web page displaying the following message: Changing Replica Instance List pods: oc get pods NAME READY STATUS RESTARTS AGE example-75778c488-7k7q2 1/1 Running 0 60m example-75778c488-c9jhd 1/1 Running 0 60m Delete one of the pods: oc delete pod <pod name> pod \"example-75778c488-7k7q2\" deleted List pods again and note that a new instance has been created as expected. The deployment specified 2 instances, so the controller tries to maintain 2 instances: oc get pods NAME READY STATUS RESTARTS AGE example-75778c488-c9jhd 1/1 Running 0 63m example-75778c488-rhjrx 1/1 Running 0 28s To reduce the number of pods, we can patch the resource in one of two ways: Scripted patch using the patch option of the command line: oc patch deployment example -p '{ \"spec\": { \"replicas\": 1 } }' Interactive patch using the edit option of the command line through vi editor: oc edit deployment example Under the spec section (not under the status section), change replicas: 2 to replicas: 1 , and save the change in the vi editor (by :wq ). The output: deployment.extensions/example edited Note: The above edits the copy that is stored in Openshift. You may also edit your local copy of Deployment.yaml and re-apply it. List the pods to show only 1 pod is running: oc get pods NAME READY STATUS RESTARTS AGE example-75778c488-c9jhd 1/1 Running 0 65m Cleanup: oc delete route example oc delete service example oc delete deployment example oc get pods Note: You may have to do this a few times, to wait for the pods to be deleted. Congratulations, you have deployed your first application to Openshift via the command line. Next Please follow the link to the next lab Operational Modernization : - Operational Modernization","title":"Introduction to Container Orchestration using Openshift"},{"location":"basic-labs/IntroOpenshift/#introduction-to-container-orchestration-using-openshift","text":"In this lab, we will introduce you to the basics of container Orchestration using Openshift. We will Perform basic navigation using the web console Deploy the hello-openshift image through the web console. Deploy the hello-openshift image through the command line.","title":"Introduction to Container Orchestration using Openshift"},{"location":"basic-labs/IntroOpenshift/#prerequisite","text":"For background on basic Openshift concepts, read Openshift Concepts for WebSphere Administrators You have the access to OpenShift Web Console with IBM Cloud account ID login. You have cloned the lab into your working directory through the web terminal session.","title":"Prerequisite"},{"location":"basic-labs/IntroOpenshift/#login-to-the-vm","text":"If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password: Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window.","title":"Login to the VM"},{"location":"basic-labs/IntroOpenshift/#deploy-the-hello-openshift-image-through-the-web-console","text":"","title":"Deploy the hello-openshift image through the web console"},{"location":"basic-labs/IntroOpenshift/#login-to-the-web-console","text":"Open the Firefox Web Browser from the VM. Select the openshift console bookmark at the top left of the browser window to access the OpenShift Container Platform web console. This will take you to a login screen. Click on the htpasswd login option. Log in to the account using the following credentials: Username: ibmadmin Password: engageibm","title":"Login to the web console"},{"location":"basic-labs/IntroOpenshift/#overview","text":"Click on the Overview tab under Home in the left menu to view a summary of events: Scroll down to view the utilization of cluster resources: Scroll further down to view the cluster inventory. Click through each item in the inventory to find out more: Note that: Nodes represent physical or virtual hardware that your Openshift cluster is running. Pods are used to host and run one or more containers. Each node may run multiple pods. Containers in the same pod share the same network and storage. Storage classes represent the different types of storage configured and made available for your Openshift cluster. Persistent Volume Claims (PVCs) represent the usage of storage by the pods. After a pod is removed, data not persistent to persistent storage are gone.","title":"Overview"},{"location":"basic-labs/IntroOpenshift/#projects","text":"Openshift projects allow you to group related resources together and to assign them separate management policies. It is common for artifacts related to different applications to be assigned to different projects . Resources that belong to the same project are stored in the same Kubernetes namespace . Click on the Projects tab under Home in the left menu, followed by Create Project : In the dialog, enter myproject as project name, then click Create : After creation, click on each of the tabs of myproject you just created. Note that: The YAML tab shows you the YAML representation of your project. Every resource in Openshift is represented as a REST data structure. We'll be working with YAML files a lot more when we interact with Openshift via the command line. The Role Bindings tab shows you the security configurations that apply to your project. For now, just take notice that there are many different roles already defined when a project is created. Each of these roles is used for a different purpose, and already mapped to different users and groups , or service accounts .","title":"Projects"},{"location":"basic-labs/IntroOpenshift/#first-application","text":"The typical artifacts you will need to run an application in Openshift are: A container image containing your application, hosted in a container registry One or more pods that specifies where to fetch an image and how it should be hosted. A deployment to control the number of instances pods. You don't normally configure a pod directly. Instead, you configure a deployment to manage a set of pods . A service that exposes the application within the internal network, and enables the application to be load balanced within the Openshift cluster. A route or ingress to make the application accessible outside of the Openshift cluster firewall.","title":"First Application"},{"location":"basic-labs/IntroOpenshift/#first-deployment","text":"Under the Workloads tab, click Deployments . Then click Create Deployment : Note that the console shows you the YAML file for the deployment. Change the number of replicas from default 3 to 2 , then click Create : Here is the specification of the deployment in its entirety: apiVersion: apps/v1 kind: Deployment metadata: name: example namespace: myproject spec: selector: matchLabels: app: hello-openshift replicas: 2 template: metadata: labels: app: hello-openshift spec: containers: - name: hello-openshift image: openshift/hello-openshift ports: - containerPort: 8080 Let's review this resource: Every resource in Openshift has a group, version, and kind. For the Deployment resource: The group is apps The version is v1 The kind is Deployment The metadata specifies data that is needed for the runtime: The name of this instance is example The namespace where the resource is running is myproject Though not shown here, any labels associated with the resource. We will see the use of labels later. The spec section defines the details specific to this kind of resource: The selector defines details of the pods that this deployment will manage. The matchLabels attribute with value app: hello-openshift means this deployment instance will search for and manage all pods whose labels contain app: hello-openshift . The replicas: 2 field specifies the number of instances to run. The template section describes information about how to run the container image and create the pods : The labels section specifies what labels to add to the pods being to be created. Note that it matches the labels defined in the selector . The containers section specifies where to fetch the container image and which ports to expose. For our example, the image to run is openshift/hello-openshift . Wait for both pods to be running: Click on the YAML tab, and note the additions to the original input YAML file. Here is a sample YAML after the deployment is created : kind: Deployment apiVersion: apps/v1 metadata: name: example namespace: myproject selfLink: /apis/apps/v1/namespaces/myproject/deployments/example uid: 7c6a339b-385c-41bf-b4bf-3b6a120ef137 resourceVersion: '297294' generation: 1 creationTimestamp: '2020-01-30T15:45:15Z' annotations: deployment.kubernetes.io/revision: '1' spec: replicas: 2 selector: matchLabels: app: hello-openshift template: metadata: creationTimestamp: null labels: app: hello-openshift spec: containers: - name: hello-openshift image: openshift/hello-openshift ports: - containerPort: 8080 protocol: TCP resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File imagePullPolicy: Always restartPolicy: Always terminationGracePeriodSeconds: 30 dnsPolicy: ClusterFirst securityContext: {} schedulerName: default-scheduler strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 25% maxSurge: 25% revisionHistoryLimit: 10 progressDeadlineSeconds: 600 status: observedGeneration: 1 replicas: 2 updatedReplicas: 2 readyReplicas: 2 availableReplicas: 2 conditions: - type: Available status: 'True' lastUpdateTime: '2020-01-30T15:45:20Z' lastTransitionTime: '2020-01-30T15:45:20Z' reason: MinimumReplicasAvailable message: Deployment has minimum availability. - type: Progressing status: 'True' lastUpdateTime: '2020-01-30T15:45:20Z' lastTransitionTime: '2020-01-30T15:45:15Z' reason: NewReplicaSetAvailable message: ReplicaSet \"example-75778c488\" has successfully progressed. In the YAML, note that: There are quite a bit more metadata . Metadata may be added by any number of controllers as needed to help with their function. The spec has more attributes filled in as well. These are default values that were not specified in our original YAML file. But sometimes it is also possible that some values are overridden by background admission controllers. The status sub-resource is how Openshift communicates that status of the resource. The status is updated regularly as the underlying state of the resource changes. Click on Pods . Note that the pods resources are managed by the controller for your deployment . You do not create the pod resources yourself. That is the reason that Pods tab is under the deployment resource you just created. Click on one of the pods: Explore the various tabs for your pod Overview: displays the overall resource usage for your pod. Note that for CPU usage, the unit is m, or milli-core, which is 1/1000th of one core. YAML: examine the YAML that describes your pod. This YAML is created by the deployment controller based on the specification you supplied in your deployment. Note that labels associated with your pod are what you had specified in the deployment. Environment: lists the environment variables defined for your pod. For our hello-openshift pod, there is none. Logs: shows the console log for your container. Note that it is the same log as the log from the Introduction to Docker lab, as the same image is being used. Terminal: Opens a remote shell into your container. As with the Introduction to Docker lab, no shell is available within the container for this image. This makes it more secure, but also more difficult to debug.","title":"First deployment"},{"location":"basic-labs/IntroOpenshift/#first-service","text":"A service enables the pods we just created to be load balanced within the Openshift cluster. Scroll down to the Networking tab on the left navigation, click Services , then click Create Service : Update the YAML parameters as follows: (Before update) a. Under spec.selector: change MyApp to hello-openshift . This is how the service will find the pods to load balance. Therefore, it matches the labels ( spec.selector.matchLabels ) that we used when creating the deployment for the hello-openshift application. b. Under spec.ports: change 80 to 8080 and change 9376 to 8080 (the same ports we used previously). c. Click Create (After update) After the service is created, click on the YAML tab: The YAML file looks like: Kind: Service apiVersion: v1 metadata: name: example namespace: myproject selfLink: /api/v1/namespaces/myproject/services/example uid: 6ca3c2b7-bbfa-432e-8757-c60dbf04b26b resourceVersion: '307351' creationTimestamp: '2020-01-30T16:28:03Z' spec: ports: - protocol: TCP port: 8080 targetPort: 8080 selector: app: hello-openshift clusterIP: 172.21.239.191 type: ClusterIP sessionAffinity: None status: loadBalancer: {} Note that for this service, there is a cluster wide IP address created, and that it is being load balanced. Also session affinity is not set for this service.","title":"First Service"},{"location":"basic-labs/IntroOpenshift/#first-route","text":"A route exposes your internal endpoints outside your cluster's built-in firewall. Click on the Route tab under Networking in the left navigation, then click Create Route : Supply input to the following parameters: Name: example Service: example Target Port: 8080 --> 8080 (TCP) Click Create Note that we are ignoring TLS configuration just for the purpose of this lab. Security will be addressed in a different lab. Access the route at the link provided under Location: If you have configured everything correctly, the browser will show Hello Openshift! . Congratulations , you just deployed your first application to Openshift.","title":"First Route"},{"location":"basic-labs/IntroOpenshift/#changing-replica-instances","text":"Click on the Projects tab under Home from the left navigation, then click on myproject : Scroll down to see the resources that were created. Recall that we have created one deployment with 2 pods in the specification. We also created one service, and one route. Under the Inventory section, click on the 2 pods link: Delete one of the pods by clicking on the menu on the right, then selecting Delete pod . When prompted, click Delete . This is not the right way to reduce number of instances. You will notice that as soon as one of the pods is being terminated, another one is being created. The reason is that the controller for the deployment resource knows that your specification is for 2 instances , and it honors that specification by creating another one. This also gives you automatic failure recovery should one of the pods crashes on its own. To change the number of instances, you will need to change the specification of your deployment. Click on the Deployments tab under Workloads in the left navigation, then click on example deployment: Click on the down arrow to reduce the replica size down to 1: After the operation is completed, click on the YAML tab: Note that the console had changed the REST specification on your behalf so that the replica count is now 1:","title":"Changing Replica Instances"},{"location":"basic-labs/IntroOpenshift/#deploy-the-hello-openshift-image-through-the-command-line","text":"You can use both oc , the openshift command line tool, or kubectl , the Kubernetes command line tool, to interact with Openshift. Resources in Openshift are configured via REST data structure. For the command line tools, the REST data structure may be stored either in a YAML file, or in a JSON file. The command line tools may be used to: List available resources Create resources Update existing resources Delete resources","title":"Deploy the hello-openshift image through the command line"},{"location":"basic-labs/IntroOpenshift/#command-line-terminal","text":"The oc command is already installed on your VM's terminal. Open a terminal window from the VM and clone the lab to your local directory via: git clone https://github.com/IBM/openshift-workshop-was.git Change directory to: openshift-workshop-was/labs/Openshift/IntroOpenshift cd openshift-workshop-was/labs/Openshift/IntroOpenshift","title":"Command Line Terminal"},{"location":"basic-labs/IntroOpenshift/#login","text":"Return to the Openshift console, click on the arrow next to your login name and select Copy Login Command . In the new window that pops up, click on Display Token : Copy the oc login command and paste it into your web terminal. oc login --token=<TOKEN> --server=<SERVER Address> After login, the project last accessed is displayed, and it may or may not be the default project shown below: Logged into \"<SERVER address\" as \"<USER>\" using the token provided. You have access to 56 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\".","title":"Login"},{"location":"basic-labs/IntroOpenshift/#listing-resources","text":"Use oc api-resources to list all available resource kinds. Note that resources in Openshift have a group , version , and kind . Some resources are global (not in a namespace), while others are scoped to a namespace . Many resources also have short names to save typing when using the command line tool. For example, you may use cm instead of ConfigMap as a command line parameter when the parameter is for a KIND . Example output: NAME SHORTNAMES APIGROUP NAMESPACED KIND bindings true Binding componentstatuses cs false ComponentStatu s configmaps cm true ConfigMap endpoints ep true Endpoints events ev true Event limitranges limits true LimitRange namespaces ns false Namespace nodes no false Node","title":"Listing resources"},{"location":"basic-labs/IntroOpenshift/#listing-instances-of-a-resource-kind","text":"List all projects: oc get projects NAME DISPLAY NAME STATUS default Active ibm-cert-store Active ibm-system Active kube-node-lease Active kube-public Active kube-system Active myproject Active ... List all pods in all namespaces: oc get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE ibm-system ibm-cloud-provider-ip-52-116-182-130-67c44c6d5d-bh9x5 1/1 Running 0 24h ibm-system ibm-cloud-provider-ip-52-116-182-130-67c44c6d5d-cs5ln 1/1 Running 0 24h kube-system calico-kube-controllers-549fdb8d79-khkvr 1/1 Running 0 25h ... List all pods within a namespace: oc get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-549fdb8d79-khkvr 1/1 Running 0 25h calico-node-jc6ln 1/1 Running 0 24h calico-node-t7zwg 1/1 Running 0 24h ...","title":"Listing instances of a resource kind"},{"location":"basic-labs/IntroOpenshift/#projects_1","text":"List all projects: oc get projects NAME DISPLAY NAME STATUS default Active ibm-cert-store Active ibm-system Active kube-node-lease Active Get current project: oc project (Note: current project may not be myproject shown below): Using project \"myproject\" on server \"https://c100-e.us-south.containers.cloud.ibm.com:32541\". Create a new project and make it the current project: oc new-project project1 The output from creating a new project: Now using project \"project1\" on server \"https://c100-e.us-south.containers.cloud.ibm.com:32541\". Switch to the default project: oc project default Switch back to project1 : oc project project1 View the REST specification of the project: oc get project project1 -o yaml The output of the resource specification in yaml apiVersion: project.openshift.io/v1 kind: Project metadata: annotations: openshift.io/description: \"\" openshift.io/display-name: \"\" openshift.io/requester: IAM#mcheng@us.ibm.com openshift.io/sa.scc.mcs: s0:c24,c9 openshift.io/sa.scc.supplemental-groups: 1000570000/10000 openshift.io/sa.scc.uid-range: 1000570000/10000 creationTimestamp: \"2020-01-30T20:28:13Z\" name: project1 resourceVersion: \"364002\" selfLink: /apis/project.openshift.io/v1/projects/project1 uid: a817b908-a7fe-4f82-9bfa-e18fa4c12584 spec: finalizers: - kubernetes status: phase: Active","title":"Projects"},{"location":"basic-labs/IntroOpenshift/#first-application_1","text":"","title":"First Application"},{"location":"basic-labs/IntroOpenshift/#first-deployment_1","text":"In your web terminal session, under the directory where you clone the labs repository (.../openshift-workshop-was/labs/Openshift/IntroOpenshift) , you will find Deployment.yaml ; For example: root@lab-tools-6d4cbb56b6-cn2k5:/openshift-workshop-was/labs/Openshift/IntroOpenshift# ls -lt drwxr-xr-x. 2 root root 4096 Apr 9 01:13 images -rw-r--r--. 1 root root 32461 Apr 9 01:13 README.md -rw-r--r--. 1 root root 171 Apr 9 01:13 Route.yaml -rw-r--r--. 1 root root 207 Apr 9 01:13 Service.yaml -rw-r--r--. 1 root root 384 Apr 9 01:13 Deployment.yaml Review the contents of Deployment.yaml and note that it is identical to the to the deployment from the last section except for the namespace. cat Deployment.yaml This allows us to deploy the same image in a different project. Using the same image customized for different environments is an important concept that will be covered further in future labs. apiVersion: apps/v1 kind: Deployment metadata: name: example namespace: project1 spec: selector: matchLabels: app: hello-openshift replicas: 2 template: metadata: labels: app: hello-openshift spec: containers: - name: hello-openshift image: openshift/hello-openshift ports: - containerPort: 8080 Apply the deployment via the command line: oc apply -f Deployment.yaml After applying the yaml, you will see a message that the deployment resource was created deployment.apps/example created Check the status of deployment: oc get deployment example -o yaml If the status does not show available replica count of 2 , wait a few seconds, then rerun the command. apiVersion: extensions/v1beta1 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: \"1\" kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"metadata\":{\"annotations\":{},\"name\":\"example\",\"namespace\":\"project1\"}, \"spec\":{\"replicas\":2,\"selector\":{\"matchLabels\":{\"app\":\"hello-openshift\"}},\"template\":{\"metadata\":{\"labels\":{\"app\":\"hello -openshift\"}},\"spec\":{\"containers\":[{\"image\":\"openshift/hello-openshift\",\"name\":\"hello-openshift\",\"ports\":[{\"containerPo rt\":8080}]}]}}}} creationTimestamp: \"2020-01-30T20:37:28Z\" generation: 1 name: example namespace: project1 resourceVersion: \"366251\" selfLink: /apis/extensions/v1beta1/namespaces/project1/deployments/example uid: f011a93d-2231-4187-b265-f350ee830971 spec: progressDeadlineSeconds: 600 replicas: 2 revisionHistoryLimit: 10 selector: matchLabels: app: hello-openshift strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: hello-openshift spec: containers: - image: openshift/hello-openshift imagePullPolicy: Always name: hello-openshift ports: - containerPort: 8080 protocol: TCP resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 status: availableReplicas: 2 conditions: - lastTransitionTime: \"2020-01-30T20:37:31Z\" lastUpdateTime: \"2020-01-30T20:37:31Z\" message: Deployment has minimum availability. reason: MinimumReplicasAvailable status: \"True\" type: Available - lastTransitionTime: \"2020-01-30T20:37:28Z\" lastUpdateTime: \"2020-01-30T20:37:31Z\" message: ReplicaSet \"example-75778c488\" has successfully progressed. reason: NewReplicaSetAvailable status: \"True\" type: Progressing observedGeneration: 1 readyReplicas: 2 replicas: 2 updatedReplicas: 2 List the running pods created by the controller for the deployment: oc get pods The pods should be in the Running state NAME READY STATUS RESTARTS AGE example-75778c488-7k7q2 1/1 Running 0 3m37s example-75778c488-c9jhd 1/1 Running 0 3m37s List the details for one of the pods: oc get pods <pod name> -o yaml Note: <pod name> is listed under NAME in the previous command's output. apiVersion: v1 kind: Pod metadata: annotations: cni.projectcalico.org/podIP: 172.30.26.229/32 openshift.io/scc: restricted creationTimestamp: \"2020-01-30T20:37:28Z\" generateName: example-75778c488- labels: app: hello-openshift pod-template-hash: 75778c488 name: example-75778c488-7k7q2 namespace: project1 ownerReferences: - apiVersion: apps/v1 blockOwnerDeletion: true controller: true kind: ReplicaSet name: example-75778c488 uid: 2b03ef6a-8a1a-4f7e-9502-7249a4dabb98 resourceVersion: \"366248\" selfLink: /api/v1/namespaces/project1/pods/example-75778c488-7k7q2 uid: 504c999e-d47d-48f7-afcb-992a5a13fc67 spec: containers: - image: openshift/hello-openshift imagePullPolicy: Always name: hello-openshift ports: - containerPort: 8080 protocol: TCP resources: {} securityContext: capabilities: drop: - KILL - MKNOD - SETGID - SETUID runAsUser: 1000570000 terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: default-token-ngv4p readOnly: true dnsPolicy: ClusterFirst enableServiceLinks: true imagePullSecrets: - name: default-dockercfg-shjmz nodeName: 10.177.93.14 priority: 0 restartPolicy: Always schedulerName: default-scheduler securityContext: fsGroup: 1000570000 seLinuxOptions: level: s0:c24,c9 serviceAccount: default serviceAccountName: default terminationGracePeriodSeconds: 30 tolerations: - effect: NoExecute key: node.kubernetes.io/not-ready operator: Exists tolerationSeconds: 300 - effect: NoExecute key: node.kubernetes.io/unreachable operator: Exists tolerationSeconds: 300 volumes: - name: default-token-ngv4p secret: defaultMode: 420 secretName: default-token-ngv4p status: conditions: - lastProbeTime: null lastTransitionTime: \"2020-01-30T20:37:28Z\" status: \"True\" type: Initialized - lastProbeTime: null lastTransitionTime: \"2020-01-30T20:37:31Z\" status: \"True\" type: Ready - lastProbeTime: null lastTransitionTime: \"2020-01-30T20:37:31Z\" status: \"True\" type: ContainersReady - lastProbeTime: null lastTransitionTime: \"2020-01-30T20:37:28Z\" status: \"True\" type: PodScheduled containerStatuses: - containerID: cri-o://42828d4a8333d4fa9d3882805680d7693616610ff78a3e07f4794d91b86862b5 image: docker.io/openshift/hello-openshift:latest imageID: docker.io/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e lastState: {} name: hello-openshift ready: true restartCount: 0 started: true state: running: startedAt: \"2020-01-30T20:37:30Z\" hostIP: 10.177.93.14 phase: Running podIP: 172.30.26.229 podIPs: - ip: 172.30.26.229 qosClass: BestEffort startTime: \"2020-01-30T20:37:28Z\" Show the logs of one of the pods: oc logs <pod name> serving on 8888 serving on 8080 Take a look at Service.yaml and note that it's for the project1 namespace: cat Service.yaml Example output: apiVersion: v1 kind: Service metadata: name: example namespace: project1 spec: ports: - protocol: TCP port: 8080 targetPort: 8080 selector: app: hello-openshift type: ClusterIP Create the service so that it's accessible and load balanced for pods with label app: hello-openshift within the project1 namespace: oc apply -f Service.yaml The service is created service/example created Examine Route.yaml: cat Route.yaml Output: apiVersion: route.openshift.io/v1 kind: Route metadata: name: example namespace: project1 spec: port: targetPort: 8080 to: kind: Service name: example Apply the route to make the service reachable from outside the cluster: oc apply -f Route.yaml The route is created route.route.openshift.io/example created Generate the URL for the route, and point your browser to it: echo http://$(oc get route example --template='{{ .spec.host }}') Output: http://example-project1.apps.demo.ibmdte.net Open your Firefox browser again and visit the URL outputted by the previous command. You should see a web page displaying the following message:","title":"First Deployment"},{"location":"basic-labs/IntroOpenshift/#changing-replica-instance","text":"List pods: oc get pods NAME READY STATUS RESTARTS AGE example-75778c488-7k7q2 1/1 Running 0 60m example-75778c488-c9jhd 1/1 Running 0 60m Delete one of the pods: oc delete pod <pod name> pod \"example-75778c488-7k7q2\" deleted List pods again and note that a new instance has been created as expected. The deployment specified 2 instances, so the controller tries to maintain 2 instances: oc get pods NAME READY STATUS RESTARTS AGE example-75778c488-c9jhd 1/1 Running 0 63m example-75778c488-rhjrx 1/1 Running 0 28s To reduce the number of pods, we can patch the resource in one of two ways: Scripted patch using the patch option of the command line: oc patch deployment example -p '{ \"spec\": { \"replicas\": 1 } }' Interactive patch using the edit option of the command line through vi editor: oc edit deployment example Under the spec section (not under the status section), change replicas: 2 to replicas: 1 , and save the change in the vi editor (by :wq ). The output: deployment.extensions/example edited Note: The above edits the copy that is stored in Openshift. You may also edit your local copy of Deployment.yaml and re-apply it. List the pods to show only 1 pod is running: oc get pods NAME READY STATUS RESTARTS AGE example-75778c488-c9jhd 1/1 Running 0 65m Cleanup: oc delete route example oc delete service example oc delete deployment example oc get pods Note: You may have to do this a few times, to wait for the pods to be deleted. Congratulations, you have deployed your first application to Openshift via the command line.","title":"Changing Replica Instance"},{"location":"basic-labs/IntroOpenshift/#next","text":"Please follow the link to the next lab Operational Modernization : - Operational Modernization","title":"Next"},{"location":"basic-labs/TA/","text":"TA Lab Under Development","title":"Home"},{"location":"basic-labs/TA/#ta-lab","text":"","title":"TA Lab"},{"location":"basic-labs/TA/#under-development","text":"","title":"Under Development"},{"location":"devops-labs/","text":"Devops Modernization Labs Under Development Lab 6: Openshift Pipelines Lab 7: GitOps with ArgoCD","title":"DevOps Labs"},{"location":"devops-labs/#devops-modernization-labs","text":"","title":"Devops Modernization Labs"},{"location":"devops-labs/argocd/","text":"Devops Modernization Lab Under Development","title":"Home"},{"location":"devops-labs/argocd/#devops-modernization-lab","text":"","title":"Devops Modernization Lab"},{"location":"devops-labs/argocd/#under-development","text":"","title":"Under Development"},{"location":"devops-labs/pipelines/","text":"Devops Management - Using OpenShift pipelines (Tekton) for CI/CD of microservices to RedHat OpenShift Container Platform 1.1 Introduction In this lab exercise, we will deploy a cloud native application to an OpenShift cluster using OpenShift Pipelines (Tekton). This is a step-by-step guide to walk you through an example of how to create a Tekton pipeline to automate the build, push, and deploy a Java application based on OpenLiberty containers on OpenShift. This example uses Buildah as the docker build engine. There are other options for the docker build engine, so it should be noted that this is not the only way to accomplish this task. 1.2 What is all this Tekton stuff? Tekton defines a set of Kubernetes custom resource definitions (CRD) as standard constructs for creating Continuous Integration and Continuous Delivery (CI/CD) pipelines. The following is a brief introduction to the Tekton CRDs. Task : A sequence of commands (steps) that are run in separate containers in a pod Pipeline: A collection of tasks that are executed in a defined order TaskRun: Runtime representation of an execution of a task PipelineRun: Runtime representation of an execution of a pipeline Let\u2019s investigate a bit more detail what makes up a Tekton Pipeline. As explained above, all objects within a Tekton pipeline are Kubernetes objects. Pipelines have tasks , which are a CRD that runs a container. Within the task you define steps , which are commands that you will run inside the container. Pipelines normally have resources associated with them, which can be accessed by all tasks within that pipeline. It should be noted that tasks can be used within multiple pipelines, so it's good practice to use pipeline resources to define the resources used, such as GitHub repositories or docker hub image definitions. 1.3 What exactly are we building here? Here, you will deploy a Tekton Pipeline and three Task objects. Clone your source code from GitHub and store it locally Build the Docker image. Once the image is built, the image is pushed to a local image repository in OpenShift. Deploy the containerized application to the Open Liberty runtime in OpenShift Here is a diagram of what you are going to build in this lab. 1.4 Lab Tasks 1.4.1 Let\u2019s get started First, launch the lab environment and login to the VM. If the VM is not already started, start it by clicking the Play button. After the VM is running (this may take several minutes), click the desktop VM to access it. Login with the ibmuser user using the password engageibm. Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the Fit to Size icon. This will enlarge the viewing area to fit the size of your browser window. Open a new terminal window from the VM desktop. 1.4.2 Clone the Git repository used for this lab and explore the contents Clone the devops-management GitHub repository to the local VM. a. Open a Terminal window on the VM, and ensure you are in the home directory of the user \u201cibmuser\u201d cd /home/ibmuser b. From the terminal window, run the following commands to clone the repo: git clone https://github.com/kellyyfoo/devops-management These commands above clone the public repo named devops-management to the local directory under /home/ibmuser/devops-management directory. c. Change to the cloned directory cd /home/ibmuser/Devops-management d. List the directory contents using the ls command You will find the following key resources: Dockerfile \u2013 Used to build the application on the Liberty runtime app \u2013 The Customer Order Services application deploy - Used to handle deployment of the application tekton-pipeline (folder) \u2013 YAML files to create the Pipeline resources for this lab In the GitHub repo, you will find all the YAML files in the tekton-pipeline sub folder. Enter cd tekton-pipeline then type ls to go to the lab directory and list the contents. In the devops-management/tekton-pipeline directory, you will find all the YAML files needed to create the Tekton pipeline resources to build and deploy the application to OpenShift. You will find the following key resources: git-clone.yaml - Creates the Tekton task to clone a Git repo from a given URL and loads a Workspace buildah.yaml \u2013 Creates the build and push Tekton tasks oc-deploy.yaml \u2013 Creates the Tekton deployment Task to deploy the application to OpenShift pipeline-deploy.yaml \u2013 Creates the pipeline that invokes the tasks defined pipeline-deploy-run.yaml \u2013 Runtime execution of the pipeline to build and deploy the app 1.4.3 Login to OpenShift and create a new project for this lab Type oc login to login to OpenShift. Use ibmadmin for the username and engageibm for the password. Type oc new-project dev which will create a new project named dev, and switch your context to that project Note: Ensure you create the new project with the name \u201c dev \u201d. Otherwise, you will be required to review and modify all YAML files that reference this OpenShift project (Namespace), prior to running the YAML files to create the pipeline resources 1.4.4 OpenShift Service Account and Secret with Login Token It is good OpenShift practice to create a service account for your applications. A service account provides an identity for processes that run in a Pod. The OpenShift Pipelines Operator has already been setup in your VM. This operator conveniently creates and configures a service account named pipeline for us that has permission to build and push images. This enables us to deploy apps to OpenShift and allows the pipeline to create pods when it is run. Usually, we would have to manually grant our service account permission to run pipelines, deploy apps to OpenShift, and push images to the image registry, but this has already been set up. Additionally, OpenShift secrets are used to hold private information that could be sensitive including data such as passwords. A secret has already been created for this service account with the token required to login and execute deployment during the pipeline run. 1.4.5 Create the Tekton task to clone a repo into your Workspace Now, you will create the Tekton task that works with Git repositories so that other tasks in your Pipeline can use them. This git-clone task clones a Git repo from a provided URL into the output Workspace so that other tasks can access it. By default, the repo is cloned into the Workspace's root directory but can be cloned into a different subdirectory by setting the Task's subdirectory parameter. For this lab, we will simply use the master branch of the repo, but we can also checkout different repo branches, tags, etc. with the Task's revision parameter. Ensure the Terminal window is in the /home/ibmuser/devops-management/tekton-pipeline directory, where the Pipeline YAML files are located. cd /home/ibmuser/devops-management/tekton-pipeline Review the contents of git-clone.yaml gedit git-clone.yaml The name of the Task Resource is git-clone The git repo is cloned into the root of the output Workspace The url to the source git repo is passed through the \u201c url \u201d parameter. Different branches, tags, etc. to checkout are passed through the \" revision \" parameter. Note: Do NOT MODIFY the YAML for this lab! a. Close the Gedit editor when you have finished reviewing the contents . DO NOT SAVE ANY CHANGES! Create the Task, using the git-clone.yaml file, then list the new \u201cgit-clone\u201d task. oc create -f git-clone.yaml oc get tasks 1.4.6 Create a Tekton Task to build the Docker image, and push the image to the OpenShift Image Registry Now, you are ready to create the Tekton Task that build the container image and push it to the internal OpenShift image registry . The task includes two steps: build the docker image from the source GitHub repo pushes the image to the OpenShift image registry For this lab, you will use buildah . ( https://buildah.io/ ) Buildah is a command-line tool for building Open Container Initiative-compatible (that means Docker- and Kubernetes-compatible, too) images quickly and easily. Buildah is easy to incorporate into scripts and build pipelines. Review the buildah.yaml file using gedit command. DO NOT MODIFY THE FILE gedit buildah.yaml The Task resource defines its spec: The source Workspace Local parameters used during the execution of the task Steps. In this example, \u201c build \u201d and \u201c push \u201d is described The image used for the task execution. here, it is quay.io/buildah/stable . The commands to execute in the \u201cbuild\u201d and \u201cpush\u201d steps. The first one builds the image, the second pushes it to the target repository. In general, steps are used to isolate individual commands, and illustrated below. Close the Gedit editor when you have finished reviewing the contents . DO NOT SAVE ANY CHANGES! Create the Task, using the buildah.yaml file, then list the new \u201cbuildah\u201d task. oc create -f buildah.yaml oc get tasks 1.4.7 Create the Deployment Task To manage the deployment of this application, tasks are needed to specify a Deployment (controller for pods) in OpenShift. The oc-deploy.yaml file defines a Tekton Task that in turn invokes a command to run deploy/overlays/dev to create the deployment. To enable this action, each task will define a step using the image-registry.openshift-image-registry.svc:5000/openshift/cli:latest image registry. Review the oc-deploy.yaml file using cat command. DO NOT MODIFY THE FILE cat oc-deploy.yaml The deploy/overlays/dev invoked by deploy-cm is used to create the deployment for the application. Note: The deploy task leverages capabilities from a tool called \u201c Kustomize \u201d by Google. Kustomize is installed with OpenShift in the lab environment. A Brief introduction to Kustomize is provided in the next section of the lab. Run the oc-deploy.yaml to create the Tekton Deployment Task. Then, list the new task. oc create -f oc-deploy.yaml oc get tasks 1.4.7.1 Brief Introduction to Kustomize As seen in the oc-deploy.yaml , the deploy task leverages a tool called Kustomize, with OpenShift. Kustomize lets you customize raw, template-free YAML files for multiple purposes, leaving the original YAML untouched and usable as is. kustomize targets kubernetes; it understands and can patch kubernetes style API objects. Kustomize is very useful in the common use case where you\u2019ll need multiple variants of a common set of resources across multiple environments: , e.g., a development , staging and production variant. For this purpose, kustomize supports the idea of an overlay and a base . Both are represented by a kustomization file. The base declares things that the variants share in common (both resources and a common customization of those resources), and the overlays declare the differences. Here\u2019s a file system layout to manage a staging and production variant of a given cluster app: In this lab, the base directory includes the kustomize.yaml and the OpenLiberty application yaml. We have an overlay folder named \u201c dev \u201d that includes the overrides for deploying the application to the \u201cdev\u201d namespace. The overlays include two customizations over the base configuration: The dev namespace is applied to all resources. The ApplicationImage in the OpenLiberty YAML is updated to the image tagged for \u201cdev\u201d To learn more about Kustomize, refer to the following resources: https://kubernetes.io/blog/2018/05/29/introducing-kustomize-template-free-configuration-customization-for-kubernetes/ https://kustomize.io/ 1.4.8 Create the Pipeline that invokes the build/push and deploy Tasks you created Now that the tasks have been created, they can be incorporated and orchestrated in a Pipeline. The pipeline in the lab does the following: First, the pipeline runs the git-clone task that clones from the git repo URL Next, the pipeline runs the buildah task that performs the build and push steps Once the build-push task completes, the deploy-cm task is executed to deploy the app to OpenShift The pipeline orchestrates the order of the task execution using the runAfter tag in the pipeline definition. If the build-push task fails, the deploy task will not run. Review the pipeline-deploy.yaml file cat pipeline-deploy.yaml Snippet showing the tasks in the pipeline-deploy.yml file Use the pipeline-deploy.yaml file to create the pipeline. Then, list the new pipeline oc create -f pipeline-deploy.yaml oc get pipelines 1.4.9 Run the Pipeline To execute the pipeline, a PipelineRun artifact should be created. A PipelineRun starts a Pipeline and ties it to the Workspace containing all required resources. It automatically creates and starts the TaskRuns for each Task in the Pipeline. Review the pipeline-deploy-run.yaml file cat pipeline-deploy-run.yaml The PipelineRun identifies the pipeline to run, and provides the workspace and parameters used during its execution. It also defines the Service Account that runs the pipeline. Execute the PipelineRun using the YAML file oc create -f pipeline-deploy-run.yaml Next, let\u2019s do some basic queries to ensure the pipeline is executing. Then, you will launch the OpenShift console to view the PipelineRun. A fully completed and successful pipelineRun will result in the pod states below. Note: It may take 10 minutes to run the pipeline, as it builds the docker image for the app, pushes the image to the OpenShift image registry, and deploys the application. The cos-<pod ID> is the application that was deployed via the pipeline. This pod MUST be running , as it is the cos application that was deployed. The tutorial-pipeline-run-1-build-push pod is the pod that ran the build/push tasks The tutorial-pipeline-run-1-deploy-to-cluster pod is the pod that ran the deploy task The tutorial-pipeline-run-1-git-clone pod is the pod that ran the clone Git repo task 1.4.10 Access the OpenShift console to view the PipelineRun status and logs Access the OpenShift console. a. Click the Firefox browser icon located on the VM desktop. b. From the browser, click the openshift console bookmark located on the bookmark toolbar and login with the htpasswd option. c. Login to the OpenShift account with username ibmadmin and password engageibm. From the OpenShift console, go the Pipelines tab and click on Pipelines . Use the Project drop down menu and select the dev project. Find the pipeline with name tutorial-pipeline, and click on its last run named tutorial-pipeline-run-1 to view its details. The pipeline run's details should be displayed on the resulting page. Its git-clone , build-push , and deploy steps are show. Click on the Logs tab to see the logs outputted by each of the steps. 1.4.11 Validate the application is deployed and runs as expected Upon successful completion of the pipeline, the sample application is deployed to OpenShift. In this section, you will view the application resources that were deployed to OpenShift and validate the sample application runs as expected. Use the following commands to verify the application is deployed and the pod is running oc get deployments oc get pods | grep cos Use the following commands to verify the service was created oc get services Test the application from the web browser. Return to the OpenShift console and scroll to the end of the logs of the deploy-to-cluster step. Copy the URL that is displayed on the last line: http://cos-dev.apps.demo.ibmdte.net/CustomerOrderServicesWeb Open this URL in a new tab. You will be prompted to login in order to access the application. Enter username skywalker and password force The application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on a movie and on the next pop-up panel, drag and drop the item into the shopping cart. Add a few items to the cart. As the items are added, they'll be shown under Current Shopping Cart and Order Total will be updated. 1.5 Conclusion Congratulations! You have completed the lab and are on your way to developing robust pipelines for CI/CD of your application deployments using containers and RedHat OpenShift Container Platform. In this lab, you learned how to create the Tekton resources to automate CI/CD for microservices deployed to OpenShift. Task Pipeline PipelineRun End of Lab: Using Tekton pipelines for CI/CD of microservices to RedHat OpenShift Container Platform Appendix: SkyTap Tips for labs How to use Copy / Paste between local desktop and Skytap VM Using copy / Paste capabilities between the lab document (PDF) on your local workstation to the VM is a good approach to more efficiently work through a lab, while reducing the typing errors that often occur when manually entering data. In SkyTap, you will find that any text copied to the clipboard on your local workstation is not available to be pasted into the VM on SkyTap. So how can you easily accomplish this? a. First copy the text you intend to paste, from the lab document, to the clipboard on your local workstation, as you always have (CTRL-C) b. Return to the SkyTap environment and click on the Clipboard at the top of the SkyTap session window. c. Use CTRL-V to paste the content into the Copy/paste VM clipboard. Or use the paste menu item that is available in the dialog, when you right mouse click in the clipboard text area. d. Once the text is pasted, just navigate away to the VM window where you want to paste the content. Then, use CTRL-C , or right mouse click & us the paste menu item to paste the content. e. The text is pasted into the VM Note: The very first time you do this, if the text does not paste, you may have to paste the contents into the Skytap clipboard twice. This is a known Skytap issue. It only happens on the 1 st attempt to copy / paste into Skytap.","title":"Devops Management - Using OpenShift pipelines (Tekton) for CI/CD of microservices to RedHat OpenShift Container Platform"},{"location":"devops-labs/pipelines/#devops-management-using-openshift-pipelines-tekton-for-cicd-of-microservices-to-redhat-openshift-container-platform","text":"","title":"Devops Management - Using OpenShift pipelines (Tekton) for CI/CD of microservices to RedHat OpenShift Container Platform"},{"location":"devops-labs/pipelines/#11-introduction","text":"In this lab exercise, we will deploy a cloud native application to an OpenShift cluster using OpenShift Pipelines (Tekton). This is a step-by-step guide to walk you through an example of how to create a Tekton pipeline to automate the build, push, and deploy a Java application based on OpenLiberty containers on OpenShift. This example uses Buildah as the docker build engine. There are other options for the docker build engine, so it should be noted that this is not the only way to accomplish this task.","title":"1.1 Introduction"},{"location":"devops-labs/pipelines/#12-what-is-all-this-tekton-stuff","text":"Tekton defines a set of Kubernetes custom resource definitions (CRD) as standard constructs for creating Continuous Integration and Continuous Delivery (CI/CD) pipelines. The following is a brief introduction to the Tekton CRDs. Task : A sequence of commands (steps) that are run in separate containers in a pod Pipeline: A collection of tasks that are executed in a defined order TaskRun: Runtime representation of an execution of a task PipelineRun: Runtime representation of an execution of a pipeline Let\u2019s investigate a bit more detail what makes up a Tekton Pipeline. As explained above, all objects within a Tekton pipeline are Kubernetes objects. Pipelines have tasks , which are a CRD that runs a container. Within the task you define steps , which are commands that you will run inside the container. Pipelines normally have resources associated with them, which can be accessed by all tasks within that pipeline. It should be noted that tasks can be used within multiple pipelines, so it's good practice to use pipeline resources to define the resources used, such as GitHub repositories or docker hub image definitions.","title":"1.2 What is all this Tekton stuff?"},{"location":"devops-labs/pipelines/#13-what-exactly-are-we-building-here","text":"Here, you will deploy a Tekton Pipeline and three Task objects. Clone your source code from GitHub and store it locally Build the Docker image. Once the image is built, the image is pushed to a local image repository in OpenShift. Deploy the containerized application to the Open Liberty runtime in OpenShift Here is a diagram of what you are going to build in this lab.","title":"1.3 What exactly are we building here?"},{"location":"devops-labs/pipelines/#14-lab-tasks","text":"","title":"1.4 Lab Tasks"},{"location":"devops-labs/pipelines/#141-lets-get-started","text":"First, launch the lab environment and login to the VM. If the VM is not already started, start it by clicking the Play button. After the VM is running (this may take several minutes), click the desktop VM to access it. Login with the ibmuser user using the password engageibm. Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the Fit to Size icon. This will enlarge the viewing area to fit the size of your browser window. Open a new terminal window from the VM desktop.","title":"1.4.1 Let\u2019s get started"},{"location":"devops-labs/pipelines/#142-clone-the-git-repository-used-for-this-lab-and-explore-the-contents","text":"Clone the devops-management GitHub repository to the local VM. a. Open a Terminal window on the VM, and ensure you are in the home directory of the user \u201cibmuser\u201d cd /home/ibmuser b. From the terminal window, run the following commands to clone the repo: git clone https://github.com/kellyyfoo/devops-management These commands above clone the public repo named devops-management to the local directory under /home/ibmuser/devops-management directory. c. Change to the cloned directory cd /home/ibmuser/Devops-management d. List the directory contents using the ls command You will find the following key resources: Dockerfile \u2013 Used to build the application on the Liberty runtime app \u2013 The Customer Order Services application deploy - Used to handle deployment of the application tekton-pipeline (folder) \u2013 YAML files to create the Pipeline resources for this lab In the GitHub repo, you will find all the YAML files in the tekton-pipeline sub folder. Enter cd tekton-pipeline then type ls to go to the lab directory and list the contents. In the devops-management/tekton-pipeline directory, you will find all the YAML files needed to create the Tekton pipeline resources to build and deploy the application to OpenShift. You will find the following key resources: git-clone.yaml - Creates the Tekton task to clone a Git repo from a given URL and loads a Workspace buildah.yaml \u2013 Creates the build and push Tekton tasks oc-deploy.yaml \u2013 Creates the Tekton deployment Task to deploy the application to OpenShift pipeline-deploy.yaml \u2013 Creates the pipeline that invokes the tasks defined pipeline-deploy-run.yaml \u2013 Runtime execution of the pipeline to build and deploy the app","title":"1.4.2 Clone the Git repository used for this lab and explore the contents"},{"location":"devops-labs/pipelines/#143-login-to-openshift-and-create-a-new-project-for-this-lab","text":"Type oc login to login to OpenShift. Use ibmadmin for the username and engageibm for the password. Type oc new-project dev which will create a new project named dev, and switch your context to that project Note: Ensure you create the new project with the name \u201c dev \u201d. Otherwise, you will be required to review and modify all YAML files that reference this OpenShift project (Namespace), prior to running the YAML files to create the pipeline resources","title":"1.4.3 Login to OpenShift and create a new project for this lab"},{"location":"devops-labs/pipelines/#144-openshift-service-account-and-secret-with-login-token","text":"It is good OpenShift practice to create a service account for your applications. A service account provides an identity for processes that run in a Pod. The OpenShift Pipelines Operator has already been setup in your VM. This operator conveniently creates and configures a service account named pipeline for us that has permission to build and push images. This enables us to deploy apps to OpenShift and allows the pipeline to create pods when it is run. Usually, we would have to manually grant our service account permission to run pipelines, deploy apps to OpenShift, and push images to the image registry, but this has already been set up. Additionally, OpenShift secrets are used to hold private information that could be sensitive including data such as passwords. A secret has already been created for this service account with the token required to login and execute deployment during the pipeline run.","title":"1.4.4 OpenShift Service Account and Secret with Login Token"},{"location":"devops-labs/pipelines/#145-create-the-tekton-task-to-clone-a-repo-into-your-workspace","text":"Now, you will create the Tekton task that works with Git repositories so that other tasks in your Pipeline can use them. This git-clone task clones a Git repo from a provided URL into the output Workspace so that other tasks can access it. By default, the repo is cloned into the Workspace's root directory but can be cloned into a different subdirectory by setting the Task's subdirectory parameter. For this lab, we will simply use the master branch of the repo, but we can also checkout different repo branches, tags, etc. with the Task's revision parameter. Ensure the Terminal window is in the /home/ibmuser/devops-management/tekton-pipeline directory, where the Pipeline YAML files are located. cd /home/ibmuser/devops-management/tekton-pipeline Review the contents of git-clone.yaml gedit git-clone.yaml The name of the Task Resource is git-clone The git repo is cloned into the root of the output Workspace The url to the source git repo is passed through the \u201c url \u201d parameter. Different branches, tags, etc. to checkout are passed through the \" revision \" parameter. Note: Do NOT MODIFY the YAML for this lab! a. Close the Gedit editor when you have finished reviewing the contents . DO NOT SAVE ANY CHANGES! Create the Task, using the git-clone.yaml file, then list the new \u201cgit-clone\u201d task. oc create -f git-clone.yaml oc get tasks","title":"1.4.5 Create the Tekton task to clone a repo into your Workspace"},{"location":"devops-labs/pipelines/#146-create-a-tekton-task-to-build-the-docker-image-and-push-the-image-to-the-openshift-image-registry","text":"Now, you are ready to create the Tekton Task that build the container image and push it to the internal OpenShift image registry . The task includes two steps: build the docker image from the source GitHub repo pushes the image to the OpenShift image registry For this lab, you will use buildah . ( https://buildah.io/ ) Buildah is a command-line tool for building Open Container Initiative-compatible (that means Docker- and Kubernetes-compatible, too) images quickly and easily. Buildah is easy to incorporate into scripts and build pipelines. Review the buildah.yaml file using gedit command. DO NOT MODIFY THE FILE gedit buildah.yaml The Task resource defines its spec: The source Workspace Local parameters used during the execution of the task Steps. In this example, \u201c build \u201d and \u201c push \u201d is described The image used for the task execution. here, it is quay.io/buildah/stable . The commands to execute in the \u201cbuild\u201d and \u201cpush\u201d steps. The first one builds the image, the second pushes it to the target repository. In general, steps are used to isolate individual commands, and illustrated below. Close the Gedit editor when you have finished reviewing the contents . DO NOT SAVE ANY CHANGES! Create the Task, using the buildah.yaml file, then list the new \u201cbuildah\u201d task. oc create -f buildah.yaml oc get tasks","title":"1.4.6 Create a Tekton Task to build the Docker image, and push the image to the OpenShift Image Registry"},{"location":"devops-labs/pipelines/#147-create-the-deployment-task","text":"To manage the deployment of this application, tasks are needed to specify a Deployment (controller for pods) in OpenShift. The oc-deploy.yaml file defines a Tekton Task that in turn invokes a command to run deploy/overlays/dev to create the deployment. To enable this action, each task will define a step using the image-registry.openshift-image-registry.svc:5000/openshift/cli:latest image registry. Review the oc-deploy.yaml file using cat command. DO NOT MODIFY THE FILE cat oc-deploy.yaml The deploy/overlays/dev invoked by deploy-cm is used to create the deployment for the application. Note: The deploy task leverages capabilities from a tool called \u201c Kustomize \u201d by Google. Kustomize is installed with OpenShift in the lab environment. A Brief introduction to Kustomize is provided in the next section of the lab. Run the oc-deploy.yaml to create the Tekton Deployment Task. Then, list the new task. oc create -f oc-deploy.yaml oc get tasks","title":"1.4.7 Create the Deployment Task"},{"location":"devops-labs/pipelines/#1471-brief-introduction-to-kustomize","text":"As seen in the oc-deploy.yaml , the deploy task leverages a tool called Kustomize, with OpenShift. Kustomize lets you customize raw, template-free YAML files for multiple purposes, leaving the original YAML untouched and usable as is. kustomize targets kubernetes; it understands and can patch kubernetes style API objects. Kustomize is very useful in the common use case where you\u2019ll need multiple variants of a common set of resources across multiple environments: , e.g., a development , staging and production variant. For this purpose, kustomize supports the idea of an overlay and a base . Both are represented by a kustomization file. The base declares things that the variants share in common (both resources and a common customization of those resources), and the overlays declare the differences. Here\u2019s a file system layout to manage a staging and production variant of a given cluster app: In this lab, the base directory includes the kustomize.yaml and the OpenLiberty application yaml. We have an overlay folder named \u201c dev \u201d that includes the overrides for deploying the application to the \u201cdev\u201d namespace. The overlays include two customizations over the base configuration: The dev namespace is applied to all resources. The ApplicationImage in the OpenLiberty YAML is updated to the image tagged for \u201cdev\u201d To learn more about Kustomize, refer to the following resources: https://kubernetes.io/blog/2018/05/29/introducing-kustomize-template-free-configuration-customization-for-kubernetes/ https://kustomize.io/","title":"1.4.7.1 Brief Introduction to Kustomize"},{"location":"devops-labs/pipelines/#148-create-the-pipeline-that-invokes-the-buildpush-and-deploy-tasks-you-created","text":"Now that the tasks have been created, they can be incorporated and orchestrated in a Pipeline. The pipeline in the lab does the following: First, the pipeline runs the git-clone task that clones from the git repo URL Next, the pipeline runs the buildah task that performs the build and push steps Once the build-push task completes, the deploy-cm task is executed to deploy the app to OpenShift The pipeline orchestrates the order of the task execution using the runAfter tag in the pipeline definition. If the build-push task fails, the deploy task will not run. Review the pipeline-deploy.yaml file cat pipeline-deploy.yaml Snippet showing the tasks in the pipeline-deploy.yml file Use the pipeline-deploy.yaml file to create the pipeline. Then, list the new pipeline oc create -f pipeline-deploy.yaml oc get pipelines","title":"1.4.8 Create the Pipeline that invokes the build/push and deploy Tasks you created"},{"location":"devops-labs/pipelines/#149-run-the-pipeline","text":"To execute the pipeline, a PipelineRun artifact should be created. A PipelineRun starts a Pipeline and ties it to the Workspace containing all required resources. It automatically creates and starts the TaskRuns for each Task in the Pipeline. Review the pipeline-deploy-run.yaml file cat pipeline-deploy-run.yaml The PipelineRun identifies the pipeline to run, and provides the workspace and parameters used during its execution. It also defines the Service Account that runs the pipeline. Execute the PipelineRun using the YAML file oc create -f pipeline-deploy-run.yaml Next, let\u2019s do some basic queries to ensure the pipeline is executing. Then, you will launch the OpenShift console to view the PipelineRun. A fully completed and successful pipelineRun will result in the pod states below. Note: It may take 10 minutes to run the pipeline, as it builds the docker image for the app, pushes the image to the OpenShift image registry, and deploys the application. The cos-<pod ID> is the application that was deployed via the pipeline. This pod MUST be running , as it is the cos application that was deployed. The tutorial-pipeline-run-1-build-push pod is the pod that ran the build/push tasks The tutorial-pipeline-run-1-deploy-to-cluster pod is the pod that ran the deploy task The tutorial-pipeline-run-1-git-clone pod is the pod that ran the clone Git repo task","title":"1.4.9 Run the Pipeline"},{"location":"devops-labs/pipelines/#1410-access-the-openshift-console-to-view-the-pipelinerun-status-and-logs","text":"Access the OpenShift console. a. Click the Firefox browser icon located on the VM desktop. b. From the browser, click the openshift console bookmark located on the bookmark toolbar and login with the htpasswd option. c. Login to the OpenShift account with username ibmadmin and password engageibm. From the OpenShift console, go the Pipelines tab and click on Pipelines . Use the Project drop down menu and select the dev project. Find the pipeline with name tutorial-pipeline, and click on its last run named tutorial-pipeline-run-1 to view its details. The pipeline run's details should be displayed on the resulting page. Its git-clone , build-push , and deploy steps are show. Click on the Logs tab to see the logs outputted by each of the steps.","title":"1.4.10 Access the OpenShift console to view the PipelineRun status and logs"},{"location":"devops-labs/pipelines/#1411-validate-the-application-is-deployed-and-runs-as-expected","text":"Upon successful completion of the pipeline, the sample application is deployed to OpenShift. In this section, you will view the application resources that were deployed to OpenShift and validate the sample application runs as expected. Use the following commands to verify the application is deployed and the pod is running oc get deployments oc get pods | grep cos Use the following commands to verify the service was created oc get services Test the application from the web browser. Return to the OpenShift console and scroll to the end of the logs of the deploy-to-cluster step. Copy the URL that is displayed on the last line: http://cos-dev.apps.demo.ibmdte.net/CustomerOrderServicesWeb Open this URL in a new tab. You will be prompted to login in order to access the application. Enter username skywalker and password force The application page titled Electronic and Movie Depot will be displayed. From the Shop tab, click on a movie and on the next pop-up panel, drag and drop the item into the shopping cart. Add a few items to the cart. As the items are added, they'll be shown under Current Shopping Cart and Order Total will be updated.","title":"1.4.11 Validate the application is deployed and runs as expected"},{"location":"devops-labs/pipelines/#15-conclusion","text":"Congratulations! You have completed the lab and are on your way to developing robust pipelines for CI/CD of your application deployments using containers and RedHat OpenShift Container Platform. In this lab, you learned how to create the Tekton resources to automate CI/CD for microservices deployed to OpenShift. Task Pipeline PipelineRun","title":"1.5 Conclusion"},{"location":"devops-labs/pipelines/#end-of-lab-using-tekton-pipelines-for-cicd-of-microservices-to-redhat-openshift-container-platform","text":"","title":"End of Lab: Using Tekton pipelines for CI/CD of microservices to RedHat OpenShift Container Platform"},{"location":"devops-labs/pipelines/#appendix-skytap-tips-for-labs","text":"","title":"Appendix: SkyTap Tips for labs"},{"location":"devops-labs/pipelines/#how-to-use-copy-paste-between-local-desktop-and-skytap-vm","text":"Using copy / Paste capabilities between the lab document (PDF) on your local workstation to the VM is a good approach to more efficiently work through a lab, while reducing the typing errors that often occur when manually entering data. In SkyTap, you will find that any text copied to the clipboard on your local workstation is not available to be pasted into the VM on SkyTap. So how can you easily accomplish this? a. First copy the text you intend to paste, from the lab document, to the clipboard on your local workstation, as you always have (CTRL-C) b. Return to the SkyTap environment and click on the Clipboard at the top of the SkyTap session window. c. Use CTRL-V to paste the content into the Copy/paste VM clipboard. Or use the paste menu item that is available in the dialog, when you right mouse click in the clipboard text area. d. Once the text is pasted, just navigate away to the VM window where you want to paste the content. Then, use CTRL-C , or right mouse click & us the paste menu item to paste the content. e. The text is pasted into the VM Note: The very first time you do this, if the text does not paste, you may have to paste the contents into the Skytap clipboard twice. This is a known Skytap issue. It only happens on the 1 st attempt to copy / paste into Skytap.","title":"How to use Copy / Paste between local desktop and Skytap VM"},{"location":"liberty-labs/","text":"Liberty Developemnt labs Under development! Getting started with Liberty Explore Liberty and WebSphere developer tools Liberty and Dev Mode Liberty with VS Code and Open Liberty Tools Liberty and WebSphere Migration Tools","title":"Hands-on Lab List"},{"location":"liberty-labs/#liberty-developemnt-labs","text":"","title":"Liberty Developemnt labs"},{"location":"liberty-labs/HelloContainer/","text":"Introduction to Containerization Table of Contents Introduction to Containerization Table of Contents Background Prerequisites What is a Container Login to the VM Check your environment Run a pre-built image Build and Run Your Own Image Managing Image Versions Extra Credit Next Background If you are expecting a lab about docker , you are at the right place. This lab will introduce you to the basic concepts of containerization, including: What are containers and container images How to start, stop, and remove containers. How to create container images How to version container images Prerequisites You have podman or docker installed. Only docker is installed for this lab. You have access to the internet. You have cloned this lab from github. If not, follows these steps: git clone https://github.com/IBM/openshift-workshop-was.git cd openshift-workshop-was/labs/Openshift/HelloContainer What is a Container Compared to virtual machines, containers supports virtualization at the process level. Think of them as virtual processes. The isolation abstraction provided by the operating system makes the process think that it's running in its own virtual machine. As processes, containers may be created, started, and stopped much more quickly than virtual machines. Everything you need to run your application, from the operating system and up, is stored in a special file called a container image. Container images are self contained and portable. You may run one or more instances anywhere. And you don't have to worry about missing prerequisites, because all prerequisites are stored in the image. Container images are created via tools such as docker or podman . Existing images are hosted in container registries. For example, docker hub, or registry.access.redhat.com, or your own internal registry. If you need more background on containers: https://www.docker.com/resources/what-container Login to the VM If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password. Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window. Check your environment Open a terminal window from within your VM. List version of docker: docker --version Example output: Docker version 20.10.7, build f0df350 For more background on docker command line: https://docs.docker.com/engine/reference/commandline/cli/ Run a pre-built image Container images must be available locally before they can be run. To list available local images: docker images Note: no container inages are listed at this point REPOSITORY TAG IMAGE ID CREATED SIZE Images are hosted in container registries. The default container registry for docker is docker hub, located at https://hub.docker.com. Let's pull a test image from docker hub: docker pull openshift/hello-openshift And the output: Using default tag: latest latest: Pulling from openshift/hello-openshift 4f4fb700ef54: Pull complete 8b32988996c5: Pull complete Digest: sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e Status: Downloaded newer image for openshift/hello-openshift:latest docker.io/openshift/hello-openshift:latest List available local images again: docker images The hello-openshift image is now listed REPOSITORY TAG IMAGE ID CREATED SIZE openshift/hello-openshift latest 7af3297a3fb4 2 years ago 6.09MB Inspect the image metadata: docker inspect openshift/hello-openshift Note : It exposes two ports: 8080 and 8888 It runs as user 1001 The entry point executable is /hello-openshift [ { \"Id\": \"sha256:7af3297a3fb4487b740ed6798163f618e6eddea1ee5fa0ba340329fcae31c8f6\", \"RepoTags\": [ \"openshift/hello-openshift:latest\" ], \"RepoDigests\": [ \"openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e\" ], ... \"Config\": { \"User\": \"1001\", ... \"ExposedPorts\": { \"8080/tcp\": {}, \"8888/tcp\": {} }, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], ... \"Entrypoint\": [ \"/hello-openshift\" ] ... } ] Run the image in an container: docker run --name hello1 -d -p 8080:8080 -p 8888:8888 openshift/hello-openshift Note that: The --name option gives the container a name. The -d option runs the command in the background as a daemon The -p option maps the port on the host to the port in the container. Through virtual networking, the port within the container is always the same for all running instances. But to support multiple concurrent running instances, the actual port on the host must be different for each instance. When you start the container, you may assign a new port on the host dynamically. The output of the command is the container ID for the running container. If the container starts successfully, the executable specified by the Entrypoint in the metadata is run. For our sample, it is /hello-openshift . Access the application in the container. a. Open the Firefox Web Browser from inside of the VM. b. Go to the URL http://localhost:8080 c. Also try port 8888 Run another instance of the same image. Note that this new instance is assigned new port numbers 8081 and 8889 on the host. This is so that they don't conflict with the ports 8080 and 8888 already allocated to the first instance. docker run --name hello2 -d -p 8081:8080 -p 8889:8888 openshift/hello-openshift Question: How does this compare to the time it takes to start a new virtual machine? Access the application in the new container the same way. a. Return to the Firefox Web Browser but instead go to the URL http://localhost:8081 b. Also try port 8889 Verify there are two containers running in the same host: docker ps : CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5a62f8527b44 openshift/hello-openshift \"/hello-openshift\" About a minute ago Up About a minute 0.0.0.0:8081->8080/tcp, 0.0.0.0:8889->8888/tcp hello2 c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 4 minutes ago Up 4 minutes 0.0.0.0:8080->8080/tcp, 0.0.0.0:8888->8888/tcp hello1 View the logs: docker logs hello1 And the output: serving on 8888 serving on 8080 View the logs on the second container: docker logs hello2 And the output: serving on 8888 serving on 8080 Note: within the container, each instance behaves as if it's running in its own virtual environment, and has opened the same ports. Outside of the container, different ports are opened. To export the file system of a running container: docker export hello1 > hello1.tar List the files on the file system: tar -tvf hello1.tar Note that this is a very small image. -rwxr-xr-x 0/0 0 2020-04-29 16:48 .dockerenv drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/ -rwxr-xr-x 0/0 0 2020-04-29 16:48 dev/console drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/pts/ drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/shm/ drwxr-xr-x 0/0 0 2020-04-29 16:48 etc/ -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/hostname -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/hosts lrwxrwxrwx 0/0 0 2020-04-29 16:48 etc/mtab -> /proc/mounts -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/resolv.conf -rwxr-xr-x 0/0 6089990 2018-04-18 10:22 hello-openshift drwxr-xr-x 0/0 0 2020-04-29 16:48 proc/ drwxr-xr-x 0/0 0 2020-04-29 16:48 sys/ Run another command in the running container. You can reach into the running container to run another command. The typical use case is to run a shell command, so you can use the shell to navigate within the container and run other commands. However, our image is tiny, and there is no built-in shell. For the purpose of this lab, we'll execute the same command again: docker exec -ti hello1 /hello-openshift`. Running this command again in the same container results in an error, because there is already another copy running in the background that is bound to the ports 8080 and 8888: serving on 8888 serving on 8080 panic: ListenAndServe: listen tcp :8888: bind: address already in use ... Stop the containers: docker stop hello1 docker stop hello2 List running containers: docker ps There should not be any running containers listed CONTAINER ID IMAGE COMMAND CREATED List all containers, including stopped containers: docker ps -a You should see the two cntainers listed, although not running; STATUS is Exited CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5a62f8527b44 openshift/hello-openshift \"/hello-openshift\" 28 minutes ago Exited (2) 28 seconds ago hello2 c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 31 minutes ago Exited (2) 32 seconds ago hello1 Restart a stopped container: docker restart hello1 List running containers: docker ps The hello-openshift container is now running again. CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 33 minutes ago Up 8 seconds 0.0.0.0:8080->8080/tcp, 0.0.0.0:8888->8888/tcp hello1 Stop the container: docker stop hello1 Remove stopped containers, and note that there are no more containers: docker rm hello1 docker rm hello2 docker ps -a Remove the image from local cache: a. View current images: docker images Example output: REPOSITORY TAG IMAGE ID CREATED SIZE openshift/hello-openshift latest 7af3297a3fb4 2 years ago 6.09MB b. Remove the image: docker rmi openshift/hello-openshift Example output: Untagged: openshift/hello-openshift:latest Untagged: openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e Deleted: sha256:7af3297a3fb4487b740ed6798163f618e6eddea1ee5fa0ba340329fcae31c8f6 Deleted: sha256:8fd6a1ece3ceceae6d714004614bae5b581c83ab962d838ef88ce760583dcb80 Deleted: sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef c. Check that the image has been removed: docker images Example output: REPOSITORY TAG IMAGE ID CREATED SIZE Build and Run Your Own Image We use a Containerfile , which contains the instructions to create the new layers of your image. For those familiar with docker, the Containerfile is equivalent to Dockerfile . Recall that an image contains the entire file system that you want to use to run your virtual process in a container. For this sample, we are building a new image for a Java EE web application ServletApp.war . It is configured to run on the WebSphere Liberty Runtime. The configuration file for the server is in the server.xml . Change directory to openshift-workshop-was/labs/Openshift/HelloContainer cd openshift-workshop-was/labs/Openshift/HelloContainer Review the provided Containerfile from the directory: cat Containerfile Content of Containerfile : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi COPY server.xml /config COPY ServletApp.war /config/dropins/app.war RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml To create a new image, you start with a pre-existing image. The first line FROM specifies the existing image to be used as the base. If this is not in the local registry, it will be pulled from a remote registry such as docker hub. The base image we are using, ibmcom/websphere-liberty , is already prepackaged for us and made available on docker hub. The second line COPY is a straight copy of the file server.xml from the local directory to /config/server.xml in the image. This adds a new layer to the image with the actual server configuration to be used. The third line, another COPY , copies ServletApp.war from the current directory into a new layer in the image you are creating, at the location /config/dropins/app.war . The last line RUN runs the installUtility command within the image to install additional features required to run the server as specified in server.xml . You can use the RUN command to run any command that is available within the image to customize the image itself. Run the build. Ensure you include . at the end of the command (the dot indicates using the file from the current directory): docker build -t app -f Containerfile . The -t option tags the name of the image as app . The -f option specifies the name of the Containerfile . The build command runs the commands in Containerfile to build a new image called app . Example output: Sending build context to Docker daemon 25.6kB Step 1/4 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi kernel-java8-ibmjava-ubi: Pulling from ibmcom/websphere-liberty ee2244abc66f: Pull complete befb03b11956: Pull complete 137dc88f6a93: Pull complete 5bdd69a33184: Pull complete d4e2554981d7: Pull complete 32c91bc0f2e1: Pull complete db7e931336a9: Pull complete 3b32f9956ae2: Pull complete 304584ffa0a2: Pull complete 9f6da4c82b7e: Pull complete b6fa5b2e2325: Pull complete Digest: sha256:d76f79695afe2f653fc7b272f9a629105446e6b78ff0d733d494c93ff05728e7 Status: Downloaded newer image for ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> 4d9265befb26 Step 2/4 : COPY server.xml /config ---> 4a02d03d3725 Step 3/4 : COPY ServletApp.war /config/dropins/app.war ---> b2def2a0feac Step 4/4 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Running in 5f5b05aec1ae Checking for missing features required by the server ... The server requires the following additional features: appsecurity-2.0 servlet-3.0. Installing features from the repository ... Establishing a connection to the configured repositories ... This process might take several minutes to complete. Successfully connected to all configured repositories. Preparing assets for installation. This process might take several minutes to complete. Additional Liberty features must be installed for this server. To install the additional features, review and accept the feature license agreement: The --acceptLicense argument was found. This indicates that you have accepted the terms of the license agreement. Step 1 of 12: Downloading ssl-1.0 ... Step 2 of 12: Installing ssl-1.0 ... Step 3 of 12: Downloading appSecurity-2.0 ... Step 4 of 12: Installing appSecurity-2.0 ... Step 5 of 12: Downloading servlet-3.0 ... Step 6 of 12: Installing servlet-3.0 ... Step 7 of 12: Downloading jndi-1.0 ... Step 8 of 12: Installing jndi-1.0 ... Step 9 of 12: Downloading distributedMap-1.0 ... Step 10 of 12: Installing distributedMap-1.0 ... Step 11 of 12: Validating installed fixes ... Step 12 of 12: Cleaning up temporary files ... All assets were successfully installed. Start product validation... Product validation completed successfully. Removing intermediate container 5f5b05aec1ae ---> e1c6bfabda76 Successfully built e1c6bfabda76 Successfully tagged app:latest List the images to see that the new image app is built: docker images Note that the base image, ibmcom/websphere-liberty has also been pulled into the local registry. REPOSITORY TAG IMAGE ID CREATED SIZE app latest baa6bb9ad29d 2 minutes ago 544 MB ibmcom/websphere-liberty kernel-java8-ibmjava-ubi 7ea3d0a2b3fe 4 hours ago 544 MB Start the container. Note: You are running with both http and https ports: docker run -d -p 9080:9080 -p 9443:9443 --name=app-instance app Access the application running in the container: a. Open the Firefox Web Browswer and go to URL: http://localhost:9080/app b. Check that it renders a page showing Simple Servlet ran successfully . c. Also point your browser to 9443: https://localhost:9443/app List the running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 595cdc49c710 app \"/opt/ibm/helpers/ru\u2026\" 8 minutes ago Up 8 minutes 0.0.0.0:9080->9080/tcp, 0.0.0.0:9443->9443/tcp app-instance Access the logs to your container: docker logs -f app-instance Use Ctrl-C To exit. Remote shell into your running container to poke around: docker exec -it app-instance /bin/sh In the shell session, a. Run whoami and then run id , note you're not running as root. whoami id b. Note that this is a stripped down environment where many commands are not available. For example, try the following: which ps to see running processes. cd /logs to find the log files. cd /liberty/wlp to find the location of the liberty install cd /liberty/wlp/usr/servers/defaultServer to find the server configuration. cd /opt/ibm/wlp/output/defaultServer to find the workarea files required by the server runtime. Exit from the container: exit Cleanup: docker stop app-instance docker rm app-instance Managing Image Versions There is no built-in versioning for container images. However, you may use a tagging convention to version your images. The convention is to use major.minor.patch , such as 1.3.5 . The default tag if you don't specify one is latest . Let's assume that the first version we will build for our environment is 1.3.5. (The earlier versions are built in a different environment.) Run the commands to tag the latest app image for our first version: docker tag app app:1 docker tag app app:1.3 docker tag app app:1.3.5 List the images again: docker images And the output: REPOSITORY TAG IMAGE ID CREATED SIZE app 1 d98cbdf82a0d 21 hours ago 542MB app 1.3 d98cbdf82a0d 21 hours ago 542MB app 1.3.5 d98cbdf82a0d 21 hours ago 542MB app latest d98cbdf82a0d 21 hours ago 542MB Note that all the different tags are currently associated with the same image, as they have the same image ID. After tagging, the command docker run app:<version> ... or docker pull app:<version> ... resolves the available versions as follows: app:1 resolves to the latest 1.x.x version, which in this case is 1.3.5 . app:1.3 resolves to the latest 1.3.x version, which in this case is the 1.3.5 app:1.3.5 resolves to the exact version 1.3.5 . After you build a new patch image containing defect fixes, you want to manage the tags for the new image so that a new docker run app:<version> ... or docker pull app:<version> ... command resolves the images as follows: app:1.3.5 : resolves to the existing 1.3.5 image. app:1.3.6 : resolves to the new image app:1.3 : resolves to the new image. app:1 : resolves to the new image Let's simulate a defect fix by building a new image using Containerfile1 instead of Containerfile : docker build -t app -f Containerfile1 . Example output: Sending build context to Docker daemon 32.26kB Step 1/5 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> bb79b9e26fd3 Step 2/5 : COPY server.xml /config ---> Using cache ---> f10659bc62b2 Step 3/5 : COPY ServletApp.war /config/dropins/app.war ---> Using cache ---> 24d85579e404 Step 4/5 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Using cache ---> 5e924d776a9c Step 5/5 : RUN echo test1 > /config/test1 ---> Running in 08e6135b00f5 Removing intermediate container 08e6135b00f5 ---> 69d20332a5e0 Successfully built 69d20332a5e0 Successfully tagged app:latest Tag it as follows : docker tag app app:1 docker tag app app:1.3 docker tag app app:1.3.6 Verify that these are the same images: app:1 , app:1.3 , app:1.3.6 . A new minor version involves compatible changes beyond just bug fixes. After you build a new minor version image, you want to manage the tags such that: app:1.3.5 : resolves to the existing 1.3.5 image. app:1.3.6 : resolves to the existing 1.3.6 image app:1.4.0 : resolves to the new image app:1.3 : resolves to the existing 1.3.6 image. app:1.4 : resolves to the new image. app:1 : resolves to the new image Build a new image using Containerfile2 : docker build -t app -f Containerfile2 . The Output: Sending build context to Docker daemon 32.26kB Step 1/6 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> bb79b9e26fd3 Step 2/6 : COPY server.xml /config ---> Using cache ---> f10659bc62b2 Step 3/6 : COPY ServletApp.war /config/dropins/app.war ---> Using cache ---> 24d85579e404 Step 4/6 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Using cache ---> 5e924d776a9c Step 5/6 : RUN echo test1 > /config/test1 ---> Using cache ---> 69d20332a5e0 Step 6/6 : RUN echo test2 > /config/test2 ---> Running in 96ea24b9ba66 Removing intermediate container 96ea24b9ba66 ---> 31b27169b3bc Successfully built 31b27169b3bc Tag it as follows: docker tag app app:1 docker tag app app:1.4 docker tag app app:1.4.0 Verify the following: 1 , 1.4 , and 1.4.0 are the same image 1.3 and 1.3.6 are the same image Congratulations! You have completed the Introduction to Containerization lab. Next Please follow the link to do the next lab Introduction to Container Orchestration using Openshift : Introduction to Container Orchestration using Openshift","title":"Introduction to Containerization"},{"location":"liberty-labs/HelloContainer/#introduction-to-containerization","text":"","title":"Introduction to Containerization"},{"location":"liberty-labs/HelloContainer/#table-of-contents","text":"Introduction to Containerization Table of Contents Background Prerequisites What is a Container Login to the VM Check your environment Run a pre-built image Build and Run Your Own Image Managing Image Versions Extra Credit Next","title":"Table of Contents"},{"location":"liberty-labs/HelloContainer/#background","text":"If you are expecting a lab about docker , you are at the right place. This lab will introduce you to the basic concepts of containerization, including: What are containers and container images How to start, stop, and remove containers. How to create container images How to version container images","title":"Background"},{"location":"liberty-labs/HelloContainer/#prerequisites","text":"You have podman or docker installed. Only docker is installed for this lab. You have access to the internet. You have cloned this lab from github. If not, follows these steps: git clone https://github.com/IBM/openshift-workshop-was.git cd openshift-workshop-was/labs/Openshift/HelloContainer","title":"Prerequisites"},{"location":"liberty-labs/HelloContainer/#what-is-a-container","text":"Compared to virtual machines, containers supports virtualization at the process level. Think of them as virtual processes. The isolation abstraction provided by the operating system makes the process think that it's running in its own virtual machine. As processes, containers may be created, started, and stopped much more quickly than virtual machines. Everything you need to run your application, from the operating system and up, is stored in a special file called a container image. Container images are self contained and portable. You may run one or more instances anywhere. And you don't have to worry about missing prerequisites, because all prerequisites are stored in the image. Container images are created via tools such as docker or podman . Existing images are hosted in container registries. For example, docker hub, or registry.access.redhat.com, or your own internal registry. If you need more background on containers: https://www.docker.com/resources/what-container","title":"What is a Container"},{"location":"liberty-labs/HelloContainer/#login-to-the-vm","text":"If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password. Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window.","title":"Login to the VM"},{"location":"liberty-labs/HelloContainer/#check-your-environment","text":"Open a terminal window from within your VM. List version of docker: docker --version Example output: Docker version 20.10.7, build f0df350 For more background on docker command line: https://docs.docker.com/engine/reference/commandline/cli/","title":"Check your environment"},{"location":"liberty-labs/HelloContainer/#run-a-pre-built-image","text":"Container images must be available locally before they can be run. To list available local images: docker images Note: no container inages are listed at this point REPOSITORY TAG IMAGE ID CREATED SIZE Images are hosted in container registries. The default container registry for docker is docker hub, located at https://hub.docker.com. Let's pull a test image from docker hub: docker pull openshift/hello-openshift And the output: Using default tag: latest latest: Pulling from openshift/hello-openshift 4f4fb700ef54: Pull complete 8b32988996c5: Pull complete Digest: sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e Status: Downloaded newer image for openshift/hello-openshift:latest docker.io/openshift/hello-openshift:latest List available local images again: docker images The hello-openshift image is now listed REPOSITORY TAG IMAGE ID CREATED SIZE openshift/hello-openshift latest 7af3297a3fb4 2 years ago 6.09MB Inspect the image metadata: docker inspect openshift/hello-openshift Note : It exposes two ports: 8080 and 8888 It runs as user 1001 The entry point executable is /hello-openshift [ { \"Id\": \"sha256:7af3297a3fb4487b740ed6798163f618e6eddea1ee5fa0ba340329fcae31c8f6\", \"RepoTags\": [ \"openshift/hello-openshift:latest\" ], \"RepoDigests\": [ \"openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e\" ], ... \"Config\": { \"User\": \"1001\", ... \"ExposedPorts\": { \"8080/tcp\": {}, \"8888/tcp\": {} }, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], ... \"Entrypoint\": [ \"/hello-openshift\" ] ... } ] Run the image in an container: docker run --name hello1 -d -p 8080:8080 -p 8888:8888 openshift/hello-openshift Note that: The --name option gives the container a name. The -d option runs the command in the background as a daemon The -p option maps the port on the host to the port in the container. Through virtual networking, the port within the container is always the same for all running instances. But to support multiple concurrent running instances, the actual port on the host must be different for each instance. When you start the container, you may assign a new port on the host dynamically. The output of the command is the container ID for the running container. If the container starts successfully, the executable specified by the Entrypoint in the metadata is run. For our sample, it is /hello-openshift . Access the application in the container. a. Open the Firefox Web Browser from inside of the VM. b. Go to the URL http://localhost:8080 c. Also try port 8888 Run another instance of the same image. Note that this new instance is assigned new port numbers 8081 and 8889 on the host. This is so that they don't conflict with the ports 8080 and 8888 already allocated to the first instance. docker run --name hello2 -d -p 8081:8080 -p 8889:8888 openshift/hello-openshift Question: How does this compare to the time it takes to start a new virtual machine? Access the application in the new container the same way. a. Return to the Firefox Web Browser but instead go to the URL http://localhost:8081 b. Also try port 8889 Verify there are two containers running in the same host: docker ps : CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5a62f8527b44 openshift/hello-openshift \"/hello-openshift\" About a minute ago Up About a minute 0.0.0.0:8081->8080/tcp, 0.0.0.0:8889->8888/tcp hello2 c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 4 minutes ago Up 4 minutes 0.0.0.0:8080->8080/tcp, 0.0.0.0:8888->8888/tcp hello1 View the logs: docker logs hello1 And the output: serving on 8888 serving on 8080 View the logs on the second container: docker logs hello2 And the output: serving on 8888 serving on 8080 Note: within the container, each instance behaves as if it's running in its own virtual environment, and has opened the same ports. Outside of the container, different ports are opened. To export the file system of a running container: docker export hello1 > hello1.tar List the files on the file system: tar -tvf hello1.tar Note that this is a very small image. -rwxr-xr-x 0/0 0 2020-04-29 16:48 .dockerenv drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/ -rwxr-xr-x 0/0 0 2020-04-29 16:48 dev/console drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/pts/ drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/shm/ drwxr-xr-x 0/0 0 2020-04-29 16:48 etc/ -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/hostname -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/hosts lrwxrwxrwx 0/0 0 2020-04-29 16:48 etc/mtab -> /proc/mounts -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/resolv.conf -rwxr-xr-x 0/0 6089990 2018-04-18 10:22 hello-openshift drwxr-xr-x 0/0 0 2020-04-29 16:48 proc/ drwxr-xr-x 0/0 0 2020-04-29 16:48 sys/ Run another command in the running container. You can reach into the running container to run another command. The typical use case is to run a shell command, so you can use the shell to navigate within the container and run other commands. However, our image is tiny, and there is no built-in shell. For the purpose of this lab, we'll execute the same command again: docker exec -ti hello1 /hello-openshift`. Running this command again in the same container results in an error, because there is already another copy running in the background that is bound to the ports 8080 and 8888: serving on 8888 serving on 8080 panic: ListenAndServe: listen tcp :8888: bind: address already in use ... Stop the containers: docker stop hello1 docker stop hello2 List running containers: docker ps There should not be any running containers listed CONTAINER ID IMAGE COMMAND CREATED List all containers, including stopped containers: docker ps -a You should see the two cntainers listed, although not running; STATUS is Exited CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5a62f8527b44 openshift/hello-openshift \"/hello-openshift\" 28 minutes ago Exited (2) 28 seconds ago hello2 c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 31 minutes ago Exited (2) 32 seconds ago hello1 Restart a stopped container: docker restart hello1 List running containers: docker ps The hello-openshift container is now running again. CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 33 minutes ago Up 8 seconds 0.0.0.0:8080->8080/tcp, 0.0.0.0:8888->8888/tcp hello1 Stop the container: docker stop hello1 Remove stopped containers, and note that there are no more containers: docker rm hello1 docker rm hello2 docker ps -a Remove the image from local cache: a. View current images: docker images Example output: REPOSITORY TAG IMAGE ID CREATED SIZE openshift/hello-openshift latest 7af3297a3fb4 2 years ago 6.09MB b. Remove the image: docker rmi openshift/hello-openshift Example output: Untagged: openshift/hello-openshift:latest Untagged: openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e Deleted: sha256:7af3297a3fb4487b740ed6798163f618e6eddea1ee5fa0ba340329fcae31c8f6 Deleted: sha256:8fd6a1ece3ceceae6d714004614bae5b581c83ab962d838ef88ce760583dcb80 Deleted: sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef c. Check that the image has been removed: docker images Example output: REPOSITORY TAG IMAGE ID CREATED SIZE","title":"Run a pre-built image"},{"location":"liberty-labs/HelloContainer/#build-and-run-your-own-image","text":"We use a Containerfile , which contains the instructions to create the new layers of your image. For those familiar with docker, the Containerfile is equivalent to Dockerfile . Recall that an image contains the entire file system that you want to use to run your virtual process in a container. For this sample, we are building a new image for a Java EE web application ServletApp.war . It is configured to run on the WebSphere Liberty Runtime. The configuration file for the server is in the server.xml . Change directory to openshift-workshop-was/labs/Openshift/HelloContainer cd openshift-workshop-was/labs/Openshift/HelloContainer Review the provided Containerfile from the directory: cat Containerfile Content of Containerfile : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi COPY server.xml /config COPY ServletApp.war /config/dropins/app.war RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml To create a new image, you start with a pre-existing image. The first line FROM specifies the existing image to be used as the base. If this is not in the local registry, it will be pulled from a remote registry such as docker hub. The base image we are using, ibmcom/websphere-liberty , is already prepackaged for us and made available on docker hub. The second line COPY is a straight copy of the file server.xml from the local directory to /config/server.xml in the image. This adds a new layer to the image with the actual server configuration to be used. The third line, another COPY , copies ServletApp.war from the current directory into a new layer in the image you are creating, at the location /config/dropins/app.war . The last line RUN runs the installUtility command within the image to install additional features required to run the server as specified in server.xml . You can use the RUN command to run any command that is available within the image to customize the image itself. Run the build. Ensure you include . at the end of the command (the dot indicates using the file from the current directory): docker build -t app -f Containerfile . The -t option tags the name of the image as app . The -f option specifies the name of the Containerfile . The build command runs the commands in Containerfile to build a new image called app . Example output: Sending build context to Docker daemon 25.6kB Step 1/4 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi kernel-java8-ibmjava-ubi: Pulling from ibmcom/websphere-liberty ee2244abc66f: Pull complete befb03b11956: Pull complete 137dc88f6a93: Pull complete 5bdd69a33184: Pull complete d4e2554981d7: Pull complete 32c91bc0f2e1: Pull complete db7e931336a9: Pull complete 3b32f9956ae2: Pull complete 304584ffa0a2: Pull complete 9f6da4c82b7e: Pull complete b6fa5b2e2325: Pull complete Digest: sha256:d76f79695afe2f653fc7b272f9a629105446e6b78ff0d733d494c93ff05728e7 Status: Downloaded newer image for ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> 4d9265befb26 Step 2/4 : COPY server.xml /config ---> 4a02d03d3725 Step 3/4 : COPY ServletApp.war /config/dropins/app.war ---> b2def2a0feac Step 4/4 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Running in 5f5b05aec1ae Checking for missing features required by the server ... The server requires the following additional features: appsecurity-2.0 servlet-3.0. Installing features from the repository ... Establishing a connection to the configured repositories ... This process might take several minutes to complete. Successfully connected to all configured repositories. Preparing assets for installation. This process might take several minutes to complete. Additional Liberty features must be installed for this server. To install the additional features, review and accept the feature license agreement: The --acceptLicense argument was found. This indicates that you have accepted the terms of the license agreement. Step 1 of 12: Downloading ssl-1.0 ... Step 2 of 12: Installing ssl-1.0 ... Step 3 of 12: Downloading appSecurity-2.0 ... Step 4 of 12: Installing appSecurity-2.0 ... Step 5 of 12: Downloading servlet-3.0 ... Step 6 of 12: Installing servlet-3.0 ... Step 7 of 12: Downloading jndi-1.0 ... Step 8 of 12: Installing jndi-1.0 ... Step 9 of 12: Downloading distributedMap-1.0 ... Step 10 of 12: Installing distributedMap-1.0 ... Step 11 of 12: Validating installed fixes ... Step 12 of 12: Cleaning up temporary files ... All assets were successfully installed. Start product validation... Product validation completed successfully. Removing intermediate container 5f5b05aec1ae ---> e1c6bfabda76 Successfully built e1c6bfabda76 Successfully tagged app:latest List the images to see that the new image app is built: docker images Note that the base image, ibmcom/websphere-liberty has also been pulled into the local registry. REPOSITORY TAG IMAGE ID CREATED SIZE app latest baa6bb9ad29d 2 minutes ago 544 MB ibmcom/websphere-liberty kernel-java8-ibmjava-ubi 7ea3d0a2b3fe 4 hours ago 544 MB Start the container. Note: You are running with both http and https ports: docker run -d -p 9080:9080 -p 9443:9443 --name=app-instance app Access the application running in the container: a. Open the Firefox Web Browswer and go to URL: http://localhost:9080/app b. Check that it renders a page showing Simple Servlet ran successfully . c. Also point your browser to 9443: https://localhost:9443/app List the running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 595cdc49c710 app \"/opt/ibm/helpers/ru\u2026\" 8 minutes ago Up 8 minutes 0.0.0.0:9080->9080/tcp, 0.0.0.0:9443->9443/tcp app-instance Access the logs to your container: docker logs -f app-instance Use Ctrl-C To exit. Remote shell into your running container to poke around: docker exec -it app-instance /bin/sh In the shell session, a. Run whoami and then run id , note you're not running as root. whoami id b. Note that this is a stripped down environment where many commands are not available. For example, try the following: which ps to see running processes. cd /logs to find the log files. cd /liberty/wlp to find the location of the liberty install cd /liberty/wlp/usr/servers/defaultServer to find the server configuration. cd /opt/ibm/wlp/output/defaultServer to find the workarea files required by the server runtime. Exit from the container: exit Cleanup: docker stop app-instance docker rm app-instance","title":"Build and Run Your Own Image"},{"location":"liberty-labs/HelloContainer/#managing-image-versions","text":"There is no built-in versioning for container images. However, you may use a tagging convention to version your images. The convention is to use major.minor.patch , such as 1.3.5 . The default tag if you don't specify one is latest . Let's assume that the first version we will build for our environment is 1.3.5. (The earlier versions are built in a different environment.) Run the commands to tag the latest app image for our first version: docker tag app app:1 docker tag app app:1.3 docker tag app app:1.3.5 List the images again: docker images And the output: REPOSITORY TAG IMAGE ID CREATED SIZE app 1 d98cbdf82a0d 21 hours ago 542MB app 1.3 d98cbdf82a0d 21 hours ago 542MB app 1.3.5 d98cbdf82a0d 21 hours ago 542MB app latest d98cbdf82a0d 21 hours ago 542MB Note that all the different tags are currently associated with the same image, as they have the same image ID. After tagging, the command docker run app:<version> ... or docker pull app:<version> ... resolves the available versions as follows: app:1 resolves to the latest 1.x.x version, which in this case is 1.3.5 . app:1.3 resolves to the latest 1.3.x version, which in this case is the 1.3.5 app:1.3.5 resolves to the exact version 1.3.5 . After you build a new patch image containing defect fixes, you want to manage the tags for the new image so that a new docker run app:<version> ... or docker pull app:<version> ... command resolves the images as follows: app:1.3.5 : resolves to the existing 1.3.5 image. app:1.3.6 : resolves to the new image app:1.3 : resolves to the new image. app:1 : resolves to the new image Let's simulate a defect fix by building a new image using Containerfile1 instead of Containerfile : docker build -t app -f Containerfile1 . Example output: Sending build context to Docker daemon 32.26kB Step 1/5 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> bb79b9e26fd3 Step 2/5 : COPY server.xml /config ---> Using cache ---> f10659bc62b2 Step 3/5 : COPY ServletApp.war /config/dropins/app.war ---> Using cache ---> 24d85579e404 Step 4/5 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Using cache ---> 5e924d776a9c Step 5/5 : RUN echo test1 > /config/test1 ---> Running in 08e6135b00f5 Removing intermediate container 08e6135b00f5 ---> 69d20332a5e0 Successfully built 69d20332a5e0 Successfully tagged app:latest Tag it as follows : docker tag app app:1 docker tag app app:1.3 docker tag app app:1.3.6 Verify that these are the same images: app:1 , app:1.3 , app:1.3.6 . A new minor version involves compatible changes beyond just bug fixes. After you build a new minor version image, you want to manage the tags such that: app:1.3.5 : resolves to the existing 1.3.5 image. app:1.3.6 : resolves to the existing 1.3.6 image app:1.4.0 : resolves to the new image app:1.3 : resolves to the existing 1.3.6 image. app:1.4 : resolves to the new image. app:1 : resolves to the new image Build a new image using Containerfile2 : docker build -t app -f Containerfile2 . The Output: Sending build context to Docker daemon 32.26kB Step 1/6 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> bb79b9e26fd3 Step 2/6 : COPY server.xml /config ---> Using cache ---> f10659bc62b2 Step 3/6 : COPY ServletApp.war /config/dropins/app.war ---> Using cache ---> 24d85579e404 Step 4/6 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Using cache ---> 5e924d776a9c Step 5/6 : RUN echo test1 > /config/test1 ---> Using cache ---> 69d20332a5e0 Step 6/6 : RUN echo test2 > /config/test2 ---> Running in 96ea24b9ba66 Removing intermediate container 96ea24b9ba66 ---> 31b27169b3bc Successfully built 31b27169b3bc Tag it as follows: docker tag app app:1 docker tag app app:1.4 docker tag app app:1.4.0 Verify the following: 1 , 1.4 , and 1.4.0 are the same image 1.3 and 1.3.6 are the same image Congratulations! You have completed the Introduction to Containerization lab.","title":"Managing Image Versions"},{"location":"liberty-labs/HelloContainer/#next","text":"Please follow the link to do the next lab Introduction to Container Orchestration using Openshift : Introduction to Container Orchestration using Openshift","title":"Next"},{"location":"liberty-labs/HelloContainer/README - Copy/","text":"Introduction to Containerization Table of Contents Introduction to Containerization Table of Contents Background Prerequisites What is a Container Login to the VM Check your environment Run a pre-built image Build and Run Your Own Image Managing Image Versions Extra Credit Next Background If you are expecting a lab about docker , you are at the right place. This lab will introduce you to the basic concepts of containerization, including: What are containers and container images How to start, stop, and remove containers. How to create container images How to version container images Prerequisites You have podman or docker installed. Only docker is installed for this lab. You have access to the internet. You have cloned this lab from github. If not, follows these steps: git clone https://github.com/IBM/openshift-workshop-was.git cd openshift-workshop-was/labs/Openshift/HelloContainer What is a Container Compared to virtual machines, containers supports virtualization at the process level. Think of them as virtual processes. The isolation abstraction provided by the operating system makes the process think that it's running in its own virtual machine. As processes, containers may be created, started, and stopped much more quickly than virtual machines. Everything you need to run your application, from the operating system and up, is stored in a special file called a container image. Container images are self contained and portable. You may run one or more instances anywhere. And you don't have to worry about missing prerequisites, because all prerequisites are stored in the image. Container images are created via tools such as docker or podman . Existing images are hosted in container registries. For example, docker hub, or registry.access.redhat.com, or your own internal registry. If you need more background on containers: https://www.docker.com/resources/what-container Login to the VM If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password: \\ Password: engageibm Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window. Check your environment Open a terminal window from within your VM. List version of docker: docker --version Example output: Docker version 20.10.7, build f0df350 For more background on docker command line: https://docs.docker.com/engine/reference/commandline/cli/ Run a pre-built image Container images must be available locally before they can be run. To list available local images: docker images REPOSITORY TAG IMAGE ID CREATED SIZE Images are hosted in container registries. The default container registry for docker is docker hub, located at https://hub.docker.com. Let's pull a test image from docker hub: docker pull openshift/hello-openshift And the output: Using default tag: latest latest: Pulling from openshift/hello-openshift 4f4fb700ef54: Pull complete 8b32988996c5: Pull complete Digest: sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e Status: Downloaded newer image for openshift/hello-openshift:latest docker.io/openshift/hello-openshift:latest List available local images again: docker images REPOSITORY TAG IMAGE ID CREATED SIZE openshift/hello-openshift latest 7af3297a3fb4 2 years ago 6.09MB Inspect the image metadata: docker inspect openshift/hello-openshift Note that: It exposes two ports: 8080 and 8888 It runs as user 1001 The entry point executable is /hello-openshift [ { \"Id\": \"sha256:7af3297a3fb4487b740ed6798163f618e6eddea1ee5fa0ba340329fcae31c8f6\", \"RepoTags\": [ \"openshift/hello-openshift:latest\" ], \"RepoDigests\": [ \"openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e\" ], ... \"Config\": { \"User\": \"1001\", ... \"ExposedPorts\": { \"8080/tcp\": {}, \"8888/tcp\": {} }, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], ... \"Entrypoint\": [ \"/hello-openshift\" ] ... } ] Run the image in an container: docker run --name hello1 -d -p 8080:8080 -p 8888:8888 openshift/hello-openshift Note that: The --name option gives the container a name. The -d option runs the command in the background as a daemon The -p option maps the port on the host to the port in the container. Through virtual networking, the port within the container is always the same for all running instances. But to support multiple concurrent running instances, the actual port on the host must be different for each instance. When you start the container, you may assign a new port on the host dynamically. The output of the command is the container ID for the running container. If the container starts successfully, the executable specified by the Entrypoint in the metadata is run. For our sample, it is /hello-openshift . Access the application in the container. Open the Firefox Web Browser from inside of the VM. Go to the URL http://localhost:8080 Also try port 8888 Run another instance of the same image. Note that this new instance is assigned new port numbers 8081 and 8889 on the host. This is so that they don't conflict with the ports 8080 and 8888 already allocated to the first instance. docker run --name hello2 -d -p 8081:8080 -p 8889:8888 openshift/hello-openshift Question: how does this compare to the time it takes to start a new virtual machine? Access the application in the new container the same way. Return to the Firefox Web Browser but instead go to the URL http://localhost:8081 Also try port 8889 Verify there are two containers running in the same host: docker ps : CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5a62f8527b44 openshift/hello-openshift \"/hello-openshift\" About a minute ago Up About a minute 0.0.0.0:8081->8080/tcp, 0.0.0.0:8889->8888/tcp hello2 c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 4 minutes ago Up 4 minutes 0.0.0.0:8080->8080/tcp, 0.0.0.0:8888->8888/tcp hello1 View the logs: docker logs hello1 And the output: ``` serving on 8888 serving on 8080 ``` View the logs on the second container: docker logs hello2 And the output: serving on 8888 serving on 8080 Note: within the container, each instance behaves as if it's running in its own virtual environment, and has opened the same ports. Outside of the container, different ports are opened. To export the file system of a running container: docker export hello1 > hello1.tar List the files on the file system: tar -tvf hello1.tar Note that this is a very small image. -rwxr-xr-x 0/0 0 2020-04-29 16:48 .dockerenv drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/ -rwxr-xr-x 0/0 0 2020-04-29 16:48 dev/console drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/pts/ drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/shm/ drwxr-xr-x 0/0 0 2020-04-29 16:48 etc/ -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/hostname -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/hosts lrwxrwxrwx 0/0 0 2020-04-29 16:48 etc/mtab -> /proc/mounts -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/resolv.conf -rwxr-xr-x 0/0 6089990 2018-04-18 10:22 hello-openshift drwxr-xr-x 0/0 0 2020-04-29 16:48 proc/ drwxr-xr-x 0/0 0 2020-04-29 16:48 sys/ Run another command in the running container. You can reach into the running container to run another command. The typical use case is to run a shell command, so you can use the shell to navigate within the container and run other commands. However, our image is tiny, and there is no built-in shell. For the purpose of this lab, we'll execute the same command again: docker exec -ti hello1 /hello-openshift . Running this command again in the same container results in an error, because there is already another copy running in the background that is bound to the ports 8080 and 8888: serving on 8888 serving on 8080 panic: ListenAndServe: listen tcp :8888: bind: address already in use ... Stop the containers: docker stop hello1 docker stop hello2 List running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES List all containers, including stopped ones: docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5a62f8527b44 openshift/hello-openshift \"/hello-openshift\" 28 minutes ago Exited (2) 28 seconds ago hello2 c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 31 minutes ago Exited (2) 32 seconds ago hello1 Restart a stopped container: docker restart hello1 List running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 33 minutes ago Up 8 seconds 0.0.0.0:8080->8080/tcp, 0.0.0.0:8888->8888/tcp hello1 Stop the container: docker stop hello1 Remove stopped containers, and note that there are no more containers: docker rm hello1 docker rm hello2 docker ps -a Remove the image from local cache: View current images: docker images Example output: REPOSITORY TAG IMAGE ID CREATED SIZE openshift/hello-openshift latest 7af3297a3fb4 2 years ago 6.09MB Remove the image: docker rmi openshift/hello-openshift Example output: Untagged: openshift/hello-openshift:latest Untagged: openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e Deleted: sha256:7af3297a3fb4487b740ed6798163f618e6eddea1ee5fa0ba340329fcae31c8f6 Deleted: sha256:8fd6a1ece3ceceae6d714004614bae5b581c83ab962d838ef88ce760583dcb80 Deleted: sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef Check that the image has been removed: docker images Example output: REPOSITORY TAG IMAGE ID CREATED SIZE Build and Run Your Own Image We use a Containerfile , which contains the instructions to create the new layers of your image. For those familiar with docker, the Containerfile is equivalent to Dockerfile . Recall that an image contains the entire file system that you want to use to run your virtual process in a container. For this sample, we are building a new image for a Java EE web application ServletApp.war. It is configured to run on the WebSphere Liberty Runtime. The configuration file for the server is in the server.xml. Change directory to openshift-workshop-was/labs/Openshift/HelloContainer cd openshift-workshop-was/labs/Openshift/HelloContainer Review the provided Containerfile from the directory: cat Containerfile Content of Containerfile : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi COPY server.xml /config COPY ServletApp.war /config/dropins/app.war RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml To create a new image, you start with a pre-existing image. The first line FROM specifies the existing image to be used as the base. If this is not in the local registry, it will be pulled from a remote registry such as docker hub. The base image we are using, ibmcom/websphere-liberty , is already prepackaged for us and made available on docker hub. The second line COPY is a straight copy of the file server.xml from the local directory to /config/server.xml in the image. This adds a new layer to the image with the actual server configuration to be used. The third line, another COPY , copies ServletApp.war from the current directory into a new layer in the image you are creating, at the location /config/dropins/app.war . The last line RUN runs the installUtility command within the image to install additional features required to run the server as specified in server.xml . You can use the RUN command to run any command that is available within the image to customize the image itself. Run the build. Ensure you include . at the end of the command (the dot indicates using the file from the current directory): docker build -t app -f Containerfile . The -t option tags the name of the image as app . The -f option specifies the name of the Containerfile . The build command runs the commands in Containerfile to build a new image called app . Example output: Sending build context to Docker daemon 25.6kB Step 1/4 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi kernel-java8-ibmjava-ubi: Pulling from ibmcom/websphere-liberty ee2244abc66f: Pull complete befb03b11956: Pull complete 137dc88f6a93: Pull complete 5bdd69a33184: Pull complete d4e2554981d7: Pull complete 32c91bc0f2e1: Pull complete db7e931336a9: Pull complete 3b32f9956ae2: Pull complete 304584ffa0a2: Pull complete 9f6da4c82b7e: Pull complete b6fa5b2e2325: Pull complete Digest: sha256:d76f79695afe2f653fc7b272f9a629105446e6b78ff0d733d494c93ff05728e7 Status: Downloaded newer image for ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> 4d9265befb26 Step 2/4 : COPY server.xml /config ---> 4a02d03d3725 Step 3/4 : COPY ServletApp.war /config/dropins/app.war ---> b2def2a0feac Step 4/4 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Running in 5f5b05aec1ae Checking for missing features required by the server ... The server requires the following additional features: appsecurity-2.0 servlet-3.0. Installing features from the repository ... Establishing a connection to the configured repositories ... This process might take several minutes to complete. Successfully connected to all configured repositories. Preparing assets for installation. This process might take several minutes to complete. Additional Liberty features must be installed for this server. To install the additional features, review and accept the feature license agreement: The --acceptLicense argument was found. This indicates that you have accepted the terms of the license agreement. Step 1 of 12: Downloading ssl-1.0 ... Step 2 of 12: Installing ssl-1.0 ... Step 3 of 12: Downloading appSecurity-2.0 ... Step 4 of 12: Installing appSecurity-2.0 ... Step 5 of 12: Downloading servlet-3.0 ... Step 6 of 12: Installing servlet-3.0 ... Step 7 of 12: Downloading jndi-1.0 ... Step 8 of 12: Installing jndi-1.0 ... Step 9 of 12: Downloading distributedMap-1.0 ... Step 10 of 12: Installing distributedMap-1.0 ... Step 11 of 12: Validating installed fixes ... Step 12 of 12: Cleaning up temporary files ... All assets were successfully installed. Start product validation... Product validation completed successfully. Removing intermediate container 5f5b05aec1ae ---> e1c6bfabda76 Successfully built e1c6bfabda76 Successfully tagged app:latest List the images to see that the new image app is built: docker images Note that the base image, ibmcom/websphere-liberty has also been pulled into the local registry. REPOSITORY TAG IMAGE ID CREATED SIZE app latest baa6bb9ad29d 2 minutes ago 544 MB ibmcom/websphere-liberty kernel-java8-ibmjava-ubi 7ea3d0a2b3fe 4 hours ago 544 MB Start the container. Note that you are running with both http and https ports: docker run -d -p 9080:9080 -p 9443:9443 --name=app-instance app Access the application running in the container: Open the Firefox Web Browswer and go to URL http://localhost:9080/app Check that it renders a page showing Simple Servlet ran successfully . Also point your browser to 9443: https://localhost:9443/app List the running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 595cdc49c710 app \"/opt/ibm/helpers/ru\u2026\" 8 minutes ago Up 8 minutes 0.0.0.0:9080->9080/tcp, 0.0.0.0:9443->9443/tcp app-instance Access the logs to your container: docker logs -f app-instance Use Ctrl-C To exit. Remote shell into your running container to poke around: docker exec -it app-instance /bin/sh In the shell session, run whoami and then run id , note you're not running as root. Note that this is a stripped down environment where many commands are not available. For example, try which ps . cd /logs to find the log files cd /liberty/wlp to find the location of the liberty install cd /liberty/wlp/usr/servers/defaultServer to find the server configuration. cd /opt/ibm/wlp/output/defaultServer to find the workarea files required by the server runtime. Exit from the container: exit Cleanup: docker stop app-instance docker rm app-instance Managing Image Versions There is no built-in versioning for container images. However, you may use a tagging convention to version your images. The convention is to use major.minor.patch , such as 1.3.5 . The default tag if you don't specify one is latest . Let's assume that the first version we will build for our environment is 1.3.5. (The earlier versions are built in a different environment.) Run the commands to tag the latest app image for our first version: docker tag app app:1 docker tag app app:1.3 docker tag app app:1.3.5 List the images again: docker images And the output: REPOSITORY TAG IMAGE ID CREATED SIZE app 1 d98cbdf82a0d 21 hours ago 542MB app 1.3 d98cbdf82a0d 21 hours ago 542MB app 1.3.5 d98cbdf82a0d 21 hours ago 542MB app latest d98cbdf82a0d 21 hours ago 542MB Note that all the different tags are currently associated with the same image, as they have the same image ID. After tagging, the command docker run app:<version> ... or docker pull app:<version> ... resolves the available versions as follows: app:1 resolves to the latest 1.x.x version, which in this case is 1.3.5 . app:1.3 resolves to the latest 1.3.x version, which in this case is the 1.3.5 app:1.3.5 resolves to the exact version 1.3.5 . After you build a new patch image containing defect fixes, you want to manage the tags for the new image so that a new docker run app:<version> ... or docker pull app:<version> ... command resolves the images as follows: app:1.3.5 : resolves to the existing 1.3.5 image. app:1.3.6 : resolves to the new image app:1.3 : resolves to the new image. app:1 : resolves to the new image Let's simulate a defect fix by building a new image using Containerfile1 instead of Containerfile : docker build -t app -f Containerfile1 . Example output: Sending build context to Docker daemon 32.26kB Step 1/5 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> bb79b9e26fd3 Step 2/5 : COPY server.xml /config ---> Using cache ---> f10659bc62b2 Step 3/5 : COPY ServletApp.war /config/dropins/app.war ---> Using cache ---> 24d85579e404 Step 4/5 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Using cache ---> 5e924d776a9c Step 5/5 : RUN echo test1 > /config/test1 ---> Running in 08e6135b00f5 Removing intermediate container 08e6135b00f5 ---> 69d20332a5e0 Successfully built 69d20332a5e0 Successfully tagged app:latest Tag it as follows : docker tag app app:1 docker tag app app:1.3 docker tag app app:1.3.6 Verify that these are the same images: app:1 , app:1.3 , app:1.3.6 . A new minor version involves compatible changes beyond just bug fixes. After you build a new minor version image, you want to manage the tags such that: app:1.3.5 : resolves to the existing 1.3.5 image. app:1.3.6 : resolves to the existing 1.3.6 image app:1.4.0 : resolves to the new image app:1.3 : resolves to the existing 1.3.6 image. app:1.4 : resolves to the new image. app:1 : resolves to the new image Build a new image using Containerfile2 : docker build -t app -f Containerfile2 . Sending build context to Docker daemon 32.26kB Step 1/6 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> bb79b9e26fd3 Step 2/6 : COPY server.xml /config ---> Using cache ---> f10659bc62b2 Step 3/6 : COPY ServletApp.war /config/dropins/app.war ---> Using cache ---> 24d85579e404 Step 4/6 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Using cache ---> 5e924d776a9c Step 5/6 : RUN echo test1 > /config/test1 ---> Using cache ---> 69d20332a5e0 Step 6/6 : RUN echo test2 > /config/test2 ---> Running in 96ea24b9ba66 Removing intermediate container 96ea24b9ba66 ---> 31b27169b3bc Successfully built 31b27169b3bc Tag it as follows: docker tag app app:1 docker tag app app:1.4 docker tag app app:1.4.0 Verify that 1 , 1.4 , and 1.4.0 are the same image 1.3 and 1.3.6 are the same image Extra Credit Search the internet for information about multi-stage build. In a single stage build, the final image contains both build and runtime artifacts. A multi-stage build allows you to build with one base image, and copy the result of the build to another base image. This gives you even more control over the output of the build, and the size of the final image. Start another instances of the app image for vertical scaling, but with different port numbers on the host. Point your browser to hub.docker.com , click \"Explore\" and explore the millions of available images. Think about how you would tag a new image at a major version, 2.0.0 . Think about what would be required to manage containers across multiple machines to support horizontal scaling. Congratulations! You have completed the Introduction to Containerization lab. Next Please follow the link to do the next lab Introduction to Container Orchestration using Openshift : - Introduction to Container Orchestration using Openshift","title":"Introduction to Containerization"},{"location":"liberty-labs/HelloContainer/README - Copy/#introduction-to-containerization","text":"","title":"Introduction to Containerization"},{"location":"liberty-labs/HelloContainer/README - Copy/#table-of-contents","text":"Introduction to Containerization Table of Contents Background Prerequisites What is a Container Login to the VM Check your environment Run a pre-built image Build and Run Your Own Image Managing Image Versions Extra Credit Next","title":"Table of Contents"},{"location":"liberty-labs/HelloContainer/README - Copy/#background","text":"If you are expecting a lab about docker , you are at the right place. This lab will introduce you to the basic concepts of containerization, including: What are containers and container images How to start, stop, and remove containers. How to create container images How to version container images","title":"Background"},{"location":"liberty-labs/HelloContainer/README - Copy/#prerequisites","text":"You have podman or docker installed. Only docker is installed for this lab. You have access to the internet. You have cloned this lab from github. If not, follows these steps: git clone https://github.com/IBM/openshift-workshop-was.git cd openshift-workshop-was/labs/Openshift/HelloContainer","title":"Prerequisites"},{"location":"liberty-labs/HelloContainer/README - Copy/#what-is-a-container","text":"Compared to virtual machines, containers supports virtualization at the process level. Think of them as virtual processes. The isolation abstraction provided by the operating system makes the process think that it's running in its own virtual machine. As processes, containers may be created, started, and stopped much more quickly than virtual machines. Everything you need to run your application, from the operating system and up, is stored in a special file called a container image. Container images are self contained and portable. You may run one or more instances anywhere. And you don't have to worry about missing prerequisites, because all prerequisites are stored in the image. Container images are created via tools such as docker or podman . Existing images are hosted in container registries. For example, docker hub, or registry.access.redhat.com, or your own internal registry. If you need more background on containers: https://www.docker.com/resources/what-container","title":"What is a Container"},{"location":"liberty-labs/HelloContainer/README - Copy/#login-to-the-vm","text":"If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password: \\ Password: engageibm Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window.","title":"Login to the VM"},{"location":"liberty-labs/HelloContainer/README - Copy/#check-your-environment","text":"Open a terminal window from within your VM. List version of docker: docker --version Example output: Docker version 20.10.7, build f0df350 For more background on docker command line: https://docs.docker.com/engine/reference/commandline/cli/","title":"Check your environment"},{"location":"liberty-labs/HelloContainer/README - Copy/#run-a-pre-built-image","text":"Container images must be available locally before they can be run. To list available local images: docker images REPOSITORY TAG IMAGE ID CREATED SIZE Images are hosted in container registries. The default container registry for docker is docker hub, located at https://hub.docker.com. Let's pull a test image from docker hub: docker pull openshift/hello-openshift And the output: Using default tag: latest latest: Pulling from openshift/hello-openshift 4f4fb700ef54: Pull complete 8b32988996c5: Pull complete Digest: sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e Status: Downloaded newer image for openshift/hello-openshift:latest docker.io/openshift/hello-openshift:latest List available local images again: docker images REPOSITORY TAG IMAGE ID CREATED SIZE openshift/hello-openshift latest 7af3297a3fb4 2 years ago 6.09MB Inspect the image metadata: docker inspect openshift/hello-openshift Note that: It exposes two ports: 8080 and 8888 It runs as user 1001 The entry point executable is /hello-openshift [ { \"Id\": \"sha256:7af3297a3fb4487b740ed6798163f618e6eddea1ee5fa0ba340329fcae31c8f6\", \"RepoTags\": [ \"openshift/hello-openshift:latest\" ], \"RepoDigests\": [ \"openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e\" ], ... \"Config\": { \"User\": \"1001\", ... \"ExposedPorts\": { \"8080/tcp\": {}, \"8888/tcp\": {} }, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], ... \"Entrypoint\": [ \"/hello-openshift\" ] ... } ] Run the image in an container: docker run --name hello1 -d -p 8080:8080 -p 8888:8888 openshift/hello-openshift Note that: The --name option gives the container a name. The -d option runs the command in the background as a daemon The -p option maps the port on the host to the port in the container. Through virtual networking, the port within the container is always the same for all running instances. But to support multiple concurrent running instances, the actual port on the host must be different for each instance. When you start the container, you may assign a new port on the host dynamically. The output of the command is the container ID for the running container. If the container starts successfully, the executable specified by the Entrypoint in the metadata is run. For our sample, it is /hello-openshift . Access the application in the container. Open the Firefox Web Browser from inside of the VM. Go to the URL http://localhost:8080 Also try port 8888 Run another instance of the same image. Note that this new instance is assigned new port numbers 8081 and 8889 on the host. This is so that they don't conflict with the ports 8080 and 8888 already allocated to the first instance. docker run --name hello2 -d -p 8081:8080 -p 8889:8888 openshift/hello-openshift Question: how does this compare to the time it takes to start a new virtual machine? Access the application in the new container the same way. Return to the Firefox Web Browser but instead go to the URL http://localhost:8081 Also try port 8889 Verify there are two containers running in the same host: docker ps : CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5a62f8527b44 openshift/hello-openshift \"/hello-openshift\" About a minute ago Up About a minute 0.0.0.0:8081->8080/tcp, 0.0.0.0:8889->8888/tcp hello2 c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 4 minutes ago Up 4 minutes 0.0.0.0:8080->8080/tcp, 0.0.0.0:8888->8888/tcp hello1 View the logs: docker logs hello1 And the output: ``` serving on 8888 serving on 8080 ``` View the logs on the second container: docker logs hello2 And the output: serving on 8888 serving on 8080 Note: within the container, each instance behaves as if it's running in its own virtual environment, and has opened the same ports. Outside of the container, different ports are opened. To export the file system of a running container: docker export hello1 > hello1.tar List the files on the file system: tar -tvf hello1.tar Note that this is a very small image. -rwxr-xr-x 0/0 0 2020-04-29 16:48 .dockerenv drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/ -rwxr-xr-x 0/0 0 2020-04-29 16:48 dev/console drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/pts/ drwxr-xr-x 0/0 0 2020-04-29 16:48 dev/shm/ drwxr-xr-x 0/0 0 2020-04-29 16:48 etc/ -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/hostname -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/hosts lrwxrwxrwx 0/0 0 2020-04-29 16:48 etc/mtab -> /proc/mounts -rwxr-xr-x 0/0 0 2020-04-29 16:48 etc/resolv.conf -rwxr-xr-x 0/0 6089990 2018-04-18 10:22 hello-openshift drwxr-xr-x 0/0 0 2020-04-29 16:48 proc/ drwxr-xr-x 0/0 0 2020-04-29 16:48 sys/ Run another command in the running container. You can reach into the running container to run another command. The typical use case is to run a shell command, so you can use the shell to navigate within the container and run other commands. However, our image is tiny, and there is no built-in shell. For the purpose of this lab, we'll execute the same command again: docker exec -ti hello1 /hello-openshift . Running this command again in the same container results in an error, because there is already another copy running in the background that is bound to the ports 8080 and 8888: serving on 8888 serving on 8080 panic: ListenAndServe: listen tcp :8888: bind: address already in use ... Stop the containers: docker stop hello1 docker stop hello2 List running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES List all containers, including stopped ones: docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5a62f8527b44 openshift/hello-openshift \"/hello-openshift\" 28 minutes ago Exited (2) 28 seconds ago hello2 c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 31 minutes ago Exited (2) 32 seconds ago hello1 Restart a stopped container: docker restart hello1 List running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c9d49aaa01b7 openshift/hello-openshift \"/hello-openshift\" 33 minutes ago Up 8 seconds 0.0.0.0:8080->8080/tcp, 0.0.0.0:8888->8888/tcp hello1 Stop the container: docker stop hello1 Remove stopped containers, and note that there are no more containers: docker rm hello1 docker rm hello2 docker ps -a Remove the image from local cache: View current images: docker images Example output: REPOSITORY TAG IMAGE ID CREATED SIZE openshift/hello-openshift latest 7af3297a3fb4 2 years ago 6.09MB Remove the image: docker rmi openshift/hello-openshift Example output: Untagged: openshift/hello-openshift:latest Untagged: openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e Deleted: sha256:7af3297a3fb4487b740ed6798163f618e6eddea1ee5fa0ba340329fcae31c8f6 Deleted: sha256:8fd6a1ece3ceceae6d714004614bae5b581c83ab962d838ef88ce760583dcb80 Deleted: sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef Check that the image has been removed: docker images Example output: REPOSITORY TAG IMAGE ID CREATED SIZE","title":"Run a pre-built image"},{"location":"liberty-labs/HelloContainer/README - Copy/#build-and-run-your-own-image","text":"We use a Containerfile , which contains the instructions to create the new layers of your image. For those familiar with docker, the Containerfile is equivalent to Dockerfile . Recall that an image contains the entire file system that you want to use to run your virtual process in a container. For this sample, we are building a new image for a Java EE web application ServletApp.war. It is configured to run on the WebSphere Liberty Runtime. The configuration file for the server is in the server.xml. Change directory to openshift-workshop-was/labs/Openshift/HelloContainer cd openshift-workshop-was/labs/Openshift/HelloContainer Review the provided Containerfile from the directory: cat Containerfile Content of Containerfile : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi COPY server.xml /config COPY ServletApp.war /config/dropins/app.war RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml To create a new image, you start with a pre-existing image. The first line FROM specifies the existing image to be used as the base. If this is not in the local registry, it will be pulled from a remote registry such as docker hub. The base image we are using, ibmcom/websphere-liberty , is already prepackaged for us and made available on docker hub. The second line COPY is a straight copy of the file server.xml from the local directory to /config/server.xml in the image. This adds a new layer to the image with the actual server configuration to be used. The third line, another COPY , copies ServletApp.war from the current directory into a new layer in the image you are creating, at the location /config/dropins/app.war . The last line RUN runs the installUtility command within the image to install additional features required to run the server as specified in server.xml . You can use the RUN command to run any command that is available within the image to customize the image itself. Run the build. Ensure you include . at the end of the command (the dot indicates using the file from the current directory): docker build -t app -f Containerfile . The -t option tags the name of the image as app . The -f option specifies the name of the Containerfile . The build command runs the commands in Containerfile to build a new image called app . Example output: Sending build context to Docker daemon 25.6kB Step 1/4 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi kernel-java8-ibmjava-ubi: Pulling from ibmcom/websphere-liberty ee2244abc66f: Pull complete befb03b11956: Pull complete 137dc88f6a93: Pull complete 5bdd69a33184: Pull complete d4e2554981d7: Pull complete 32c91bc0f2e1: Pull complete db7e931336a9: Pull complete 3b32f9956ae2: Pull complete 304584ffa0a2: Pull complete 9f6da4c82b7e: Pull complete b6fa5b2e2325: Pull complete Digest: sha256:d76f79695afe2f653fc7b272f9a629105446e6b78ff0d733d494c93ff05728e7 Status: Downloaded newer image for ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> 4d9265befb26 Step 2/4 : COPY server.xml /config ---> 4a02d03d3725 Step 3/4 : COPY ServletApp.war /config/dropins/app.war ---> b2def2a0feac Step 4/4 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Running in 5f5b05aec1ae Checking for missing features required by the server ... The server requires the following additional features: appsecurity-2.0 servlet-3.0. Installing features from the repository ... Establishing a connection to the configured repositories ... This process might take several minutes to complete. Successfully connected to all configured repositories. Preparing assets for installation. This process might take several minutes to complete. Additional Liberty features must be installed for this server. To install the additional features, review and accept the feature license agreement: The --acceptLicense argument was found. This indicates that you have accepted the terms of the license agreement. Step 1 of 12: Downloading ssl-1.0 ... Step 2 of 12: Installing ssl-1.0 ... Step 3 of 12: Downloading appSecurity-2.0 ... Step 4 of 12: Installing appSecurity-2.0 ... Step 5 of 12: Downloading servlet-3.0 ... Step 6 of 12: Installing servlet-3.0 ... Step 7 of 12: Downloading jndi-1.0 ... Step 8 of 12: Installing jndi-1.0 ... Step 9 of 12: Downloading distributedMap-1.0 ... Step 10 of 12: Installing distributedMap-1.0 ... Step 11 of 12: Validating installed fixes ... Step 12 of 12: Cleaning up temporary files ... All assets were successfully installed. Start product validation... Product validation completed successfully. Removing intermediate container 5f5b05aec1ae ---> e1c6bfabda76 Successfully built e1c6bfabda76 Successfully tagged app:latest List the images to see that the new image app is built: docker images Note that the base image, ibmcom/websphere-liberty has also been pulled into the local registry. REPOSITORY TAG IMAGE ID CREATED SIZE app latest baa6bb9ad29d 2 minutes ago 544 MB ibmcom/websphere-liberty kernel-java8-ibmjava-ubi 7ea3d0a2b3fe 4 hours ago 544 MB Start the container. Note that you are running with both http and https ports: docker run -d -p 9080:9080 -p 9443:9443 --name=app-instance app Access the application running in the container: Open the Firefox Web Browswer and go to URL http://localhost:9080/app Check that it renders a page showing Simple Servlet ran successfully . Also point your browser to 9443: https://localhost:9443/app List the running containers: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 595cdc49c710 app \"/opt/ibm/helpers/ru\u2026\" 8 minutes ago Up 8 minutes 0.0.0.0:9080->9080/tcp, 0.0.0.0:9443->9443/tcp app-instance Access the logs to your container: docker logs -f app-instance Use Ctrl-C To exit. Remote shell into your running container to poke around: docker exec -it app-instance /bin/sh In the shell session, run whoami and then run id , note you're not running as root. Note that this is a stripped down environment where many commands are not available. For example, try which ps . cd /logs to find the log files cd /liberty/wlp to find the location of the liberty install cd /liberty/wlp/usr/servers/defaultServer to find the server configuration. cd /opt/ibm/wlp/output/defaultServer to find the workarea files required by the server runtime. Exit from the container: exit Cleanup: docker stop app-instance docker rm app-instance","title":"Build and Run Your Own Image"},{"location":"liberty-labs/HelloContainer/README - Copy/#managing-image-versions","text":"There is no built-in versioning for container images. However, you may use a tagging convention to version your images. The convention is to use major.minor.patch , such as 1.3.5 . The default tag if you don't specify one is latest . Let's assume that the first version we will build for our environment is 1.3.5. (The earlier versions are built in a different environment.) Run the commands to tag the latest app image for our first version: docker tag app app:1 docker tag app app:1.3 docker tag app app:1.3.5 List the images again: docker images And the output: REPOSITORY TAG IMAGE ID CREATED SIZE app 1 d98cbdf82a0d 21 hours ago 542MB app 1.3 d98cbdf82a0d 21 hours ago 542MB app 1.3.5 d98cbdf82a0d 21 hours ago 542MB app latest d98cbdf82a0d 21 hours ago 542MB Note that all the different tags are currently associated with the same image, as they have the same image ID. After tagging, the command docker run app:<version> ... or docker pull app:<version> ... resolves the available versions as follows: app:1 resolves to the latest 1.x.x version, which in this case is 1.3.5 . app:1.3 resolves to the latest 1.3.x version, which in this case is the 1.3.5 app:1.3.5 resolves to the exact version 1.3.5 . After you build a new patch image containing defect fixes, you want to manage the tags for the new image so that a new docker run app:<version> ... or docker pull app:<version> ... command resolves the images as follows: app:1.3.5 : resolves to the existing 1.3.5 image. app:1.3.6 : resolves to the new image app:1.3 : resolves to the new image. app:1 : resolves to the new image Let's simulate a defect fix by building a new image using Containerfile1 instead of Containerfile : docker build -t app -f Containerfile1 . Example output: Sending build context to Docker daemon 32.26kB Step 1/5 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> bb79b9e26fd3 Step 2/5 : COPY server.xml /config ---> Using cache ---> f10659bc62b2 Step 3/5 : COPY ServletApp.war /config/dropins/app.war ---> Using cache ---> 24d85579e404 Step 4/5 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Using cache ---> 5e924d776a9c Step 5/5 : RUN echo test1 > /config/test1 ---> Running in 08e6135b00f5 Removing intermediate container 08e6135b00f5 ---> 69d20332a5e0 Successfully built 69d20332a5e0 Successfully tagged app:latest Tag it as follows : docker tag app app:1 docker tag app app:1.3 docker tag app app:1.3.6 Verify that these are the same images: app:1 , app:1.3 , app:1.3.6 . A new minor version involves compatible changes beyond just bug fixes. After you build a new minor version image, you want to manage the tags such that: app:1.3.5 : resolves to the existing 1.3.5 image. app:1.3.6 : resolves to the existing 1.3.6 image app:1.4.0 : resolves to the new image app:1.3 : resolves to the existing 1.3.6 image. app:1.4 : resolves to the new image. app:1 : resolves to the new image Build a new image using Containerfile2 : docker build -t app -f Containerfile2 . Sending build context to Docker daemon 32.26kB Step 1/6 : FROM ibmcom/websphere-liberty:kernel-java8-ibmjava-ubi ---> bb79b9e26fd3 Step 2/6 : COPY server.xml /config ---> Using cache ---> f10659bc62b2 Step 3/6 : COPY ServletApp.war /config/dropins/app.war ---> Using cache ---> 24d85579e404 Step 4/6 : RUN /liberty/wlp/bin/installUtility install --acceptLicense /config/server.xml ---> Using cache ---> 5e924d776a9c Step 5/6 : RUN echo test1 > /config/test1 ---> Using cache ---> 69d20332a5e0 Step 6/6 : RUN echo test2 > /config/test2 ---> Running in 96ea24b9ba66 Removing intermediate container 96ea24b9ba66 ---> 31b27169b3bc Successfully built 31b27169b3bc Tag it as follows: docker tag app app:1 docker tag app app:1.4 docker tag app app:1.4.0 Verify that 1 , 1.4 , and 1.4.0 are the same image 1.3 and 1.3.6 are the same image","title":"Managing Image Versions"},{"location":"liberty-labs/HelloContainer/README - Copy/#extra-credit","text":"Search the internet for information about multi-stage build. In a single stage build, the final image contains both build and runtime artifacts. A multi-stage build allows you to build with one base image, and copy the result of the build to another base image. This gives you even more control over the output of the build, and the size of the final image. Start another instances of the app image for vertical scaling, but with different port numbers on the host. Point your browser to hub.docker.com , click \"Explore\" and explore the millions of available images. Think about how you would tag a new image at a major version, 2.0.0 . Think about what would be required to manage containers across multiple machines to support horizontal scaling. Congratulations! You have completed the Introduction to Containerization lab.","title":"Extra Credit"},{"location":"liberty-labs/HelloContainer/README - Copy/#next","text":"Please follow the link to do the next lab Introduction to Container Orchestration using Openshift : - Introduction to Container Orchestration using Openshift","title":"Next"},{"location":"liberty-labs/IntroOpenshift/","text":"Introduction to Container Orchestration using Openshift In this lab, we will introduce you to the basics of container Orchestration using Openshift. We will Perform basic navigation using the web console Deploy the hello-openshift image through the web console. Deploy the hello-openshift image through the command line. Prerequisite For background on basic Openshift concepts, read Openshift Concepts for WebSphere Administrators You have the access to OpenShift Web Console with IBM Cloud account ID login. You have cloned the lab into your working directory through the web terminal session. Login to the VM If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password: \\ Password: engageibm Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window. Deploy the hello-openshift image through the web console Login to the web console Open the Firefox Web Browser from the VM. Select the openshift console bookmark at the top left of the browser window to access the OpenShift Container Platform web console. This will take you to a login screen. Click on the htpasswd login option. Log in to the account using the following credentials: Username: ibmadmin Password: engageibm Overview Click on the Overview tab under Home in the left menu to view a summary of events: Scroll down to view the utilization of cluster resources: Scroll further down to view the cluster inventory. Click through each item in the inventory to find out more: Note that: Nodes represent physical or virtual hardware that your Openshift cluster is running. Pods are used to host and run one or more containers. Each node may run multiple pods. Containers in the same pod share the same network and storage. Storage classes represent the different types of storage configured and made available for your Openshift cluster. Persistent Volume Claims (PVCs) represent the usage of storage by the pods. After a pod is removed, data not persistent to persistent storage are gone. Projects Openshift projects allow you to group related resources together and to assign them separate management policies. It is common for artifacts related to different applications to be assigned to different projects . Resources that belong to the same project are stored in the same Kubernetes namespace . Click on the Projects tab under Home in the left menu, followed by Create Project : In the dialog, enter myproject as project name, then click Create : After creation, click on each of the tabs of myproject you just created. Note that: the YAML tab shows you the YAML representation of your project. Every resource in Openshift is represented as a REST data structure. We'll be working with YAML files a lot more when we interact with Openshift via the command line. The Role Bindings tab shows you the security configurations that apply to your project. For now, just take notice that there are many different roles already defined when a project is created. Each of these roles is used for a different purpose, and already mapped to different users and groups, or service accounts. First Application The typical artifacts you will need to run an application in Openshift are: A container image containing your application, hosted in a container registry One or more pods that specifies where to fetch an image and how it should be hosted. A deployment to control the number of instances pods. You don't normally configure a pod directly. Instead, you configure a deployment to manage a set of pods . A service that exposes the application within the internal network, and enables the application to be load balanced within the Openshift cluster. A route or ingress to make the application accessible outside of the Openshift cluster firewall. First deployment Under the Workloads tab, click Deployments , followed by Create Deployment : Note that the console shows you the YAML file for the deployment. Change the number of replicas from default 3 to 2 , then click Create : Here is the specification of the deployment in its entirety: apiVersion: apps/v1 kind: Deployment metadata: name: example namespace: myproject spec: selector: matchLabels: app: hello-openshift replicas: 2 template: metadata: labels: app: hello-openshift spec: containers: - name: hello-openshift image: openshift/hello-openshift ports: - containerPort: 8080 Let's review this resource: Every resource in Openshift has a group, version, and kind. For the Deployment resource: The group is apps The version is v1 The kind is Deployment The metadata specifies data that is needed for the runtime: The name of this instance is example The namespace where the resource is running is myproject Though not shown here, any labels associated with the resource. We will see the use of labels later. The spec section defines the details specific to this kind of resource: The selector defines details of the pods that this deployment will manage. The matchLabels attribute with value app: hello-openshift means this deployment instance will search for and manage all pods whose labels contain app: hello-openshift . The replicas: 2 field specifies the number of instances to run. The template section describes information about how to run the container image and create the pods : The labels section specifies what labels to add to the pods being to be created. Note that it matches the labels defined in the selector . The containers section specifies where to fetch the container image and which ports to expose. For our example, the image to run is openshift/hello-openshift . Wait for both pods to be running: Click on the YAML tab, and note the additions to the original input YAML file. Here is a sample YAML after the deployment is created : kind: Deployment apiVersion: apps/v1 metadata: name: example namespace: myproject selfLink: /apis/apps/v1/namespaces/myproject/deployments/example uid: 7c6a339b-385c-41bf-b4bf-3b6a120ef137 resourceVersion: '297294' generation: 1 creationTimestamp: '2020-01-30T15:45:15Z' annotations: deployment.kubernetes.io/revision: '1' spec: replicas: 2 selector: matchLabels: app: hello-openshift template: metadata: creationTimestamp: null labels: app: hello-openshift spec: containers: - name: hello-openshift image: openshift/hello-openshift ports: - containerPort: 8080 protocol: TCP resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File imagePullPolicy: Always restartPolicy: Always terminationGracePeriodSeconds: 30 dnsPolicy: ClusterFirst securityContext: {} schedulerName: default-scheduler strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 25% maxSurge: 25% revisionHistoryLimit: 10 progressDeadlineSeconds: 600 status: observedGeneration: 1 replicas: 2 updatedReplicas: 2 readyReplicas: 2 availableReplicas: 2 conditions: - type: Available status: 'True' lastUpdateTime: '2020-01-30T15:45:20Z' lastTransitionTime: '2020-01-30T15:45:20Z' reason: MinimumReplicasAvailable message: Deployment has minimum availability. - type: Progressing status: 'True' lastUpdateTime: '2020-01-30T15:45:20Z' lastTransitionTime: '2020-01-30T15:45:15Z' reason: NewReplicaSetAvailable message: ReplicaSet \"example-75778c488\" has successfully progressed. Note that: There are quite a bit more metadata . Metadata may be added by any number of controllers as needed to help with their function. The spec has more attributes filled in as well. These are default values that were not specified in our original YAML file. But sometimes it is also possible that some values are overridden by background admission controllers. The status sub-resource is how Openshift communicates that status of the resource. The status is updated regularly as the underlying state of the resource changes. Click on Pods . Note that the pods resources are managed by the controller for your deployment . You do not create the pod resources yourself. That is the reason that Pods tab is under the deployment resource you just created. Click on one of the pods: Explore the various tabs for your pod Overview: displays the overall resource usage for your pod. Note that for CPU usage, the unit is m, or milli-core, which is 1/1000th of one core. YAML: examine the YAML that describes your pod. This YAML is created by the deployment controller based on the specification you supplied in your deployment. Note that labels associated with your pod are what you had specified in the deployment. Environment: lists the environment variables defined for your pod. For our hello-openshift pod, there is none. Logs: shows the console log for your container. Note that it is the same log as the log from the Introduction to Docker lab, as the same image is being used. Terminal: Opens a remote shell into your container. As with the Introduction to Docker lab, no shell is available within the container for this image. This makes it more secure, but also more difficult to debug. First Service A service enables the pods we just created to be load balanced within the Openshift cluster. Scroll down to the Networking tab on the left navigation, click Services , then click Create Service : Update the YAML parameters as follows: (Before update) Under spec.selector, change MyApp to hello-openshift . This is how the service will find the pods to load balance. Therefore, it matches the labels ( spec.selector.matchLabels ) that we used when creating the deployment for the hello-openshift application. Under spec.ports, change 80 to 8080 and change 9376 to 8080 (the same ports we used previously). Click Create (After update) After the service is created, click on the YAML tab: The YAML file looks like: Kind: Service apiVersion: v1 metadata: name: example namespace: myproject selfLink: /api/v1/namespaces/myproject/services/example uid: 6ca3c2b7-bbfa-432e-8757-c60dbf04b26b resourceVersion: '307351' creationTimestamp: '2020-01-30T16:28:03Z' spec: ports: - protocol: TCP port: 8080 targetPort: 8080 selector: app: hello-openshift clusterIP: 172.21.239.191 type: ClusterIP sessionAffinity: None status: loadBalancer: {} Note that for this service, there is a cluster wide IP address created, and that it is being load balanced. Also session affinity is not set for this service. First Route A route exposes your internal endpoints outside your cluster's built-in firewall. Click on the Route tab under Networking in the left navigation, then click Create Route : Supply input to the following parameters: Name: example Service: example Target Port: 8080 --> 8080 (TCP) Click Create Note that we are ignoring TLS configuration just for the purpose of this lab. Security will be addressed in a different lab. Access the route at the link provided under Location: If you have configured everything correctly, the browser will show Hello Openshift! . Congratulations, you just deployed your first application to Openshift. Changing Replica Instances Click on the Projects tab under Home from the left navigation, then click on myproject : Scroll down to see the resources that were created. Recall that we have created one deployment with 2 pods in the specification. We also created one service, and one route. Click on the 2 pods: Delete one of the pods by clicking on the menu on the right, then selecting Delete pod . When prompted, click Delete . This is not the right way to reduce number of instances. You will notice that as soon as one of the pods is being terminated, another one is being created. The reason is that the controller for the deployment resource knows that your specification is for 2 instances, and it honors that specification by creating another one. This also gives you automatic failure recovery should one of the pods crashes on its own. To change the number of instances, you will need to change the specification of your deployment. Click on the Deployments tab under Workloads in the left navigation, then click on example deployment: Click on the down arrow to reduce the replica size down to 1: After the operation is completed, click on the YAML tab: Note that the console had changed the REST specification on your behalf so that the replica count is now 1: Deploy the hello-openshift image through the command line You can use both oc , the openshift command line tool, or kubectl , the Kubernetes command line tool, to interact with Openshift. Resources in Openshift are configured via REST data structure. For the command line tools, the REST data structure may be stored either in a YAML file, or in a JSON file. The command line tools may be used to: List available resources Create resources Update existing resources Delete resources Command Line Terminal The oc command is already installed on your VM's terminal. Open a terminal window from the VM and clone the lab to your local directory via: git clone https://github.com/IBM/openshift-workshop-was.git Change directory to: openshift-workshop-was/labs/Openshift/IntroOpenshift cd openshift-workshop-was/labs/Openshift/IntroOpenshift Login Return to the Openshift console, click on the arrow next to your login name and select Copy Login Command . In the new window that pops up, click on Display Token : Copy the oc login command and paste it into your web terminal. oc login --token=<TOKEN> --server=<SERVER Address> After login, the project last accessed is displayed, and it may or may not be the default project shown below: Logged into \"<SERVER address\" as \"<USER>\" using the token provided. You have access to 56 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\". Listing resources Use oc api-resources to list all available resource kinds. Note that resources in Openshift have a group, version, and kind. Some resources are global (not in a namespace), while others are scoped to a namespace. Many resources also have short names to save typing when using the command line tool. For example, you may use cm instead of ConfigMap as a command line parameter when the parameter is for a KIND . Example output: NAME SHORTNAMES APIGROUP NAMESPACED KIND bindings true Binding componentstatuses cs false ComponentStatu s configmaps cm true ConfigMap endpoints ep true Endpoints events ev true Event limitranges limits true LimitRange namespaces ns false Namespace nodes no false Node Listing instances of a resource kind List all projects: oc get projects NAME DISPLAY NAME STATUS default Active ibm-cert-store Active ibm-system Active kube-node-lease Active kube-public Active kube-system Active myproject Active ... List all pods in all namespaces: oc get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE ibm-system ibm-cloud-provider-ip-52-116-182-130-67c44c6d5d-bh9x5 1/1 Running 0 24h ibm-system ibm-cloud-provider-ip-52-116-182-130-67c44c6d5d-cs5ln 1/1 Running 0 24h kube-system calico-kube-controllers-549fdb8d79-khkvr 1/1 Running 0 25h ... List all pods within a namespace: oc get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-549fdb8d79-khkvr 1/1 Running 0 25h calico-node-jc6ln 1/1 Running 0 24h calico-node-t7zwg 1/1 Running 0 24h ... Projects List all projects: oc get projects NAME DISPLAY NAME STATUS default Active ibm-cert-store Active ibm-system Active kube-node-lease Active Get current project: oc project (Note: current project may not be myproject shown below): Using project \"myproject\" on server \"https://c100-e.us-south.containers.cloud.ibm.com:32541\". Create a new project and make it the current project: oc new-project project1 Now using project \"project1\" on server \"https://c100-e.us-south.containers.cloud.ibm.com:32541\". Switch to the default project: oc project default Switch back to project1 : oc project project1 View the REST specification of the project: oc get project project1 -o yaml : apiVersion: project.openshift.io/v1 kind: Project metadata: annotations: openshift.io/description: \"\" openshift.io/display-name: \"\" openshift.io/requester: IAM#mcheng@us.ibm.com openshift.io/sa.scc.mcs: s0:c24,c9 openshift.io/sa.scc.supplemental-groups: 1000570000/10000 openshift.io/sa.scc.uid-range: 1000570000/10000 creationTimestamp: \"2020-01-30T20:28:13Z\" name: project1 resourceVersion: \"364002\" selfLink: /apis/project.openshift.io/v1/projects/project1 uid: a817b908-a7fe-4f82-9bfa-e18fa4c12584 spec: finalizers: - kubernetes status: phase: Active First Application First Deployment In your web terminal session, under the directory where you clone the labs repository (.../openshift-workshop-was/labs/Openshift/IntroOpenshift), you will find Deployment.yaml , for example, root@lab-tools-6d4cbb56b6-cn2k5:/openshift-workshop-was/labs/Openshift/IntroOpenshift# ls -lt drwxr-xr-x. 2 root root 4096 Apr 9 01:13 images -rw-r--r--. 1 root root 32461 Apr 9 01:13 README.md -rw-r--r--. 1 root root 171 Apr 9 01:13 Route.yaml -rw-r--r--. 1 root root 207 Apr 9 01:13 Service.yaml -rw-r--r--. 1 root root 384 Apr 9 01:13 Deployment.yaml Review the contents of Deployment.yaml and note that it is identical to the to the deployment from the last section except for the namespace. This allows us to deploy the same image in a different project. Using the same image customized for different environments is an important concept that will be covered further in future labs. cat Deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: example namespace: project1 spec: selector: matchLabels: app: hello-openshift replicas: 2 template: metadata: labels: app: hello-openshift spec: containers: - name: hello-openshift image: openshift/hello-openshift ports: - containerPort: 8080 Apply the deployment via the command line: oc apply -f Deployment.yaml deployment.apps/example created Check the status of deployment: oc get deployment example -o yaml . If the status does not show available replica count of 2, wait a few seconds before retrying. apiVersion: extensions/v1beta1 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: \"1\" kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"metadata\":{\"annotations\":{},\"name\":\"example\",\"namespace\":\"project1\"}, \"spec\":{\"replicas\":2,\"selector\":{\"matchLabels\":{\"app\":\"hello-openshift\"}},\"template\":{\"metadata\":{\"labels\":{\"app\":\"hello -openshift\"}},\"spec\":{\"containers\":[{\"image\":\"openshift/hello-openshift\",\"name\":\"hello-openshift\",\"ports\":[{\"containerPo rt\":8080}]}]}}}} creationTimestamp: \"2020-01-30T20:37:28Z\" generation: 1 name: example namespace: project1 resourceVersion: \"366251\" selfLink: /apis/extensions/v1beta1/namespaces/project1/deployments/example uid: f011a93d-2231-4187-b265-f350ee830971 spec: progressDeadlineSeconds: 600 replicas: 2 revisionHistoryLimit: 10 selector: matchLabels: app: hello-openshift strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: hello-openshift spec: containers: - image: openshift/hello-openshift imagePullPolicy: Always name: hello-openshift ports: - containerPort: 8080 protocol: TCP resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 status: availableReplicas: 2 conditions: - lastTransitionTime: \"2020-01-30T20:37:31Z\" lastUpdateTime: \"2020-01-30T20:37:31Z\" message: Deployment has minimum availability. reason: MinimumReplicasAvailable status: \"True\" type: Available - lastTransitionTime: \"2020-01-30T20:37:28Z\" lastUpdateTime: \"2020-01-30T20:37:31Z\" message: ReplicaSet \"example-75778c488\" has successfully progressed. reason: NewReplicaSetAvailable status: \"True\" type: Progressing observedGeneration: 1 readyReplicas: 2 replicas: 2 updatedReplicas: 2 List the running pods created by the controller for the deployment: oc get pods NAME READY STATUS RESTARTS AGE example-75778c488-7k7q2 1/1 Running 0 3m37s example-75778c488-c9jhd 1/1 Running 0 3m37s List the details for one of the pods: oc get pods <pod name> -o yaml > Note: <pod name> is listed under NAME in the previous command's output. apiVersion: v1 kind: Pod metadata: annotations: cni.projectcalico.org/podIP: 172.30.26.229/32 openshift.io/scc: restricted creationTimestamp: \"2020-01-30T20:37:28Z\" generateName: example-75778c488- labels: app: hello-openshift pod-template-hash: 75778c488 name: example-75778c488-7k7q2 namespace: project1 ownerReferences: - apiVersion: apps/v1 blockOwnerDeletion: true controller: true kind: ReplicaSet name: example-75778c488 uid: 2b03ef6a-8a1a-4f7e-9502-7249a4dabb98 resourceVersion: \"366248\" selfLink: /api/v1/namespaces/project1/pods/example-75778c488-7k7q2 uid: 504c999e-d47d-48f7-afcb-992a5a13fc67 spec: containers: - image: openshift/hello-openshift imagePullPolicy: Always name: hello-openshift ports: - containerPort: 8080 protocol: TCP resources: {} securityContext: capabilities: drop: - KILL - MKNOD - SETGID - SETUID runAsUser: 1000570000 terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: default-token-ngv4p readOnly: true dnsPolicy: ClusterFirst enableServiceLinks: true imagePullSecrets: - name: default-dockercfg-shjmz nodeName: 10.177.93.14 priority: 0 restartPolicy: Always schedulerName: default-scheduler securityContext: fsGroup: 1000570000 seLinuxOptions: level: s0:c24,c9 serviceAccount: default serviceAccountName: default terminationGracePeriodSeconds: 30 tolerations: - effect: NoExecute key: node.kubernetes.io/not-ready operator: Exists tolerationSeconds: 300 - effect: NoExecute key: node.kubernetes.io/unreachable operator: Exists tolerationSeconds: 300 volumes: - name: default-token-ngv4p secret: defaultMode: 420 secretName: default-token-ngv4p status: conditions: - lastProbeTime: null lastTransitionTime: \"2020-01-30T20:37:28Z\" status: \"True\" type: Initialized - lastProbeTime: null lastTransitionTime: \"2020-01-30T20:37:31Z\" status: \"True\" type: Ready - lastProbeTime: null lastTransitionTime: \"2020-01-30T20:37:31Z\" status: \"True\" type: ContainersReady - lastProbeTime: null lastTransitionTime: \"2020-01-30T20:37:28Z\" status: \"True\" type: PodScheduled containerStatuses: - containerID: cri-o://42828d4a8333d4fa9d3882805680d7693616610ff78a3e07f4794d91b86862b5 image: docker.io/openshift/hello-openshift:latest imageID: docker.io/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e lastState: {} name: hello-openshift ready: true restartCount: 0 started: true state: running: startedAt: \"2020-01-30T20:37:30Z\" hostIP: 10.177.93.14 phase: Running podIP: 172.30.26.229 podIPs: - ip: 172.30.26.229 qosClass: BestEffort startTime: \"2020-01-30T20:37:28Z\" Show the logs of one of the pods: oc logs <pod name> serving on 8888 serving on 8080 Take a look at Service.yaml and note that it's for the project1 namespace: cat Service.yaml Example output: apiVersion: v1 kind: Service metadata: name: example namespace: project1 spec: ports: - protocol: TCP port: 8080 targetPort: 8080 selector: app: hello-openshift type: ClusterIP Create the service so that it's accessible and load balanced for pods with label app: hello-openshift within the project1 namespace: oc apply -f Service.yaml service/example created Examine Route.yaml: cat Route.yaml Output: apiVersion: route.openshift.io/v1 kind: Route metadata: name: example namespace: project1 spec: port: targetPort: 8080 to: kind: Service name: example Apply the route to make the service reachable from outside the cluster: oc apply -f Route.yaml route.route.openshift.io/example created Generate the URL for the route, and point your browser to it: echo http://$(oc get route example --template='{{ .spec.host }}') Output: http://example-project1.apps.demo.ibmdte.net Open your Firefox browser again and visit the URL outputted by the previous command. You should see a web page displaying the following message: Changing Replica Instance List pods: oc get pods NAME READY STATUS RESTARTS AGE example-75778c488-7k7q2 1/1 Running 0 60m example-75778c488-c9jhd 1/1 Running 0 60m Delete one of the pods: oc delete pod <pod name> pod \"example-75778c488-7k7q2\" deleted List pods again and note that a new instance has been created as expected. The deployment specified 2 instances, so the controller tries to maintain 2 instances: oc get pods NAME READY STATUS RESTARTS AGE example-75778c488-c9jhd 1/1 Running 0 63m example-75778c488-rhjrx 1/1 Running 0 28s To reduce the number of pods, we can patch the resource in one of two ways: Scripted patch using the patch option of the command line: oc patch deployment example -p '{ \"spec\": { \"replicas\": 1 } }' Interactive patch using the edit option of the command line through vi editor: oc edit deployment example Under the spec section (not under the status section), change replicas: 2 to replicas: 1 , and save the change (by :wq ). The output: deployment.extensions/example edited Note: The above edits the copy that is stored in Openshift. You may also edit your local copy of Deployment.yaml and re-apply it. List the pods to show only 1 pod is running: oc get pods NAME READY STATUS RESTARTS AGE example-75778c488-c9jhd 1/1 Running 0 65m Cleanup: oc delete route example oc delete service example oc delete deployment example oc get pods . You may have to do this a few times, to wait for the pods to be deleted. Congratulations, you have deployed your first application to Openshift via the command line. Next Please follow the link to the next lab Operational Modernization : - Operational Modernization","title":"Introduction to Container Orchestration using Openshift"},{"location":"liberty-labs/IntroOpenshift/#introduction-to-container-orchestration-using-openshift","text":"In this lab, we will introduce you to the basics of container Orchestration using Openshift. We will Perform basic navigation using the web console Deploy the hello-openshift image through the web console. Deploy the hello-openshift image through the command line.","title":"Introduction to Container Orchestration using Openshift"},{"location":"liberty-labs/IntroOpenshift/#prerequisite","text":"For background on basic Openshift concepts, read Openshift Concepts for WebSphere Administrators You have the access to OpenShift Web Console with IBM Cloud account ID login. You have cloned the lab into your working directory through the web terminal session.","title":"Prerequisite"},{"location":"liberty-labs/IntroOpenshift/#login-to-the-vm","text":"If the VM is not already started, start it by clicking the Play button. After the VM is started, click the desktop VM to access it. Login with ibmuser ID. Click on the ibmuser icon on the Ubuntu screen. When prompted for the password for ibmuser , enter \" engageibm \" as the password: \\ Password: engageibm Resize the Skytap environment window for a larger viewing area while doing the lab. From the Skytap menu bar, click on the \" Fit to Size \" icon. This will enlarge the viewing area to fit the size of your browser window.","title":"Login to the VM"},{"location":"liberty-labs/IntroOpenshift/#deploy-the-hello-openshift-image-through-the-web-console","text":"","title":"Deploy the hello-openshift image through the web console"},{"location":"liberty-labs/IntroOpenshift/#login-to-the-web-console","text":"Open the Firefox Web Browser from the VM. Select the openshift console bookmark at the top left of the browser window to access the OpenShift Container Platform web console. This will take you to a login screen. Click on the htpasswd login option. Log in to the account using the following credentials: Username: ibmadmin Password: engageibm","title":"Login to the web console"},{"location":"liberty-labs/IntroOpenshift/#overview","text":"Click on the Overview tab under Home in the left menu to view a summary of events: Scroll down to view the utilization of cluster resources: Scroll further down to view the cluster inventory. Click through each item in the inventory to find out more: Note that: Nodes represent physical or virtual hardware that your Openshift cluster is running. Pods are used to host and run one or more containers. Each node may run multiple pods. Containers in the same pod share the same network and storage. Storage classes represent the different types of storage configured and made available for your Openshift cluster. Persistent Volume Claims (PVCs) represent the usage of storage by the pods. After a pod is removed, data not persistent to persistent storage are gone.","title":"Overview"},{"location":"liberty-labs/IntroOpenshift/#projects","text":"Openshift projects allow you to group related resources together and to assign them separate management policies. It is common for artifacts related to different applications to be assigned to different projects . Resources that belong to the same project are stored in the same Kubernetes namespace . Click on the Projects tab under Home in the left menu, followed by Create Project : In the dialog, enter myproject as project name, then click Create : After creation, click on each of the tabs of myproject you just created. Note that: the YAML tab shows you the YAML representation of your project. Every resource in Openshift is represented as a REST data structure. We'll be working with YAML files a lot more when we interact with Openshift via the command line. The Role Bindings tab shows you the security configurations that apply to your project. For now, just take notice that there are many different roles already defined when a project is created. Each of these roles is used for a different purpose, and already mapped to different users and groups, or service accounts.","title":"Projects"},{"location":"liberty-labs/IntroOpenshift/#first-application","text":"The typical artifacts you will need to run an application in Openshift are: A container image containing your application, hosted in a container registry One or more pods that specifies where to fetch an image and how it should be hosted. A deployment to control the number of instances pods. You don't normally configure a pod directly. Instead, you configure a deployment to manage a set of pods . A service that exposes the application within the internal network, and enables the application to be load balanced within the Openshift cluster. A route or ingress to make the application accessible outside of the Openshift cluster firewall.","title":"First Application"},{"location":"liberty-labs/IntroOpenshift/#first-deployment","text":"Under the Workloads tab, click Deployments , followed by Create Deployment : Note that the console shows you the YAML file for the deployment. Change the number of replicas from default 3 to 2 , then click Create : Here is the specification of the deployment in its entirety: apiVersion: apps/v1 kind: Deployment metadata: name: example namespace: myproject spec: selector: matchLabels: app: hello-openshift replicas: 2 template: metadata: labels: app: hello-openshift spec: containers: - name: hello-openshift image: openshift/hello-openshift ports: - containerPort: 8080 Let's review this resource: Every resource in Openshift has a group, version, and kind. For the Deployment resource: The group is apps The version is v1 The kind is Deployment The metadata specifies data that is needed for the runtime: The name of this instance is example The namespace where the resource is running is myproject Though not shown here, any labels associated with the resource. We will see the use of labels later. The spec section defines the details specific to this kind of resource: The selector defines details of the pods that this deployment will manage. The matchLabels attribute with value app: hello-openshift means this deployment instance will search for and manage all pods whose labels contain app: hello-openshift . The replicas: 2 field specifies the number of instances to run. The template section describes information about how to run the container image and create the pods : The labels section specifies what labels to add to the pods being to be created. Note that it matches the labels defined in the selector . The containers section specifies where to fetch the container image and which ports to expose. For our example, the image to run is openshift/hello-openshift . Wait for both pods to be running: Click on the YAML tab, and note the additions to the original input YAML file. Here is a sample YAML after the deployment is created : kind: Deployment apiVersion: apps/v1 metadata: name: example namespace: myproject selfLink: /apis/apps/v1/namespaces/myproject/deployments/example uid: 7c6a339b-385c-41bf-b4bf-3b6a120ef137 resourceVersion: '297294' generation: 1 creationTimestamp: '2020-01-30T15:45:15Z' annotations: deployment.kubernetes.io/revision: '1' spec: replicas: 2 selector: matchLabels: app: hello-openshift template: metadata: creationTimestamp: null labels: app: hello-openshift spec: containers: - name: hello-openshift image: openshift/hello-openshift ports: - containerPort: 8080 protocol: TCP resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File imagePullPolicy: Always restartPolicy: Always terminationGracePeriodSeconds: 30 dnsPolicy: ClusterFirst securityContext: {} schedulerName: default-scheduler strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 25% maxSurge: 25% revisionHistoryLimit: 10 progressDeadlineSeconds: 600 status: observedGeneration: 1 replicas: 2 updatedReplicas: 2 readyReplicas: 2 availableReplicas: 2 conditions: - type: Available status: 'True' lastUpdateTime: '2020-01-30T15:45:20Z' lastTransitionTime: '2020-01-30T15:45:20Z' reason: MinimumReplicasAvailable message: Deployment has minimum availability. - type: Progressing status: 'True' lastUpdateTime: '2020-01-30T15:45:20Z' lastTransitionTime: '2020-01-30T15:45:15Z' reason: NewReplicaSetAvailable message: ReplicaSet \"example-75778c488\" has successfully progressed. Note that: There are quite a bit more metadata . Metadata may be added by any number of controllers as needed to help with their function. The spec has more attributes filled in as well. These are default values that were not specified in our original YAML file. But sometimes it is also possible that some values are overridden by background admission controllers. The status sub-resource is how Openshift communicates that status of the resource. The status is updated regularly as the underlying state of the resource changes. Click on Pods . Note that the pods resources are managed by the controller for your deployment . You do not create the pod resources yourself. That is the reason that Pods tab is under the deployment resource you just created. Click on one of the pods: Explore the various tabs for your pod Overview: displays the overall resource usage for your pod. Note that for CPU usage, the unit is m, or milli-core, which is 1/1000th of one core. YAML: examine the YAML that describes your pod. This YAML is created by the deployment controller based on the specification you supplied in your deployment. Note that labels associated with your pod are what you had specified in the deployment. Environment: lists the environment variables defined for your pod. For our hello-openshift pod, there is none. Logs: shows the console log for your container. Note that it is the same log as the log from the Introduction to Docker lab, as the same image is being used. Terminal: Opens a remote shell into your container. As with the Introduction to Docker lab, no shell is available within the container for this image. This makes it more secure, but also more difficult to debug.","title":"First deployment"},{"location":"liberty-labs/IntroOpenshift/#first-service","text":"A service enables the pods we just created to be load balanced within the Openshift cluster. Scroll down to the Networking tab on the left navigation, click Services , then click Create Service : Update the YAML parameters as follows: (Before update) Under spec.selector, change MyApp to hello-openshift . This is how the service will find the pods to load balance. Therefore, it matches the labels ( spec.selector.matchLabels ) that we used when creating the deployment for the hello-openshift application. Under spec.ports, change 80 to 8080 and change 9376 to 8080 (the same ports we used previously). Click Create (After update) After the service is created, click on the YAML tab: The YAML file looks like: Kind: Service apiVersion: v1 metadata: name: example namespace: myproject selfLink: /api/v1/namespaces/myproject/services/example uid: 6ca3c2b7-bbfa-432e-8757-c60dbf04b26b resourceVersion: '307351' creationTimestamp: '2020-01-30T16:28:03Z' spec: ports: - protocol: TCP port: 8080 targetPort: 8080 selector: app: hello-openshift clusterIP: 172.21.239.191 type: ClusterIP sessionAffinity: None status: loadBalancer: {} Note that for this service, there is a cluster wide IP address created, and that it is being load balanced. Also session affinity is not set for this service.","title":"First Service"},{"location":"liberty-labs/IntroOpenshift/#first-route","text":"A route exposes your internal endpoints outside your cluster's built-in firewall. Click on the Route tab under Networking in the left navigation, then click Create Route : Supply input to the following parameters: Name: example Service: example Target Port: 8080 --> 8080 (TCP) Click Create Note that we are ignoring TLS configuration just for the purpose of this lab. Security will be addressed in a different lab. Access the route at the link provided under Location: If you have configured everything correctly, the browser will show Hello Openshift! . Congratulations, you just deployed your first application to Openshift.","title":"First Route"},{"location":"liberty-labs/IntroOpenshift/#changing-replica-instances","text":"Click on the Projects tab under Home from the left navigation, then click on myproject : Scroll down to see the resources that were created. Recall that we have created one deployment with 2 pods in the specification. We also created one service, and one route. Click on the 2 pods: Delete one of the pods by clicking on the menu on the right, then selecting Delete pod . When prompted, click Delete . This is not the right way to reduce number of instances. You will notice that as soon as one of the pods is being terminated, another one is being created. The reason is that the controller for the deployment resource knows that your specification is for 2 instances, and it honors that specification by creating another one. This also gives you automatic failure recovery should one of the pods crashes on its own. To change the number of instances, you will need to change the specification of your deployment. Click on the Deployments tab under Workloads in the left navigation, then click on example deployment: Click on the down arrow to reduce the replica size down to 1: After the operation is completed, click on the YAML tab: Note that the console had changed the REST specification on your behalf so that the replica count is now 1:","title":"Changing Replica Instances"},{"location":"liberty-labs/IntroOpenshift/#deploy-the-hello-openshift-image-through-the-command-line","text":"You can use both oc , the openshift command line tool, or kubectl , the Kubernetes command line tool, to interact with Openshift. Resources in Openshift are configured via REST data structure. For the command line tools, the REST data structure may be stored either in a YAML file, or in a JSON file. The command line tools may be used to: List available resources Create resources Update existing resources Delete resources","title":"Deploy the hello-openshift image through the command line"},{"location":"liberty-labs/IntroOpenshift/#command-line-terminal","text":"The oc command is already installed on your VM's terminal. Open a terminal window from the VM and clone the lab to your local directory via: git clone https://github.com/IBM/openshift-workshop-was.git Change directory to: openshift-workshop-was/labs/Openshift/IntroOpenshift cd openshift-workshop-was/labs/Openshift/IntroOpenshift","title":"Command Line Terminal"},{"location":"liberty-labs/IntroOpenshift/#login","text":"Return to the Openshift console, click on the arrow next to your login name and select Copy Login Command . In the new window that pops up, click on Display Token : Copy the oc login command and paste it into your web terminal. oc login --token=<TOKEN> --server=<SERVER Address> After login, the project last accessed is displayed, and it may or may not be the default project shown below: Logged into \"<SERVER address\" as \"<USER>\" using the token provided. You have access to 56 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\".","title":"Login"},{"location":"liberty-labs/IntroOpenshift/#listing-resources","text":"Use oc api-resources to list all available resource kinds. Note that resources in Openshift have a group, version, and kind. Some resources are global (not in a namespace), while others are scoped to a namespace. Many resources also have short names to save typing when using the command line tool. For example, you may use cm instead of ConfigMap as a command line parameter when the parameter is for a KIND . Example output: NAME SHORTNAMES APIGROUP NAMESPACED KIND bindings true Binding componentstatuses cs false ComponentStatu s configmaps cm true ConfigMap endpoints ep true Endpoints events ev true Event limitranges limits true LimitRange namespaces ns false Namespace nodes no false Node","title":"Listing resources"},{"location":"liberty-labs/IntroOpenshift/#listing-instances-of-a-resource-kind","text":"List all projects: oc get projects NAME DISPLAY NAME STATUS default Active ibm-cert-store Active ibm-system Active kube-node-lease Active kube-public Active kube-system Active myproject Active ... List all pods in all namespaces: oc get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE ibm-system ibm-cloud-provider-ip-52-116-182-130-67c44c6d5d-bh9x5 1/1 Running 0 24h ibm-system ibm-cloud-provider-ip-52-116-182-130-67c44c6d5d-cs5ln 1/1 Running 0 24h kube-system calico-kube-controllers-549fdb8d79-khkvr 1/1 Running 0 25h ... List all pods within a namespace: oc get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-549fdb8d79-khkvr 1/1 Running 0 25h calico-node-jc6ln 1/1 Running 0 24h calico-node-t7zwg 1/1 Running 0 24h ...","title":"Listing instances of a resource kind"},{"location":"liberty-labs/IntroOpenshift/#projects_1","text":"List all projects: oc get projects NAME DISPLAY NAME STATUS default Active ibm-cert-store Active ibm-system Active kube-node-lease Active Get current project: oc project (Note: current project may not be myproject shown below): Using project \"myproject\" on server \"https://c100-e.us-south.containers.cloud.ibm.com:32541\". Create a new project and make it the current project: oc new-project project1 Now using project \"project1\" on server \"https://c100-e.us-south.containers.cloud.ibm.com:32541\". Switch to the default project: oc project default Switch back to project1 : oc project project1 View the REST specification of the project: oc get project project1 -o yaml : apiVersion: project.openshift.io/v1 kind: Project metadata: annotations: openshift.io/description: \"\" openshift.io/display-name: \"\" openshift.io/requester: IAM#mcheng@us.ibm.com openshift.io/sa.scc.mcs: s0:c24,c9 openshift.io/sa.scc.supplemental-groups: 1000570000/10000 openshift.io/sa.scc.uid-range: 1000570000/10000 creationTimestamp: \"2020-01-30T20:28:13Z\" name: project1 resourceVersion: \"364002\" selfLink: /apis/project.openshift.io/v1/projects/project1 uid: a817b908-a7fe-4f82-9bfa-e18fa4c12584 spec: finalizers: - kubernetes status: phase: Active","title":"Projects"},{"location":"liberty-labs/IntroOpenshift/#first-application_1","text":"","title":"First Application"},{"location":"liberty-labs/IntroOpenshift/#first-deployment_1","text":"In your web terminal session, under the directory where you clone the labs repository (.../openshift-workshop-was/labs/Openshift/IntroOpenshift), you will find Deployment.yaml , for example, root@lab-tools-6d4cbb56b6-cn2k5:/openshift-workshop-was/labs/Openshift/IntroOpenshift# ls -lt drwxr-xr-x. 2 root root 4096 Apr 9 01:13 images -rw-r--r--. 1 root root 32461 Apr 9 01:13 README.md -rw-r--r--. 1 root root 171 Apr 9 01:13 Route.yaml -rw-r--r--. 1 root root 207 Apr 9 01:13 Service.yaml -rw-r--r--. 1 root root 384 Apr 9 01:13 Deployment.yaml Review the contents of Deployment.yaml and note that it is identical to the to the deployment from the last section except for the namespace. This allows us to deploy the same image in a different project. Using the same image customized for different environments is an important concept that will be covered further in future labs. cat Deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: example namespace: project1 spec: selector: matchLabels: app: hello-openshift replicas: 2 template: metadata: labels: app: hello-openshift spec: containers: - name: hello-openshift image: openshift/hello-openshift ports: - containerPort: 8080 Apply the deployment via the command line: oc apply -f Deployment.yaml deployment.apps/example created Check the status of deployment: oc get deployment example -o yaml . If the status does not show available replica count of 2, wait a few seconds before retrying. apiVersion: extensions/v1beta1 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: \"1\" kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"metadata\":{\"annotations\":{},\"name\":\"example\",\"namespace\":\"project1\"}, \"spec\":{\"replicas\":2,\"selector\":{\"matchLabels\":{\"app\":\"hello-openshift\"}},\"template\":{\"metadata\":{\"labels\":{\"app\":\"hello -openshift\"}},\"spec\":{\"containers\":[{\"image\":\"openshift/hello-openshift\",\"name\":\"hello-openshift\",\"ports\":[{\"containerPo rt\":8080}]}]}}}} creationTimestamp: \"2020-01-30T20:37:28Z\" generation: 1 name: example namespace: project1 resourceVersion: \"366251\" selfLink: /apis/extensions/v1beta1/namespaces/project1/deployments/example uid: f011a93d-2231-4187-b265-f350ee830971 spec: progressDeadlineSeconds: 600 replicas: 2 revisionHistoryLimit: 10 selector: matchLabels: app: hello-openshift strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: hello-openshift spec: containers: - image: openshift/hello-openshift imagePullPolicy: Always name: hello-openshift ports: - containerPort: 8080 protocol: TCP resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 status: availableReplicas: 2 conditions: - lastTransitionTime: \"2020-01-30T20:37:31Z\" lastUpdateTime: \"2020-01-30T20:37:31Z\" message: Deployment has minimum availability. reason: MinimumReplicasAvailable status: \"True\" type: Available - lastTransitionTime: \"2020-01-30T20:37:28Z\" lastUpdateTime: \"2020-01-30T20:37:31Z\" message: ReplicaSet \"example-75778c488\" has successfully progressed. reason: NewReplicaSetAvailable status: \"True\" type: Progressing observedGeneration: 1 readyReplicas: 2 replicas: 2 updatedReplicas: 2 List the running pods created by the controller for the deployment: oc get pods NAME READY STATUS RESTARTS AGE example-75778c488-7k7q2 1/1 Running 0 3m37s example-75778c488-c9jhd 1/1 Running 0 3m37s List the details for one of the pods: oc get pods <pod name> -o yaml > Note: <pod name> is listed under NAME in the previous command's output. apiVersion: v1 kind: Pod metadata: annotations: cni.projectcalico.org/podIP: 172.30.26.229/32 openshift.io/scc: restricted creationTimestamp: \"2020-01-30T20:37:28Z\" generateName: example-75778c488- labels: app: hello-openshift pod-template-hash: 75778c488 name: example-75778c488-7k7q2 namespace: project1 ownerReferences: - apiVersion: apps/v1 blockOwnerDeletion: true controller: true kind: ReplicaSet name: example-75778c488 uid: 2b03ef6a-8a1a-4f7e-9502-7249a4dabb98 resourceVersion: \"366248\" selfLink: /api/v1/namespaces/project1/pods/example-75778c488-7k7q2 uid: 504c999e-d47d-48f7-afcb-992a5a13fc67 spec: containers: - image: openshift/hello-openshift imagePullPolicy: Always name: hello-openshift ports: - containerPort: 8080 protocol: TCP resources: {} securityContext: capabilities: drop: - KILL - MKNOD - SETGID - SETUID runAsUser: 1000570000 terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: default-token-ngv4p readOnly: true dnsPolicy: ClusterFirst enableServiceLinks: true imagePullSecrets: - name: default-dockercfg-shjmz nodeName: 10.177.93.14 priority: 0 restartPolicy: Always schedulerName: default-scheduler securityContext: fsGroup: 1000570000 seLinuxOptions: level: s0:c24,c9 serviceAccount: default serviceAccountName: default terminationGracePeriodSeconds: 30 tolerations: - effect: NoExecute key: node.kubernetes.io/not-ready operator: Exists tolerationSeconds: 300 - effect: NoExecute key: node.kubernetes.io/unreachable operator: Exists tolerationSeconds: 300 volumes: - name: default-token-ngv4p secret: defaultMode: 420 secretName: default-token-ngv4p status: conditions: - lastProbeTime: null lastTransitionTime: \"2020-01-30T20:37:28Z\" status: \"True\" type: Initialized - lastProbeTime: null lastTransitionTime: \"2020-01-30T20:37:31Z\" status: \"True\" type: Ready - lastProbeTime: null lastTransitionTime: \"2020-01-30T20:37:31Z\" status: \"True\" type: ContainersReady - lastProbeTime: null lastTransitionTime: \"2020-01-30T20:37:28Z\" status: \"True\" type: PodScheduled containerStatuses: - containerID: cri-o://42828d4a8333d4fa9d3882805680d7693616610ff78a3e07f4794d91b86862b5 image: docker.io/openshift/hello-openshift:latest imageID: docker.io/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e lastState: {} name: hello-openshift ready: true restartCount: 0 started: true state: running: startedAt: \"2020-01-30T20:37:30Z\" hostIP: 10.177.93.14 phase: Running podIP: 172.30.26.229 podIPs: - ip: 172.30.26.229 qosClass: BestEffort startTime: \"2020-01-30T20:37:28Z\" Show the logs of one of the pods: oc logs <pod name> serving on 8888 serving on 8080 Take a look at Service.yaml and note that it's for the project1 namespace: cat Service.yaml Example output: apiVersion: v1 kind: Service metadata: name: example namespace: project1 spec: ports: - protocol: TCP port: 8080 targetPort: 8080 selector: app: hello-openshift type: ClusterIP Create the service so that it's accessible and load balanced for pods with label app: hello-openshift within the project1 namespace: oc apply -f Service.yaml service/example created Examine Route.yaml: cat Route.yaml Output: apiVersion: route.openshift.io/v1 kind: Route metadata: name: example namespace: project1 spec: port: targetPort: 8080 to: kind: Service name: example Apply the route to make the service reachable from outside the cluster: oc apply -f Route.yaml route.route.openshift.io/example created Generate the URL for the route, and point your browser to it: echo http://$(oc get route example --template='{{ .spec.host }}') Output: http://example-project1.apps.demo.ibmdte.net Open your Firefox browser again and visit the URL outputted by the previous command. You should see a web page displaying the following message:","title":"First Deployment"},{"location":"liberty-labs/IntroOpenshift/#changing-replica-instance","text":"List pods: oc get pods NAME READY STATUS RESTARTS AGE example-75778c488-7k7q2 1/1 Running 0 60m example-75778c488-c9jhd 1/1 Running 0 60m Delete one of the pods: oc delete pod <pod name> pod \"example-75778c488-7k7q2\" deleted List pods again and note that a new instance has been created as expected. The deployment specified 2 instances, so the controller tries to maintain 2 instances: oc get pods NAME READY STATUS RESTARTS AGE example-75778c488-c9jhd 1/1 Running 0 63m example-75778c488-rhjrx 1/1 Running 0 28s To reduce the number of pods, we can patch the resource in one of two ways: Scripted patch using the patch option of the command line: oc patch deployment example -p '{ \"spec\": { \"replicas\": 1 } }' Interactive patch using the edit option of the command line through vi editor: oc edit deployment example Under the spec section (not under the status section), change replicas: 2 to replicas: 1 , and save the change (by :wq ). The output: deployment.extensions/example edited Note: The above edits the copy that is stored in Openshift. You may also edit your local copy of Deployment.yaml and re-apply it. List the pods to show only 1 pod is running: oc get pods NAME READY STATUS RESTARTS AGE example-75778c488-c9jhd 1/1 Running 0 65m Cleanup: oc delete route example oc delete service example oc delete deployment example oc get pods . You may have to do this a few times, to wait for the pods to be deleted. Congratulations, you have deployed your first application to Openshift via the command line.","title":"Changing Replica Instance"},{"location":"liberty-labs/IntroOpenshift/#next","text":"Please follow the link to the next lab Operational Modernization : - Operational Modernization","title":"Next"},{"location":"overview/labs/","text":"Hnads-on Labs Here are the App Modernization Labs. All labs are bunsiness use case driven and they are independent of each other, you can pick and choose any lab you like to start with. If you have any questions, please contact Dr. Yi Tang at yitang@us.ibm.com. Basic Docker and Red Hat OpenShift Container Platform (OCP) Labs Docker Introduction OCP Introduction App Modernization Use Case Labs Operational Moderniztion Lab Runtime Moderniztion Lab Devops Moderniztoin Lab Application Management Modernization Lab","title":"Hands-on Lab List"},{"location":"overview/labs/#hnads-on-labs","text":"Here are the App Modernization Labs. All labs are bunsiness use case driven and they are independent of each other, you can pick and choose any lab you like to start with. If you have any questions, please contact Dr. Yi Tang at yitang@us.ibm.com.","title":"Hnads-on Labs"},{"location":"overview/labs/#basic-docker-and-red-hat-openshift-container-platform-ocp-labs","text":"Docker Introduction OCP Introduction","title":"Basic Docker and Red Hat OpenShift Container Platform (OCP) Labs"},{"location":"overview/labs/#app-modernization-use-case-labs","text":"Operational Moderniztion Lab Runtime Moderniztion Lab Devops Moderniztoin Lab Application Management Modernization Lab","title":"App Modernization Use Case Labs"},{"location":"overview/overview/","text":"Learning with Labs - Overview On the journey to cloud, enterprise customers are facing challenges moving their existing on-premises applications to the cloud quickly and cost-effectively. There are new cloud-native skills required for cloud deployments, including new cloud runtime configuration and administration, containerized development and testing, and Kubernetes orchestration. IBM WebSphre Hybrid Edition provides a complete and consistent experience and solution to modernize enterprise applications for cloud-native deployments. Customers can easily modernize their existing applications with IBM\u2019s integrated tools and develop new cloud-native applications faster for deployment on any cloud. This is a collection of App Modernization tutorials, focusing on the solutions for app modernization and move to cloud journey. This lab series walk you through different end-to-end app modernization and DevOps solutions and show how to create native cloud app or move the existing WebSphere workload to cloud, using technologies including Docker, Kubernetes, OpenShift, Liberty, Transformation Advisor, and Cloud Native Tool Dvopestools like Tekton and Argo CD. Note: All labs are independent of each other, you can choose any topic you like to start with The hands-on labs contain use case based lab exercises .","title":"Learning with Labs - Overview"},{"location":"overview/overview/#learning-with-labs-overview","text":"On the journey to cloud, enterprise customers are facing challenges moving their existing on-premises applications to the cloud quickly and cost-effectively. There are new cloud-native skills required for cloud deployments, including new cloud runtime configuration and administration, containerized development and testing, and Kubernetes orchestration. IBM WebSphre Hybrid Edition provides a complete and consistent experience and solution to modernize enterprise applications for cloud-native deployments. Customers can easily modernize their existing applications with IBM\u2019s integrated tools and develop new cloud-native applications faster for deployment on any cloud. This is a collection of App Modernization tutorials, focusing on the solutions for app modernization and move to cloud journey. This lab series walk you through different end-to-end app modernization and DevOps solutions and show how to create native cloud app or move the existing WebSphere workload to cloud, using technologies including Docker, Kubernetes, OpenShift, Liberty, Transformation Advisor, and Cloud Native Tool Dvopestools like Tekton and Argo CD. Note: All labs are independent of each other, you can choose any topic you like to start with The hands-on labs contain use case based lab exercises .","title":"Learning with Labs - Overview"},{"location":"resources/resources/","text":"Resources WebSphere Hybrid Edition Overview Documentation Installing components of WebShere Hybrid Edition Open Liberty Guides Transformation Advisor Documentation Transformation Advisor Web Page","title":"Resources"},{"location":"resources/resources/#resources","text":"WebSphere Hybrid Edition Overview Documentation Installing components of WebShere Hybrid Edition Open Liberty Guides Transformation Advisor Documentation Transformation Advisor Web Page","title":"Resources"}]}